@article{Beri2015,
abstract = {We present a novel proteomic standard for assessing liquid chromatography-tandem mass spectrometry (LC-MS/MS) instrument performance, in terms of chromatographic reproducibility and dynamic range within a single LC-MS/MS injection. The peptide mixture standard consists of six peptides that were specifically synthesized to cover a wide range of hydrophobicities (grand average hydropathy (GRAVY) scores of -0.6 to 1.9). A combination of stable isotope labeled amino acids ((13)C and (15)N) were inserted to create five isotopologues. By combining these isotopologues at different ratios, they span four orders of magnitude within each distinct peptide sequence. Each peptide, from lightest to heaviest, increases in abundance by a factor of 10. We evaluate several metrics on our quadrupole orbitrap instrument using the 6 × 5 LC-MS/MS reference mixture spiked into a complex lysate background as a function of dynamic range, including mass measurement accuracy (MMA) and the linear range of quantitation of MS1 and parallel reaction monitoring experiments. Detection and linearity of the instrument routinely spanned three orders of magnitude across the gradient (500 fmol to 0.5 fmol on column) and no systematic trend was observed for MMA of targeted peptides as a function of abundance by analysis of variance analysis (p = 0.17). Detection and linearity of the fifth isotopologue (i.e., 0.05 fmol on column) was dependent on the peptide and instrument scan type (MS1 vs PRM). We foresee that this standard will serve as a powerful method to conduct both intra-instrument performance monitoring/evaluation, technology development, and inter-instrument comparisons.},
author = {Beri, Joshua and Rosenblatt, Michael M and Strauss, Ethan and Urh, Marjeta and Bereman, Michael S},
doi = {10.1021/acs.analchem.5b04121},
file = {:Users/ed/Downloads/acs.analchem.5b04121.pdf:pdf},
issn = {1520-6882},
journal = {Anal. Chem.},
keywords = {Amino Acids,Amino Acids: chemistry,Chromatography,HEK293 Cells,Humans,Hydrophobic and Hydrophilic Interactions,Indicators and Reagents,Indicators and Reagents: chemistry,Liquid,Liquid: methods,Peptides,Peptides: chemical synthesis,Peptides: chemistry,Proteomics,Proteomics: methods,Tandem Mass Spectrometry,Tandem Mass Spectrometry: methods},
language = {EN},
month = {dec},
number = {23},
pages = {11635--40},
pmid = {26537636},
publisher = {American Chemical Society},
title = {{Reagent for Evaluating Liquid Chromatography-Tandem Mass Spectrometry (LC-MS/MS) Performance in Bottom-Up Proteomic Experiments.}},
url = {http://pubs.acs.org/doi/abs/10.1021/acs.analchem.5b04121{\#}.WACfLcQLo7E.mendeley},
volume = {87},
year = {2015}
}
@article{Sweredoski2011,
author = {Sweredoski, Michael J and Smith, Geoffrey T and Kalli, Anastasia and Graham, Robert L J and Hess, Sonja},
journal = {J. Biomol. Tech},
number = {4},
pages = {122--126},
title = {{LogViewer: A software tool to visualize quality control parameters to optimize proteomics experiments using Orbitrap and LTQ-FT mass spectrometers}},
volume = {22},
year = {2011}
}
@article{Bereman2016,
abstract = {We report the development of a completely automated pipeline to monitor system suitability in bottom-up proteomic experiments. LC MS/MS runs are automatically imported into Skyline and multiple identification free metrics are extracted from targeted peptides. These data are then uploaded to the Panorama Skyline document repository where metrics can be viewed in a web based interface using powerful process control techniques, including Levey-Jennings and Pareto plots. The interface is versatile and takes user input which allows the user significant control over the visualization of the data. The pipeline is vendor and instrument type neutral, supports multiple acquisition techniques (e.g. MS 1 filtering, data independent acquisition, parallel reaction monitoring, and selected reaction monitoring), can track performance of multiple instruments, and requires no manual intervention aside from initial setup. Data can be viewed from any computer with internet access and a web browser -- facilitating sharing of ...},
author = {Bereman, Michael S. and Beri, Joshua and Sharma, Vagisha and Nathe, Cory and Eckels, Josh and MacLean, Brendan and MacCoss, Michael J.},
doi = {10.1021/acs.jproteome.6b00744},
issn = {1535-3893},
journal = {J. Proteome Res.},
language = {EN},
month = {sep},
pages = {acs.jproteome.6b00744},
publisher = {American Chemical Society},
title = {{An Automated Pipeline to Monitor System Performance in Liquid Chromatography Tandem Mass Spectrometry Proteomic Experiments}},
url = {http://pubs.acs.org/doi/abs/10.1021/acs.jproteome.6b00744{\#}.V{\_}I9-p3tpIA.mendeley},
year = {2016}
}
@article{Bittremieux2016c,
abstract = {As mass-spectrometry-based proteomics has matured during the past decade a growing emphasis has been placed on quality control. For this purpose multiple computational quality control tools have been introduced. These tools generate a set of metrics that can be used to assess the quality of a mass spectrometry experiment. Here we review which different types of quality control metrics can be generated, and how they can be used to monitor both intra- and inter-experiment performance. We discuss the principal computational tools for quality control and list their main characteristics and applicability. As most of these tools have specific use cases it is not straightforward to compare their performance. For this survey we used different sets of quality control metrics derived from information at various stages in a mass spectrometry process and evaluated their effectiveness at capturing qualitative information about an experiment using a supervised learning approach. Furthermore, we discuss currently available algorithmic solutions that enable the usage of these quality control metrics for decision-making. This article is protected by copyright. All rights reserved.},
author = {Bittremieux, Wout and Valkenborg, Dirk and Martens, Lennart and Laukens, Kris},
doi = {10.1002/pmic.201600159},
issn = {1615-9861},
journal = {Proteomics},
keywords = {Mass spectrometry,Proteomics,Quality control},
language = {en},
month = {aug},
pmid = {27549080},
title = {{Computational quality control tools for mass spectrometry proteomics.}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/pmic.201600159/abstract},
year = {2016}
}
@article{Bittremieux2015a,
abstract = {Over the past few years, awareness has risen that for mass-spectrometry-based proteomics methods to mature into everyday analytical and clinical practices, extensive quality assessment is mandatory. A currently overlooked source of qualitative information originates from the mass spectrometer itself. Apart from the actual mass spectral data, raw-data objects also contain parameter settings and sensory information about the mass instrument. This information gives a detailed account of the operation of the instrument, which eventually can be related to observations in mass spectral data. The advantage of instrument information at the lowest level is the high sensitivity to detect emerging defects in a timely fashion. To this end, we introduce the Instrument MONitoring DataBase (iMonDB), which allows us to automatically extract, store, and manage the instrument parameters from raw-data objects into a highly efficient database structure. This enables us to monitor the instrument parameters over a considerable time period. Time course information about the instrument performance is necessary to define the normal range of operation and to detect anomalies that may correlate with instrument failure. The proposed tools foster an additional handle on quality control and are released as open source under the permissive Apache 2.0 license. The tools can be downloaded from https://bitbucket.org/proteinspector/imondb.},
author = {Bittremieux, Wout and Willems, Hanny and Kelchtermans, Pieter and Martens, Lennart and Laukens, Kris and Valkenborg, Dirk},
doi = {10.1021/acs.jproteome.5b00127},
issn = {1535-3907},
journal = {J. Proteome Res.},
keywords = {Databases, Factual,Equipment Failure Analysis,Humans,Mass Spectrometry,Mass Spectrometry: standards,Proteomics,Proteomics: instrumentation,Quality Control,Software},
language = {EN},
month = {may},
number = {5},
pages = {2360--6},
pmid = {25798920},
publisher = {American Chemical Society},
title = {{iMonDB: Mass Spectrometry Quality Control through Instrument Monitoring.}},
url = {http://pubs.acs.org/doi/abs/10.1021/acs.jproteome.5b00127{\#}.V-DaExZdCDo.mendeley},
volume = {14},
year = {2015}
}
@article{Campos2015,
abstract = {Proteomic technologies based on mass spectrometry (MS) have greatly evolved in the past years, and nowadays it is possible to routinely identify thousands of peptides from complex biological samples in a single LC–MS/MS experiment. Despite the advancements in proteomic technologies, the scientific community still faces important challenges in terms of depth and reproducibility of proteomics analyses. Here, we present a multicenter study designed to evaluate long-term performance of LC–MS/MS platforms within the Spanish Proteomics Facilities Network (ProteoRed-ISCIII). The study was performed under well-established standard operating procedures, and demonstrated that it is possible to attain qualitative and quantitative reproducibility over time. Our study highlights the importance of deploying quality assessment metrics routinely in individual laboratories and in multi-laboratory studies. The mass spectrometry data have been deposited to the ProteomeXchange Consortium with the data set identifier PXD000205.This article is part of a Special Issue entitled: HUPO 2014.},
author = {Campos, Alex and D{\'{i}}az, Ram{\'{o}}n and Mart{\'{i}}nez-Bartolom{\'{e}}, Salvador and Sierra, Jose and Gallardo, Oscar and Sabid{\'{o}}, Eduard and L{\'{o}}pez-Lucendo, Maria and {Ignacio Casal}, J. and Pasquarello, Carla and Scherl, Alexander and Chiva, Cristina and Borras, Eva and Odena, Antonia and Elortza, F{\'{e}}lix and Azkargorta, Mikel and Ibarrola, Nieves and Canals, Francesc and Albar, Juan P. and Oliveira, Eliandre},
doi = {10.1016/j.jprot.2015.05.012},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Campos et al. - 2015 - Multicenter experiment for quality control of peptide-centric LC–MSMS analysis — A longitudinal performance a.pdf:pdf},
issn = {18743919},
journal = {J. Proteomics},
pages = {264--274},
title = {{Multicenter experiment for quality control of peptide-centric LC–MS/MS analysis — A longitudinal performance assessment with nLC coupled to orbitrap MS analyzers}},
volume = {127},
year = {2015}
}
@article{VanderHeyden1999,
abstract = {A robustness test was performed on a chromatographic method to identify and assay an active substance and two related compounds in film-coated tablet. For a number of responses the originally applied system suitability criteria were evaluated based on the results of the robustness test. Ambiguous situations can occur in situations where a method is found to be robust to assay the substances, as was the case here, but when system suitability criteria for some responses are violated. To avoid this, a proposal is made to define or re-define system suitability limits based on the results of the robustness test. From the effects found in the robustness test, the experimental conditions giving the worst result that still is acceptable and probable to occur are predicted and the system suitability limits are defined from replicated experiments in these conditions.},
author = {{Vander Heyden}, Y. and Jimidar, M. and Hund, E. and Niemeijer, N. and Peeters, R. and Smeyers-Verbeke, J. and Massart, D.L. and Hoogmartens, J.},
doi = {10.1016/S0021-9673(99)00328-3},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Vander Heyden et al. - 1999 - Determination of system suitability limits with a robustness test.pdf:pdf},
issn = {00219673},
journal = {J. Chromatogr. A},
number = {1},
pages = {145--154},
title = {{Determination of system suitability limits with a robustness test}},
volume = {845},
year = {1999}
}
@article{Vogt2011,
abstract = {Quality-by-design (QbD) is a systematic approach to drug development, which begins with predefined objectives, and uses science and risk management approaches to gain product and process understanding and ultimately process control. The concept of QbD can be extended to analytical methods. QbD mandates the definition of a goal for the method, and emphasizes thorough evaluation and scouting of alternative methods in a systematic way to obtain optimal method performance. Candidate methods are then carefully assessed in a structured manner for risks, and are challenged to determine if robustness and ruggedness criteria are satisfied. As a result of these studies, the method performance can be understood and improved if necessary, and a control strategy can be defined to manage risk and ensure the method performs as desired when validated and deployed. In this review, the current state of analytical QbD in the industry is detailed with examples of the application of analytical QbD principles to a range of analytical methods, including high-performance liquid chromatography, Karl Fischer titration for moisture content, vibrational spectroscopy for chemical identification, quantitative color measurement, and trace analysis for genotoxic impurities.},
author = {Vogt, Frederick G and Kord, Alireza S},
doi = {10.1002/jps.22325},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Vogt, Kord - 2011 - Development of quality-by-design analytical methods.pdf:pdf},
issn = {1520-6017},
journal = {J. Pharm. Sci.},
keywords = {Chemistry Techniques, Analytical,Chemistry Techniques, Analytical: statistics {\&} num,Clinical Laboratory Techniques,Drug Contamination,Drug Contamination: prevention {\&} control,Pharmaceutical Preparations,Quality Control,Reproducibility of Results,Risk Management,Risk Management: methods,Technology, Pharmaceutical},
language = {en},
month = {mar},
number = {3},
pages = {797--812},
pmid = {21280050},
publisher = {Wiley Subscription Services, Inc., A Wiley Company},
title = {{Development of quality-by-design analytical methods.}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/jps.22325/full},
volume = {100},
year = {2011}
}
@article{Masson2007,
abstract = {The process of quality assurance should demonstrate that the method and the analytical instrument provide accurate and precise results, or whether deterioration occurs. For this, the quality procedure should include tests which provide information on the characteristic performance of the method. According to the recent literature, useful procedures contributing to the overall quality of analytical results are illustrated in relation with liquid chromatography. Parameters examined are carry-over, column statement, accuracy and precision. The performance of the whole system from the extraction to measurement of analytes of interest must be monitored with control charts. Data representing the routine performance of chromatographic equipment systems and method allow predicting long-term uncertainties and confidence intervals.},
author = {Masson, Pierre},
doi = {10.1016/j.chroma.2007.03.003},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Masson - 2007 - Quality control techniques for routine analysis with liquid chromatography in laboratories.pdf:pdf},
issn = {00219673},
journal = {J. Chromatogr. A},
number = {1},
pages = {168--173},
title = {{Quality control techniques for routine analysis with liquid chromatography in laboratories}},
volume = {1158},
year = {2007}
}
@article{Scheltema2012,
abstract = {With the advent of high-throughput mass spectrometry (MS)-based proteomics, the magnitude and complexity of the performed experiments has increased dramatically. Likewise, investments in chromatographic and MS instrumentation are a large proportion of the budget of proteomics laboratories. Guarding measurement quality and maximizing uptime of the LC-MS/MS systems therefore requires constant care despite automated workflows. We describe a real-time surveillance system, called SprayQc, that continuously monitors the status of the peripheral equipment to ensure that operational parameters are within an acceptable range. SprayQc is composed of multiple plug-in software components that use computer vision to analyze electrospray conditions, monitor the chromatographic device for stable backpressure, interact with a column oven to control pressure by temperature, and ensure that the mass spectrometer is still acquiring data. Action is taken when a failure condition has been detected, such as stopping the column oven and the LC flow, as well as automatically notifying the appropriate operator. Additionally, all defined metrics can be recorded synchronized on retention time with the MS acquisition file, allowing for later inspection and providing valuable information for optimization. SprayQc has been extensively tested in our laboratory, supports third-party plug-in development, and is freely available for download from http://sourceforge.org/projects/sprayqc .},
author = {Scheltema, Richard A and Mann, Matthias},
doi = {10.1021/pr201219e},
issn = {1535-3907},
journal = {J. Proteome Res.},
keywords = {Chromatography, Liquid,Chromatography, Liquid: standards,Quality Control,Software,Spectrometry, Mass, Electrospray Ionization,Spectrometry, Mass, Electrospray Ionization: instr,Spectrometry, Mass, Electrospray Ionization: stand,Tandem Mass Spectrometry,Tandem Mass Spectrometry: instrumentation,Tandem Mass Spectrometry: standards,Temperature,Video Recording},
language = {EN},
month = {jun},
number = {6},
pages = {3458--66},
pmid = {22515319},
publisher = {American Chemical Society},
title = {{SprayQc: a real-time LC-MS/MS quality monitoring system to maximize uptime using off the shelf components.}},
url = {http://pubs.acs.org/doi/abs/10.1021/pr201219e{\#}.V9rU6iZeCQs.mendeley},
volume = {11},
year = {2012}
}
@techreport{Bielow2015b,
abstract = {Mass spectrometry-based proteomics coupled to liquid chromatography has matured into an automatized, high-throughput technology, producing data on the scale of multiple gigabytes per instrument per day. Consequently, an automated quality control (QC) and quality analysis (QA) capable of detecting measurement bias, verifying consistency, and avoiding propagation of error is paramount for instrument operators and scientists in charge of downstream analysis. We have developed an R-based QC pipeline called Proteomics Quality Control (PTXQC) for bottom-up LC–MS data generated by the MaxQuant1 software pipeline. PTXQC creates a QC report containing a comprehensive and powerful set of QC metrics, augmented with automated scoring functions. The automated scores are collated to create an overview heatmap at the beginning of the report, giving valuable guidance also to nonspecialists. Our software supports a wide range of experimental designs, including stable isotope labeling by amino acids in cell culture (SILAC)...},
author = {Bielow, Chris and Mastrobuoni, Guido and Kempa, Stefan},
isbn = {10.1021/acs.jproteome.5b00780},
keywords = {MaxQuant,PTXQC,quality control},
language = {EN},
month = {dec},
publisher = {American Chemical Society},
title = {{Proteomics Quality Control: Quality Control Software for MaxQuant Results}},
url = {http://pubs.acs.org/doi/suppl/10.1021/acs.jproteome.5b00780{\#}.V9rUVcOYKL4.mendeley},
year = {2015}
}
@article{Chang2015,
author = {Chang, Winston and Cheng, Joe and Allaire, J J and Xie, Yihui and McPherson, Jonathan},
publisher = {Citeseer},
title = {{Package ‘shiny'}},
year = {2015}
}
@article{Bittremieux2015,
abstract = {Over the past few years, awareness has risen that for mass-spectrometry-based proteomics methods to mature into everyday analytical and clinical practices, extensive quality assessment is mandatory. A currently overlooked source of qualitative information originates from the mass spectrometer itself. Apart from the actual mass spectral data, raw-data objects also contain parameter settings and sensory information about the mass instrument. This information gives a detailed account of the operation of the instrument, which eventually can be related to observations in mass spectral data. The advantage of instrument information at the lowest level is the high sensitivity to detect emerging defects in a timely fashion. To this end, we introduce the Instrument MONitoring DataBase (iMonDB), which allows us to automatically extract, store, and manage the instrument parameters from raw-data objects into a highly efficient database structure. This enables us to monitor the instrument parameters over a considerable time period. Time course information about the instrument performance is necessary to define the normal range of operation and to detect anomalies that may correlate with instrument failure. The proposed tools foster an additional handle on quality control and are released as open source under the permissive Apache 2.0 license. The tools can be downloaded from https://bitbucket.org/proteinspector/imondb.},
author = {Bittremieux, Wout and Willems, Hanny and Kelchtermans, Pieter and Martens, Lennart and Laukens, Kris and Valkenborg, Dirk},
doi = {10.1021/acs.jproteome.5b00127},
issn = {1535-3907},
journal = {J. Proteome Res.},
keywords = {Databases, Factual,Equipment Failure Analysis,Humans,Mass Spectrometry,Mass Spectrometry: standards,Proteomics,Proteomics: instrumentation,Quality Control,Software},
language = {EN},
month = {may},
number = {5},
pages = {2360--6},
pmid = {25798920},
publisher = {American Chemical Society},
title = {{iMonDB: Mass Spectrometry Quality Control through Instrument Monitoring.}},
url = {http://pubs.acs.org/doi/abs/10.1021/acs.jproteome.5b00127{\#}.V9gxh3wi2Mk.mendeley},
volume = {14},
year = {2015}
}
@article{Bantscheff2012,
author = {Bantscheff, Marcus and Lemeer, Simone and Savitski, Mikhail M and Kuster, Bernhard},
issn = {1618-2642},
journal = {Anal. Bioanal. Chem.},
number = {4},
pages = {939--965},
publisher = {Springer},
title = {{Quantitative mass spectrometry in proteomics: critical review update from 2007 to the present}},
volume = {404},
year = {2012}
}
@article{Team2015,
author = {Team, R Core},
journal = {Doc. Free. available internet http//www. r-project. org},
title = {{R: A language and environment for statistical computing [Internet]. Vienna, Austria: R Foundation for Statistical Computing; 2013}},
year = {2015}
}
@article{Picotti2012,
annote = {10.1038/nmeth.2015},
author = {Picotti, Paola and Aebersold, Ruedi},
issn = {1548-7091},
journal = {Nat Meth},
month = {jun},
number = {6},
pages = {555--566},
publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
title = {{Selected reaction monitoring-based proteomics: workflows, potential, pitfalls and future directions}},
url = {http://dx.doi.org/10.1038/nmeth.2015 http://www.nature.com/nmeth/journal/v9/n6/abs/nmeth.2015.html{\#}supplementary-information},
volume = {9},
year = {2012}
}
@article{Gallien2011,
abstract = {Selected reaction monitoring (SRM) performed on triple quadrupole mass spectrometers has been the reference quantitative technique to analyze small molecules for several decades. It is now emerging in proteomics as the ideal tool to complement shotgun qualitative studies; targeted SRM quantitative analysis offers high selectivity, sensitivity and a wide dynamic range. However, SRM applied to proteomics presents singularities that distinguish it from small molecules analysis. This review is an overview of SRM technology and describes the specificities and the technical aspects of proteomics experiments. Ongoing developments aiming at increasing multiplexing capabilities of SRM are discussed; they dramatically improve its throughput and extend its field of application to directed or supervised discovery experiments.},
author = {Gallien, Sebastien and Duriez, Elodie and Domon, Bruno},
doi = {10.1002/jms.1895},
issn = {1096-9888},
journal = {J. Mass Spectrom.},
keywords = {Mass Spectrometry,Mass Spectrometry: methods,Proteins,Proteins: chemistry,Proteomics,Proteomics: methods},
language = {en},
month = {mar},
number = {3},
pages = {298--312},
pmid = {21394846},
publisher = {John Wiley {\&} Sons, Ltd.},
title = {{Selected reaction monitoring applied to proteomics.}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/jms.1895/full},
volume = {46},
year = {2011}
}
@article{MacLean2010a,
author = {MacLean, Brendan and Tomazela, Daniela M and Shulman, Nicholas and Chambers, Matthew and Finney, Gregory L and Frewen, Barbara and Kern, Randall and Tabb, David L and Liebler, Daniel C and MacCoss, Michael J},
issn = {1367-4803},
journal = {Bioinformatics},
number = {7},
pages = {966--968},
publisher = {Oxford Univ Press},
title = {{Skyline: an open source document editor for creating and analyzing targeted proteomics experiments}},
volume = {26},
year = {2010}
}
@article{Maboudou-Tchao2011,
author = {Maboudou-Tchao, Edgard M. and Hawkins, Douglas M.},
journal = {J. Qual. Technol.},
number = {2},
pages = {113--126},
title = {{Self-Starting Multivariate Control Charts for Location and Scale}},
volume = {43},
year = {2011}
}
@article{Keefe2015,
abstract = {ABSTRACTThe recommended size of the Phase I data set used to estimate the in-control parameters has been discussed many times in the process monitoring literature. Collecting baseline data, however, can be difficult or slow in some applications. Such issues have resulted in the development of self-starting control charts that allow charting early, near the start of data collection. In our article, we use the average of the in-control average run length (AARL) and the standard deviation of the in-control average run length (SDARL) to assess the in-control run length performance of self-starting charts conditioned on the preliminary data used. This approach accounts for practitioner-to-practitioner variability in the in-control average run length (ARL) of self-starting charts, which has not been considered previously. We found that there was a significant amount of variation in the in-control ARL values obtained by practitioners due to the sampling variation of the initial estimators of the in-control param...},
author = {Keefe, Matthew J. and Woodall, William H. and Jones-Farmer, L. Allison},
doi = {10.1080/08982112.2015.1065323},
issn = {0898-2112},
journal = {Qual. Eng.},
keywords = {SPC,Shewhart chart,estimation effect,standard deviation of the average run length,statistical process control},
language = {en},
month = {aug},
number = {4},
pages = {488--499},
publisher = {Taylor {\&} Francis},
title = {{The Conditional In-Control Performance of Self-Starting Control Charts}},
url = {http://www.tandfonline.com/doi/abs/10.1080/08982112.2015.1065323{\#}.V7MRY6vTfUk.mendeley},
volume = {27},
year = {2015}
}
@article{Dogu2013,
author = {Doğu, Eralp and Kocako{\c{c}}, İpek Deveci},
issn = {0361-0918},
journal = {Commun. Stat. Comput.},
number = {6},
pages = {1235--1255},
publisher = {Taylor {\&} Francis},
title = {{A multivariate change point detection procedure for monitoring mean and covariance simultaneously}},
volume = {42},
year = {2013}
}
@article{Cornelissen1997,
author = {Corn{\'{e}}lissen, G and Halberg, F and Hawkins, D and Otsuka, K and Henke, W},
issn = {0309-1902},
journal = {J. Med. Eng. Technol.},
number = {3-4},
pages = {111--120},
publisher = {Taylor {\&} Francis},
title = {{Individual assessment of antihypertensive response by self-starting cumulative sums}},
volume = {21},
year = {1997}
}
@inproceedings{Albloushi2015,
author = {Albloushi, Tamador and Suwaidi, Aisha and Zarouni, Noura and Abdelrahman, Asma and Shamsuzzaman, Mohammad},
booktitle = {Ind. Eng. Oper. Manag. (IEOM), 2015 Int. Conf.},
isbn = {1479960640},
pages = {1--5},
publisher = {IEEE},
title = {{Design of X̅{\&}R control charts for monitoring quality of care for hypertension}},
year = {2015}
}
@article{Hawkins2005,
author = {Hawkins, Douglas M and Zamba, Keogile D},
issn = {0022-4065},
journal = {J. Qual. Technol.},
number = {1},
pages = {21--31},
publisher = {[Milwaukee]: American Society for Quality Control.},
title = {{A change-point model for a shift in variance}},
volume = {37},
year = {2005}
}
@article{Quesenberry1993,
author = {Quesenberry, Charles P},
journal = {J. Qual. Technol.},
number = {4},
pages = {237--247},
title = {{The effect of sample size on estimated limits for X and S control charts}},
volume = {25},
year = {1993}
}
@article{HAWKINS2011,
author = {Hawkins, Douglas M},
journal = {J. Qual. Technol.},
number = {2},
pages = {113--126},
title = {{Self-starting multivariate control charts for location and scale}},
volume = {43},
year = {2011}
}
@article{JonesL.Allison;ChampCharlesW.;Rigdon2004,
author = {Jones, L. Allison and Champ, Charles W. and Rigdon, Steven E.},
journal = {J. Qual. Technol.},
number = {1},
pages = {95--108},
title = {{The Run Length Distribution of the CUSUM with Estimated Parameters}},
volume = {36},
year = {2004}
}
@article{Zou2007,
author = {Zou, Changliang and Zhou, Chunguang and Wang, Zhaojun and Tsung, Fugee},
issn = {0022-4065},
journal = {J. Qual. Technol.},
number = {4},
pages = {364},
publisher = {American Society for Quality},
title = {{A self-starting control chart for linear profiles}},
volume = {39},
year = {2007}
}
@article{Quesenberry1991a,
author = {Quesenberry, Charles P},
journal = {J. Qual. Technol.},
number = {3},
pages = {213--224},
title = {{SPC Q charts for start-up processes and short or long runs}},
volume = {23},
year = {1991}
}
@article{ZambaK.D.2009,
author = {{Zamba K. D.} and Hawkins, Douglas M},
journal = {J. Qual. Technol.},
number = {3},
pages = {285--303},
title = {{A multivariate change point model for change in mean vector and/or covariance structure}},
volume = {41},
year = {2009}
}
@article{Thaga2006,
author = {Thaga, Keoagile and Gabaitiri, Lesego},
issn = {1869-6147},
journal = {Econ. Qual. Control},
number = {1},
pages = {113--125},
title = {{Multivariate Max-Chart}},
volume = {21},
year = {2006}
}
@article{Quesenberry1995,
author = {Quesenberry, Charles P},
journal = {J. Qual. Technol.},
number = {3},
pages = {184--203},
title = {{On properties of Q charts for variables}},
volume = {27},
year = {1995}
}
@article{Dogu2014,
abstract = {Change point estimation procedures simplify the efforts to search for and identify special causes in multivariate statistical process monitoring. After a signal is generated by the simultaneously used control charts or a single control chart, add-on change point procedure estimates the time of the change. In this study, multivariate joint change point estimation performance for simultaneous monitoring of both location and dispersion is compared under the assumption that various single charts are used to monitor the process. The change detection performance for several structural changes for the mean vector and covariance matrix is also discussed. It is concluded that choice of the control chart to obtain a signal may affect the change point detection performance.},
author = {Dogu, Eralp},
doi = {10.1080/00949655.2014.880704},
issn = {0094-9655},
journal = {J. Stat. Comput. Simul.},
keywords = {62H12,62P30,Hotelling's T2 control chart,change point estimation,generalized variance control chart,maximum-likelihood estimation,multivariate single control charts},
language = {en},
month = {jan},
number = {8},
pages = {1529--1543},
publisher = {Taylor {\&} Francis},
title = {{Identifying the time of a step change with multivariate single control charts}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00949655.2014.880704{\#}.V7MPkhXFKlU.mendeley},
volume = {85},
year = {2015}
}
@article{Hawkins2005a,
abstract = {Statistical process control (SPC) involves ongoing checks to ensure that neither the mean nor the variability of the process readings has changed. Conventionally, this is done by pairs of charts—Shewhart X and S (or R) charts, cumulative sum charts for mean and for variance, or exponentially weighted moving average charts for mean and variance. The traditional methods of calculating the statistical properties of control charts are based on the assumption that the in-control true mean and variance were known exactly, and use these assumed true values to set center lines, control limits, and decision intervals. The reality, however, is that true parameter values are seldom if ever known exactly; rather, they are commonly estimated from a phase I sample. The random errors in the estimates lead to uncertain run length distribution of the resulting charts. An attractive alternative to the traditional charting methods is a single chart using the unknown-parameter likelihood ratio test for a change in mean and/o...},
author = {Hawkins, Douglas M and Zamba, K. D},
doi = {10.1198/004017004000000644},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Control charts,Generalized likelihood ratio,Phase i,Phase ii},
language = {en},
month = {may},
number = {2},
pages = {164--173},
publisher = {Taylor {\&} Francis},
title = {{Statistical Process Control for Shifts in Mean or Variance Using a Changepoint Formulation}},
url = {http://amstat.tandfonline.com/doi/abs/10.1198/004017004000000644{\#}.V7MPN{\_}EXCe0.mendeley},
volume = {47},
year = {2005}
}
@article{SULLIVAN2000,
abstract = {A preliminary control chart is given for detecting a shift in the mean vector, the covariance matrix, or both, when multivariate individual observations are available. The data are partitioned after each observation in turn, and the likelihood ratio statistic for a shift is calculated. The control chart is obtained by plotting these statistics after dividing by the expected value under the condition of no shift. This adjustment is done in order to reduce the variation in sensitivity with the location of any shift. Using generalized inverses allows the detection of a shift after as few as two of the observations, or with as few as two remaining observations, or at any intermediate point. Multiple shifts often can be detected by recursive application of the method. When a shift is detected, the plotted statistic is divided into a part due to the shift in the sample mean vector and another part attributable to a shift in the sample covariance matrix. This is done for diagnostic purposes. Using simulation, ap...},
author = {Sullivan, Joe H. and Woodall, William H.},
doi = {10.1080/07408170008963929},
issn = {0740-817X},
journal = {IIE Trans.},
language = {en},
month = {jun},
number = {6},
pages = {537--549},
publisher = {Taylor {\&} Francis Group},
title = {{Change-point detection of mean vector or covariance matrix shifts using multivariate individual observations}},
url = {http://www.tandfonline.com/doi/abs/10.1080/07408170008963929{\#}.V7MOi55ngGA.mendeley},
volume = {32},
year = {2000}
}
@article{Sullivan2002,
abstract = {Multivariate versions of cumulative sum, exponentially weighted moving average (EWMA), and Hotelling's T2 charts typically assume knowledge of the in-control process parameters or, with a new or changed process, use parameter estimates from an in-control reference sample of preliminary observations. In contrast, the self-starting chart begins controlling the process without the need for preliminary observations, an advantage when production is slow or when the cost of early out-of-control production is high. Furthermore, the use of estimated parameters substantially degrades the expected performance of conventional charts, a problem avoided by the proposed chart. The self-starting chart uses the deviation of each observation vector from the average of all previous observations. These deviations, or innovations, can be plotted on a T2 control chart or accumulated in a multivariate EWMA (MEWMA) chart. The run-length performance is evaluated for step shifts occurring at various points, and the MEWMA charts a...},
author = {Sullivan, Joe H and Jones, L. Allison},
doi = {10.1198/004017002753398290},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Exponentially weighted,Innovation,MEWMA,Probability integral transformation,Q-chart,Recursive residual,Statistical quality control},
language = {en},
month = {feb},
number = {1},
pages = {24--33},
publisher = {Taylor {\&} Francis},
title = {{A Self-Starting Control Chart for Multivariate Individual Observations}},
url = {http://www.tandfonline.com/doi/abs/10.1198/004017002753398290{\#}.V7MOUoNLQPg.mendeley},
volume = {44},
year = {2002}
}
@article{Hawkins2007,
abstract = {Multivariate control charts are valuable tools for industrial quality control. The conventional discussion of them rests on the presumption that the in-control process parameters are known a priori. The more common reality is that practitioners plug in parameter estimates gathered from a special phase I sample to establish parameter values for the charts. But no sample will establish the exact process parameters, and quite small random errors translate into serious distortions of the run behavior, particularly of sensitive charts, and can affect chart performance. So-called “self-starting” methods can begin the control of the process right after startup without the preliminary step of a large phase I sample. Univariate self-starting methods for converting the unknown-parameter stream of process readings into a known-parameter sequence have been available for some time now. This article develops a multivariate equivalent by providing a way to transform the process readings into a stream of vectors followin...},
author = {Hawkins, Douglas M and Maboudou-Tchao, Edgard M},
doi = {10.1198/004017007000000083},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Cholesky decomposition,Recursive residual,Regression adjustment},
language = {en},
month = {may},
number = {2},
pages = {199--209},
publisher = {Taylor {\&} Francis},
title = {{Self-Starting Multivariate Exponentially Weighted Moving Average Control Charting}},
url = {http://www.tandfonline.com/doi/abs/10.1198/004017007000000083{\#}.V7MNB24YuFc.mendeley},
volume = {49},
year = {2007}
}
@article{Jones2001,
abstract = {The exponentially weighted moving average (EWMA) control chart is typically designed assuming that standards are given for the process parameters. In practice, the parameters are rarely known, and control charts are constructed using estimates in place of the parameters. This practice can affect the control chart's run-length performance in both in- and out-of-control situations. Specifically, estimation can lead to substantially more frequent false alarms and yet reduce the sensitivity of the chart to detecting process changes. In this article, the run-length distribution of the EWMA chart with estimated parameters is derived. The effect of estimation on the performance of the chart is discussed in a variety of practical scenarios.},
author = {Jones, L. Allison and Champ, Charles W and Rigdon, Steven E},
doi = {10.1198/004017001750386279},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Average run length,Control chart,EWMA,Integral equation,Quadrature},
language = {en},
month = {may},
number = {2},
pages = {156--167},
publisher = {Taylor {\&} Francis},
title = {{The Performance of Exponentially Weighted Moving Average Charts With Estimated Parameters}},
url = {http://www.tandfonline.com/doi/abs/10.1198/004017001750386279{\#}.V7MLvAW9V0E.mendeley},
volume = {43},
year = {2001}
}
@article{HOLMES1993,
author = {Holmes, Donald S. and Mergen, A. Erhan},
doi = {10.1080/08982119308919004},
issn = {0898-2112},
journal = {Qual. Eng.},
keywords = {Mean Square Successive Differences (MSSD),Multivariate SPC,T2  control chart,Variance-covariance matrix},
language = {en},
month = {jan},
number = {4},
pages = {619--625},
publisher = {Taylor {\&} Francis Group},
title = {{Improving the performance of the T{\^{}}2 control chart}},
url = {http://www.tandfonline.com/doi/abs/10.1080/08982119308919004?journalCode=lqen20{\#}.V7MLjopH10c.mendeley},
volume = {5},
year = {1993}
}
@article{Hawkins2008,
abstract = {Multivariate exponentially weighted moving average (MEWMA) charts are among the best control charts for detecting small changes in any direction. The well-known MEWMA is directed at changes in the mean vector. But changes can occur in either the location or the variability of the correlated multivariate quality characteristics, calling for parallel methodologies for detecting changes in the covariance matrix. This article discusses an exponentially weighted moving covariance matrix for monitoring the stability of the covariance matrix of a process. Used together with the location MEWMA, this chart provides a way to satisfy Shewhart's dictum that proper process control monitor both mean and variability. We show that the chart is competitive, generally outperforming current control charts for the covariance matrix.},
author = {Hawkins, Douglas M. and Maboudou-Tchao, Edgard M.},
doi = {10.1198/004017008000000163},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Average run length,Average run length bias,Regression adjustment},
language = {en},
month = {may},
number = {2},
pages = {155--166},
publisher = {Taylor {\&} Francis},
title = {{Multivariate Exponentially Weighted Moving Covariance Matrix}},
url = {http://www.tandfonline.com/doi/abs/10.1198/004017008000000163{\#}.V7MLYN8kb5Y.mendeley},
volume = {50},
year = {2008}
}
@article{Lowry2012,
abstract = {A multivariate extension of the exponentially weighted moving average (EWMA) control chart is presented, and guidelines given for designing this easy-to-implement multivariate procedure. A comparison shows that the average run length (ARL) performance of this chart is similar to that of multivariate cumulative sum (CUSUM) control charts in detecting a shift in the mean vector of a multivariate normal distribution. As with the Hotelling's $\chi$2 and multivariate CUSUM charts, the ARL performance of the multivariate EWMA chart depends on the underlying mean vector and covariance matrix only through the value of the noncentrality parameter. Worst-case scenarios show that Hotelling's $\chi$2 charts should always be used in conjunction with multivariate CUSUM and EWMA charts to avoid potential inertia problems. Examples are given to illustrate the use of the proposed procedure.},
author = {Lowry, Cynthia A. and Woodall, William H. and Champ, Charles W. and Rigdon, Steven E.},
journal = {Technometrics},
keywords = {Average run length,Hotelling's T 2 chart,Multivariate CUSUM,Statistical process control},
language = {en},
month = {mar},
publisher = {Taylor {\&} Francis Group},
title = {{A Multivariate Exponentially Weighted Moving Average Control Chart}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00401706.1992.10485232{\#}.V7MLQIrdTTg.mendeley},
year = {2012}
}
@article{Zhang2010,
abstract = {Recently, monitoring the process mean and variability simultaneously for multivariate processes by using a single control chart has drawn some attention. However, due to the complexity of multivariate distributions, existing methods in univariate processes cannot be readily extended to multivariate processes. In this paper, we propose a new single control chart which integrates the exponentially weighted moving average (EWMA) procedure with the generalized likelihood ratio (GLR) test for jointly monitoring both the multivariate process mean and variability. Due to the powerful properties of the GLR test and the EWMA procedure, the new chart provides quite robust and satisfactory performance in various cases, including detection of the decrease in variability and individual observation at the sampling point, which are very important cases in many practical applications but may not be well handled by existing approaches in the literature. The application of our proposed method is illustrated by a real data example in ambulatory monitoring.},
author = {Zhang, Jiujun and Li, Zhonghua and Wang, Zhaojun},
doi = {10.1016/j.csda.2010.03.027},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Li, Wang - 2010 - A multivariate control chart for simultaneously monitoring process mean and variability.pdf:pdf},
issn = {01679473},
journal = {Comput. Stat. Data Anal.},
number = {10},
pages = {2244--2252},
title = {{A multivariate control chart for simultaneously monitoring process mean and variability}},
volume = {54},
year = {2010}
}
@article{Li2010,
abstract = {A self-starting control chart, based on the likelihood ratio test and the exponentially weighted moving average procedure, is proposed for monitoring the process mean and variance simultaneously when the process parameters are unknown. A table is presented to assist in the design of the control chart with different parameters. Its in-control average run length can be evaluated by a two-dimensional Markov chain model. Moreover, the diagnostic aids of the proposed chart are given. Monte Carlo simulation results compared with some competing methods in the literature show that the proposed approach has quite satisfactory charting performance across a range of possible shifts when the process parameters are unknown, even including the detection of a decrease in variability. A real data example from industrial manufacturing is used to demonstrate its implementation.},
author = {Li, Zhonghua and Zhang, Jiujun and Wang, Zhaojun},
doi = {10.1080/00207540903051692},
issn = {0020-7543},
journal = {Int. J. Prod. Res.},
keywords = {SPC,quality control,reliability engineering},
language = {en},
month = {aug},
number = {15},
pages = {4537--4553},
publisher = {Taylor {\&} Francis Group},
title = {{Self-starting control chart for simultaneously monitoring process mean and variance}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00207540903051692{\#}.V7MIxIdjf0I.mendeley},
volume = {48},
year = {2010}
}
@article{Cheng2006,
author = {Cheng, Smiley W. and Thaga, Keoagile},
doi = {10.1002/qre.730},
issn = {0748-8017},
journal = {Qual. Reliab. Eng. Int.},
keywords = {autocorrelated processes,single chart,statistical process control,variables data},
language = {en},
month = {nov},
number = {7},
pages = {811--820},
publisher = {John Wiley {\&} Sons, Ltd.},
title = {{Single Variables Control Charts: an Overview}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/qre.730/abstract},
volume = {22},
year = {2006}
}
@incollection{Alt1985,
author = {Alt, Frank B.},
booktitle = {Encycl. Stat. Sci.},
editor = {{S. Kotz N. L. Johnson}},
pages = {110--122},
publisher = {Wiley},
title = {{Multivariate quality control}},
year = {1985}
}
@article{Chen2005,
author = {Chen, Gemai and Cheng, Smiley W and Xie, Hansheng},
issn = {0361-0918},
journal = {Commun. Stat. Comput.},
number = {1},
pages = {203--217},
publisher = {Taylor {\&} Francis},
title = {{A new multivariate control chart for monitoring both location and dispersion}},
volume = {34},
year = {2005}
}
@article{Cheng2016,
abstract = {AbstractA single multivariate control chart referred to as the Max-MCUSUM chart that is capable of simultaneously detecting shifts in both mean vector and covariance matrix of a multivariate process is proposed. This chart is based on standardizing the sample mean vectors and covariance matrices and finding the sampling distribution of the standardized values. This chart is compared with the Max-MEWMA chart proposed by Xie [19]. The proposed chart performs better than the Max-MEWMA chart in detecting small shifts in the process spread and location.},
author = {Cheng, Smiley W. and Thaga, Keoagile},
doi = {10.1080/16843703.2005.11673095},
issn = {1684-3703},
journal = {Qual. Technol. Quant. Manag.},
keywords = {ARL,CUSUM,SPC,multivariate SPC,power,simultaneous monitoring},
language = {en},
month = {feb},
number = {2},
pages = {221--235},
publisher = {Taylor {\&} Francis},
title = {{Multivariate Max-CUSUM Chart}},
url = {http://www.tandfonline.com/doi/abs/10.1080/16843703.2005.11673095{\#}.V7MGAfGwONE.mendeley},
volume = {2},
year = {2005}
}
@misc{Kim2012,
author = {Kim, Min-Jung},
file = {:Users/ed/Downloads/Min-Jung{\_}Kim{\_}Dissertation-2.pdf:pdf},
publisher = {The Pennsylvania State University},
title = {{Statistical Quality Methods to Monitor and Transform Healthcare Data}},
year = {2012}
}
@article{Page1954,
author = {Page, E S},
issn = {0006-3444},
journal = {Biometrika},
number = {1/2},
pages = {100--115},
publisher = {JSTOR},
title = {{Continuous inspection schemes}},
volume = {41},
year = {1954}
}
@article{Alt1988,
abstract = {There are many situations in which it is necessary to simultaneously monitor two or more correlated quality characteristics. Such problems are referred to as multivariate quality control problems. A manufacturing plant where the product is plastic film is considered to illustrate the need for a multivariate approach. The usefulness of the film depends on its transparency and its tear resistance. These two quality characteristics are jointly distributed as a bivariate normal, and is obtained and plotted against time on an EQ $\backslash$x $\backslash$to(x) -chart. The use of separate EQ $\backslash$x $\backslash$to(x) -charts is equivalent to plotting on a single chart formed by superimposing one EQ $\backslash$x $\backslash$to(x) -chart over the other. The use of separate control charts or the equivalent rectangular region can be misleading. It will be shortly demonstrated that the true control region is elliptical in nature, and the process is judged out of control only if the pair of means plots outside this elliptical region.},
author = {Alt, Frank B. and Smith, Nancy D.},
doi = {10.1016/S0169-7161(88)07019-1},
isbn = {9780444702906},
issn = {01697161},
journal = {Handb. Stat.},
pages = {333--351},
title = {{17 Multivariate process control}},
volume = {7},
year = {1988}
}
@article{Samuel1998,
author = {Samuel, Thomas R and {Pignatiello Jr}, Joseph J and Calvin, James A},
issn = {0898-2112},
journal = {Qual. Eng.},
number = {3},
pages = {521--527},
publisher = {Taylor {\&} Francis},
title = {{Identifying the time of a step change with X control charts}},
volume = {10},
year = {1998}
}
@article{Bennett2015,
author = {Bennett, Keiryn L and Wang, Xia and Bystrom, Cory E and Chambers, Matthew C and Andacht, Tracy M and Dangott, Larry J and Elortza, F{\'{e}}lix and Leszyk, John and Molina, Henrik and Moritz, Robert L},
file = {:Users/ed/Dropbox/10. Proteomics SPC/2015-Bennett-ABRF Proteomic Research Group Study.pdf:pdf},
issn = {1535-9476},
journal = {Mol. Cell. Proteomics},
number = {12},
pages = {3299--3309},
publisher = {ASBMB},
title = {{The 2012/2013 ABRF Proteomic Research Group Study: Assessing longitudinal intralaboratory variability in routine peptide liquid chromatography tandem mass spectrometry analyses}},
volume = {14},
year = {2015}
}
@article{Westgard1979,
author = {Westgard, J O and Groth, T},
issn = {0009-9147},
journal = {Clin. Chem.},
number = {6},
pages = {863--869},
publisher = {Am Assoc Clin Chem},
title = {{Power functions for statistical control rules.}},
volume = {25},
year = {1979}
}
@book{Hawkins2012,
author = {Hawkins, Douglas M and Olwell, David H},
isbn = {1461216869},
publisher = {Springer Science {\&} Business Media},
title = {{Cumulative sum charts and charting for quality improvement}},
year = {2012}
}
@article{Samuel1998a,
author = {Samuel, Thomas R and {Pignatiello Jr}, Joseph J and Calvin, James A},
issn = {0898-2112},
journal = {Qual. Eng.},
number = {3},
pages = {529--538},
publisher = {Taylor {\&} Francis},
title = {{Identifying the time of a step change in a normal process variance}},
volume = {10},
year = {1998}
}
@article{Alemi2004,
author = {Alemi, Farrokh},
issn = {1063-8628},
journal = {Qual. Manag. Healthc.},
number = {4},
pages = {216--221},
publisher = {LWW},
title = {{Tukey's control chart}},
volume = {13},
year = {2004}
}
@article{Wheeler2010,
author = {Wheeler, Donald},
file = {:Users/ed/Downloads/DJW206.pdf:pdf},
journal = {Qual. Dig.},
title = {{Individual charts done right and wrong}},
volume = {2},
year = {2010}
}
@article{Quesenberry1991,
author = {Quesenberry, Charles P},
journal = {J. Qual. Technol.},
number = {3},
pages = {213--224},
title = {{SPC Q charts for start-up processes and short or long runs}},
volume = {23},
year = {1991}
}
@article{Hawkins1987,
author = {Hawkins, Douglas M},
file = {:Users/ed/Downloads/Self starting .pdf:pdf},
issn = {0039-0526},
journal = {Stat.},
pages = {299--316},
publisher = {JSTOR},
title = {{Self-starting CUSUM charts for location and scale}},
year = {1987}
}
@book{Montgomery2007,
author = {Montgomery, Douglas C},
isbn = {812651471X},
publisher = {John Wiley {\&} Sons},
title = {{Introduction to statistical quality control}},
year = {2007}
}
@book{Quesenberry1997,
author = {Quesenberry, Charles P},
isbn = {0471130877},
publisher = {John Wiley {\&} Sons},
title = {{SPC methods for quality improvement}},
year = {1997}
}
@article{Sharma2014a,
author = {Sharma, Vagisha and Eckels, Josh and Taylor, Greg K and Shulman, Nicholas J and Stergachis, Andrew B and Joyner, Shannon A and Yan, Ping and Whiteaker, Jeffrey R and Halusa, Goran N and Schilling, Birgit},
issn = {1535-3893},
journal = {J. Proteome Res.},
number = {9},
pages = {4205--4210},
publisher = {ACS Publications},
title = {{Panorama: a targeted proteomics knowledge base}},
volume = {13},
year = {2014}
}
@article{LEVEY1950,
author = {Levey, Stanley and Jennings, E R},
issn = {0002-9173},
journal = {Am. J. Clin. Pathol.},
number = {11},
pages = {1059},
title = {{The use of control charts in the clinical laboratory.}},
volume = {20},
year = {1950}
}
@article{Bell2009,
annote = {10.1038/nmeth.1333},
author = {Bell, Alexander W and Deutsch, Eric W and Au, Catherine E and Kearney, Robert E and Beavis, Ron and Sechi, Salvatore and Nilsson, Tommy and Bergeron, John J M},
issn = {1548-7091},
journal = {Nat Meth},
month = {jun},
number = {6},
pages = {423--430},
publisher = {Nature Publishing Group},
title = {{A HUPO test sample study reveals common problems in mass spectrometry-based proteomics}},
url = {http://dx.doi.org/10.1038/nmeth.1333 http://www.nature.com/nmeth/journal/v6/n6/suppinfo/nmeth.1333{\_}S1.html},
volume = {6},
year = {2009}
}
@article{Shewhart1938,
author = {Shewhart, W.A.},
doi = {10.1016/S0016-0032(38)90436-3},
issn = {00160032},
journal = {J. Franklin Inst.},
month = {aug},
number = {2},
pages = {163--186},
publisher = {Pergamon},
title = {{Application of statistical methods to manufacturing problems}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0016003238904363},
volume = {226},
year = {1938}
}
@article{Shewhart1924,
author = {Shewhart, W. A.},
doi = {10.1002/j.1538-7305.1924.tb01347.x},
issn = {00058580},
journal = {Bell Syst. Tech. J.},
month = {jan},
number = {1},
pages = {43--87},
title = {{Some Applications of Statistical Methods to the Analysis of Physical and Engineering Data}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6767776},
volume = {3},
year = {1924}
}
@article{Sharma2014,
abstract = {Panorama is a web application for storing, sharing, analyzing, and reusing targeted assays created and refined with Skyline,1 an increasingly popular Windows client software tool for targeted proteomics experiments. Panorama allows laboratories to store and organize curated results contained in Skyline documents with fine-grained permissions, which facilitates distributed collaboration and secure sharing of published and unpublished data via a web-browser interface. It is fully integrated with the Skyline workflow and supports publishing a document directly to a Panorama server from the Skyline user interface. Panorama captures the complete Skyline document information content in a relational database schema. Curated results published to Panorama can be aggregated and exported as chromatogram libraries. These libraries can be used in Skyline to pick optimal targets in new experiments and to validate peak identification of target peptides. Panorama is open-source and freely available. It is distributed as part of LabKey Server,2 an open source biomedical research data management system. Laboratories and organizations can set up Panorama locally by downloading and installing the software on their own servers. They can also request freely hosted projects on https://panoramaweb.org , a Panorama server maintained by the Department of Genome Sciences at the University of Washington.},
author = {Sharma, Vagisha and Eckels, Josh and Taylor, Greg K and Shulman, Nicholas J and Stergachis, Andrew B and Joyner, Shannon A and Yan, Ping and Whiteaker, Jeffrey R and Halusa, Goran N and Schilling, Birgit and Gibson, Bradford W and Colangelo, Christopher M and Paulovich, Amanda G and Carr, Steven A and Jaffe, Jacob D and MacCoss, Michael J and MacLean, Brendan},
doi = {10.1021/pr5006636},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Sharma et al. - 2014 - Panorama a targeted proteomics knowledge base.pdf:pdf},
issn = {1535-3907},
journal = {J. Proteome Res.},
keywords = {Databases, Protein,Internet,Knowledge Bases,Mass Spectrometry,Proteomics,Proteomics: methods,Software},
language = {EN},
month = {sep},
number = {9},
pages = {4205--10},
pmid = {25102069},
publisher = {American Chemical Society},
title = {{Panorama: a targeted proteomics knowledge base.}},
url = {http://pubs.acs.org/doi/abs/10.1021/pr5006636},
volume = {13},
year = {2014}
}
@article{MacLean2010,
abstract = {SUMMARY: Skyline is a Windows client application for targeted proteomics method creation and quantitative data analysis. It is open source and freely available for academic and commercial use. The Skyline user interface simplifies the development of mass spectrometer methods and the analysis of data from targeted proteomics experiments performed using selected reaction monitoring (SRM). Skyline supports using and creating MS/MS spectral libraries from a wide variety of sources to choose SRM filters and verify results based on previously observed ion trap data. Skyline exports transition lists to and imports the native output files from Agilent, Applied Biosystems, Thermo Fisher Scientific and Waters triple quadrupole instruments, seamlessly connecting mass spectrometer output back to the experimental design document. The fast and compact Skyline file format is easily shared, even for experiments requiring many sample injections. A rich array of graphs displays results and provides powerful tools for inspecting data integrity as data are acquired, helping instrument operators to identify problems early. The Skyline dynamic report designer exports tabular data from the Skyline document model for in-depth analysis with common statistical tools.

AVAILABILITY: Single-click, self-updating web installation is available at http://proteome.gs.washington.edu/software/skyline. This web site also provides access to instructional videos, a support board, an issues list and a link to the source code project.},
author = {MacLean, Brendan and Tomazela, Daniela M and Shulman, Nicholas and Chambers, Matthew and Finney, Gregory L and Frewen, Barbara and Kern, Randall and Tabb, David L and Liebler, Daniel C and MacCoss, Michael J},
doi = {10.1093/bioinformatics/btq054},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/MacLean et al. - 2010 - Skyline an open source document editor for creating and analyzing targeted proteomics experiments.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics},
keywords = {Databases, Protein,Mass Spectrometry,Mass Spectrometry: methods,Proteomics,Proteomics: methods,Software,User-Computer Interface},
month = {apr},
number = {7},
pages = {966--8},
pmid = {20147306},
title = {{Skyline: an open source document editor for creating and analyzing targeted proteomics experiments.}},
url = {http://bioinformatics.oxfordjournals.org/content/26/7/966.short},
volume = {26},
year = {2010}
}
@article{Ozlu,
abstract = {Current analytical protein methods show phosphorylation to be the most ubiquitous, evolutionary conserved post-translational modification Post-Translational Modification (PTM). The reversible and transient nature of protein phosphorylation allows signal transduction pathways to carry out diverse cellular functions. From bacteria to humans, phosphorylation serves to modify protein function by altering protein stability, cellular location, substrate affinity, complex formation, and activity; thus allowing essential events such as cell cycle and growth to occur at precise times and locations. Phosphorylation controls a variety of events at many biological levels including: housekeeping activities controlled by single cells such as DNA transcription, cell-cycle regulation, and energy metabolism; and cellular processes that involve signaling between cells or the environment including such as neuronal migration and immune system recognition. This review summarizes state-of-the-art proteomics technologies available to study phosphorylation in biological systems. We highlight the tremendous steps the field has made in the last 5 years which allow quantitative global analyses while pointing out caveats in experimentation.},
author = {Ozlu, Nurhan and Akten, Bikem and Timm, Wiebke and Haseley, Nathan and Steen, Hanno and Steen, Judith A J},
doi = {10.1002/wsbm.41},
issn = {1939-005X},
journal = {Wiley Interdiscip. Rev. Syst. Biol. Med.},
keywords = {Animals,Humans,Metabolome,Metabolome: physiology,Models, Biological,Phosphoproteins,Phosphoproteins: metabolism,Phosphorylation,Phosphorylation: physiology,Proteome,Proteome: metabolism,Proteomics,Proteomics: methods},
month = {jan},
number = {3},
pages = {255--76},
pmid = {20836028},
title = {{Phosphoproteomics.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20836028},
volume = {2}
}
@article{Vorkas2015,
abstract = {Metabolic profiling studies aim to achieve broad metabolome coverage in specific biological samples. However, wide metabolome coverage has proven difficult to achieve, mostly because of the diverse physicochemical properties of small molecules, obligating analysts to seek multiplatform and multimethod approaches. Challenges are even greater when it comes to applications to tissue samples, where tissue lysis and metabolite extraction can induce significant systematic variation in composition. We have developed a pipeline for obtaining the aqueous and organic compounds from diseased arterial tissue using two consecutive extractions, followed by a different untargeted UPLC-MS analysis method for each extract. Methods were rationally chosen and optimized to address the different physicochemical properties of each extract: hydrophilic interaction liquid chromatography (HILIC) for the aqueous extract and reversed-phase chromatography for the organic. This pipeline can be generic for tissue analysis as demonstrated by applications to different tissue types. The experimental setup and fast turnaround time of the two methods contributed toward obtaining highly reproducible features with exceptional chromatographic performance (CV {\%} {\textless} 0.5{\%}), making this pipeline suitable for metabolic profiling applications. We structurally assigned 226 metabolites from a range of chemical classes (e.g., carnitines, $\alpha$-amino acids, purines, pyrimidines, phospholipids, sphingolipids, free fatty acids, and glycerolipids) which were mapped to their corresponding pathways, biological functions and known disease mechanisms. The combination of the two untargeted UPLC-MS methods showed high metabolite complementarity. We demonstrate the application of this pipeline to cardiovascular disease, where we show that the analyzed diseased groups (n = 120) of arterial tissue could be distinguished based on their metabolic profiles.},
author = {Vorkas, Panagiotis A and Isaac, Giorgis and Anwar, Muzaffar A and Davies, Alun H and Want, Elizabeth J and Nicholson, Jeremy K and Holmes, Elaine},
doi = {10.1021/ac503775m},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Vorkas et al. - 2015 - Untargeted UPLC-MS profiling pipeline to expand tissue metabolome coverage application to cardiovascular disease.pdf:pdf},
issn = {1520-6882},
journal = {Anal. Chem.},
keywords = {Amino Acids,Amino Acids: analysis,Amino Acids: metabolism,Arteries,Arteries: chemistry,Arteries: metabolism,Cardiovascular Diseases,Carnitine,Carnitine: analysis,Carnitine: metabolism,Chromatography, High Pressure Liquid,Chromatography, High Pressure Liquid: instrumentat,Fatty Acids,Fatty Acids: analysis,Fatty Acids: metabolism,Lipids,Lipids: analysis,Mass Spectrometry,Mass Spectrometry: instrumentation,Purines,Purines: analysis,Purines: metabolism,Pyrimidines,Pyrimidines: analysis,Pyrimidines: metabolism},
month = {apr},
number = {8},
pages = {4184--93},
pmid = {25664760},
title = {{Untargeted UPLC-MS profiling pipeline to expand tissue metabolome coverage: application to cardiovascular disease.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4407508{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {87},
year = {2015}
}
@article{Eren-dogu2013,
author = {Eren-dogu, Zeynep Filiz and Dogu, Eralp},
file = {:Users/ed/Dropbox/451-1419538651.pdf:pdf},
keywords = {control charts,cusum,or efficiency,surgery time,utilization},
number = {2},
title = {{MONITORING THE EFFICIENCY OF USE OF OPERATING ROOM TIME WITH}},
volume = {2},
year = {2013}
}
@article{Egertson2015,
abstract = {Here we describe the use of data-independent acquisition (DIA) on a Q-Exactive mass spectrometer for the detection and quantification of peptides in complex mixtures using the Skyline Targeted Proteomics Environment (freely available online at http://skyline.maccosslab.org). The systematic acquisition of mass spectrometry (MS) or tandem MS (MS/MS) spectra by DIA is in contrast to DDA, in which the acquired MS/MS spectra are only suitable for the identification of a stochastically sampled set of peptides. Similarly to selected reaction monitoring (SRM), peptides can be quantified from DIA data using targeted chromatogram extraction. Unlike SRM, data acquisition is not constrained to a predetermined set of target peptides. In this protocol, a spectral library is generated using data-dependent acquisition (DDA), and chromatograms are extracted from the DIA data for all peptides in the library. As in SRM, quantification using DIA data is based on the area under the curve of extracted MS/MS chromatograms. In addition, a quality control (QC) method suitable for DIA based on targeted MS/MS acquisition is detailed. Not including time spent acquiring data, and time for database searching, the procedure takes ∼1-2 h to complete. Typically, data acquisition requires roughly 1-4 h per sample, and a database search will take 0.5-2 h to complete.},
author = {Egertson, Jarrett D and MacLean, Brendan and Johnson, Richard and Xuan, Yue and MacCoss, Michael J},
doi = {10.1038/nprot.2015.055},
issn = {1750-2799},
journal = {Nat. Protoc.},
keywords = {Mass Spectrometry,Peptide Library,Peptides,Peptides: analysis,Quality Control,Software},
language = {en},
month = {jun},
number = {6},
pages = {887--903},
pmid = {25996789},
publisher = {Nature Publishing Group},
title = {{Multiplexed peptide analysis using data-independent acquisition and Skyline.}},
url = {http://www.nature.com.ezproxy.neu.edu/nprot/journal/v10/n6/full/nprot.2015.055.html{\#}f9},
volume = {10},
year = {2015}
}
@article{Walzer2014,
abstract = {Quality control is increasingly recognized as a crucial aspect of mass spectrometry based proteomics. Several recent papers discuss relevant parameters for quality control and present applications to extract these from the instrumental raw data. What has been missing, however, is a standard data exchange format for reporting these performance metrics. We therefore developed the qcML format, an XML-based standard that follows the design principles of the related mzML, mzIdentML, mzQuantML, and TraML standards from the HUPO-PSI (Proteomics Standards Initiative). In addition to the XML format, we also provide tools for the calculation of a wide range of quality metrics as well as a database format and interconversion tools, so that existing LIMS systems can easily add relational storage of the quality control data to their existing schema. We here describe the qcML specification, along with possible use cases and an illustrative example of the subsequent analysis possibilities. All information about qcML is available at http://code.google.com/p/qcml.},
author = {Walzer, Mathias and Pernas, Lucia Espona and Nasso, Sara and Bittremieux, Wout and Nahnsen, Sven and Kelchtermans, Pieter and Pichler, Peter and van den Toorn, Henk W P and Staes, An and Vandenbussche, Jonathan and Mazanek, Michael and Taus, Thomas and Scheltema, Richard A and Kelstrup, Christian D and Gatto, Laurent and van Breukelen, Bas and Aiche, Stephan and Valkenborg, Dirk and Laukens, Kris and Lilley, Kathryn S and Olsen, Jesper V and Heck, Albert J R and Mechtler, Karl and Aebersold, Ruedi and Gevaert, Kris and Vizca{\'{i}}no, Juan Antonio and Hermjakob, Henning and Kohlbacher, Oliver and Martens, Lennart},
doi = {10.1074/mcp.M113.035907},
issn = {1535-9484},
journal = {Mol. Cell. Proteomics},
keywords = {Databases, Protein,Mass Spectrometry,Mass Spectrometry: standards,Programming Languages,Proteomics,Proteomics: standards,Quality Control,Software},
month = {aug},
number = {8},
pages = {1905--13},
pmid = {24760958},
title = {{qcML: an exchange format for quality control metrics from mass spectrometry experiments.}},
url = {http://www.mcponline.org/content/13/8/1905.full},
volume = {13},
year = {2014}
}
@article{Bielow2015a,
abstract = {Mass spectrometry-based proteomics coupled to liquid chromatography has matured into an automatized, high-throughput technology, producing data on the scale of multiple gigabytes per instrument per day. Consequently, an automated quality control (QC) and quality analysis (QA) capable of detecting measurement bias, verifying consistency, and avoiding propagation of error is paramount for instrument operators and scientists in charge of downstream analysis. We have developed an R-based QC pipeline called Proteomics Quality Control (PTXQC) for bottom-up LC?MS data generated by the MaxQuant1 software pipeline. PTXQC creates a QC report containing a comprehensive and powerful set of QC metrics, augmented with automated scoring functions. The automated scores are collated to create an overview heatmap at the beginning of the report, giving valuable guidance also to nonspecialists. Our software supports a wide range of experimental designs, including stable isotope labeling by amino acids in cell culture (SILAC), tandem mass tags (TMT), and label-free data. Furthermore, we introduce new metrics to score MaxQuant?s Match-between-runs (MBR) functionality by which peptide identifications can be transferred across Raw files based on accurate retention time and m/z. Last but not least, PTXQC is easy to install and use and represents the first QC software capable of processing MaxQuant result tables. PTXQC is freely available at https://github.com/cbielow/PTXQC.
Mass spectrometry-based proteomics coupled to liquid chromatography has matured into an automatized, high-throughput technology, producing data on the scale of multiple gigabytes per instrument per day. Consequently, an automated quality control (QC) and quality analysis (QA) capable of detecting measurement bias, verifying consistency, and avoiding propagation of error is paramount for instrument operators and scientists in charge of downstream analysis. We have developed an R-based QC pipeline called Proteomics Quality Control (PTXQC) for bottom-up LC?MS data generated by the MaxQuant1 software pipeline. PTXQC creates a QC report containing a comprehensive and powerful set of QC metrics, augmented with automated scoring functions. The automated scores are collated to create an overview heatmap at the beginning of the report, giving valuable guidance also to nonspecialists. Our software supports a wide range of experimental designs, including stable isotope labeling by amino acids in cell culture (SILAC), tandem mass tags (TMT), and label-free data. Furthermore, we introduce new metrics to score MaxQuant?s Match-between-runs (MBR) functionality by which peptide identifications can be transferred across Raw files based on accurate retention time and m/z. Last but not least, PTXQC is easy to install and use and represents the first QC software capable of processing MaxQuant result tables. PTXQC is freely available at https://github.com/cbielow/PTXQC.},
author = {Bielow, Chris and Mastrobuoni, Guido and Kempa, Stefan},
doi = {10.1021/acs.jproteome.5b00780},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Bielow, Mastrobuoni, Kempa - 2015 - Proteomics Quality Control Quality Control Software for MaxQuant Results.pdf:pdf},
issn = {1535-3893},
journal = {J. Proteome Res.},
month = {dec},
number = {3},
pages = {acs.jproteome.5b00780},
publisher = {American Chemical Society},
title = {{Proteomics Quality Control: Quality Control Software for MaxQuant Results}},
url = {http://dx.doi.org/10.1021/acs.jproteome.5b00780},
volume = {15},
year = {2015}
}
@article{Bittremieux2016,
author = {Bittremieux, Wout and Meysman, Pieter and Martens, Lennart and Valkenborg, Dirk and Laukens, Kris},
doi = {10.1021/acs.jproteome.6b00028},
file = {:Users/ed/Dropbox/10. Proteomics SPC/2016-Bitter-Unsupervised Quality Assessment of Mass Spectrometry Proteomics Experiments by Multivariate Quality Control Metrics.pdf:pdf},
issn = {1535-3893},
journal = {J. Proteome Res.},
keywords = {mass spectrometry,outlier detection,outlier interpretation,proteomics,quality assessment,quality control},
pages = {acs.jproteome.6b00028},
pmid = {26974716},
title = {{Unsupervised Quality Assessment of Mass Spectrometry Proteomics Experiments by Multivariate Quality Control Metrics}},
url = {http://pubs.acs.org/doi/abs/10.1021/acs.jproteome.6b00028},
year = {2016}
}
@article{Abbatiello2013,
abstract = {Multiple reaction monitoring (MRM) mass spectrometry coupled with stable isotope dilution (SID) and liquid chromatography (LC) is increasingly used in biological and clinical studies for precise and reproducible quantification of peptides and proteins in complex sample matrices. Robust LC-SID-MRM-MS-based assays that can be replicated across laboratories and ultimately in clinical laboratory settings require standardized protocols to demonstrate that the analysis platforms are performing adequately. We developed a system suitability protocol (SSP), which employs a predigested mixture of six proteins, to facilitate performance evaluation of LC-SID-MRM-MS instrument platforms, configured with nanoflow-LC systems interfaced to triple quadrupole mass spectrometers. The SSP was designed for use with low multiplex analyses as well as high multiplex approaches when software-driven scheduling of data acquisition is required. Performance was assessed by monitoring of a range of chromatographic and mass spectrometric metrics including peak width, chromatographic resolution, peak capacity, and the variability in peak area and analyte retention time (RT) stability. The SSP, which was evaluated in 11 laboratories on a total of 15 different instruments, enabled early diagnoses of LC and MS anomalies that indicated suboptimal LC-MRM-MS performance. The observed range in variation of each of the metrics scrutinized serves to define the criteria for optimized LC-SID-MRM-MS platforms for routine use, with pass/fail criteria for system suitability performance measures defined as peak area coefficient of variation {\textless}0.15, peak width coefficient of variation {\textless}0.15, standard deviation of RT {\textless}0.15 min (9 s), and the RT drift {\textless}0.5min (30 s). The deleterious effect of a marginally performing LC-SID-MRM-MS system on the limit of quantification (LOQ) in targeted quantitative assays illustrates the use and need for a SSP to establish robust and reliable system performance. Use of a SSP helps to ensure that analyte quantification measurements can be replicated with good precision within and across multiple laboratories and should facilitate more widespread use of MRM-MS technology by the basic biomedical and clinical laboratory research communities.},
author = {Abbatiello, Susan E and Mani, D R and Schilling, Birgit and Maclean, Brendan and Zimmerman, Lisa J and Feng, Xingdong and Cusack, Michael P and Sedransk, Nell and Hall, Steven C and Addona, Terri and Allen, Simon and Dodder, Nathan G and Ghosh, Mousumi and Held, Jason M and Hedrick, Victoria and Inerowicz, H Dorota and Jackson, Angela and Keshishian, Hasmik and Kim, Jong Won and Lyssand, John S and Riley, C Paige and Rudnick, Paul and Sadowski, Pawel and Shaddox, Kent and Smith, Derek and Tomazela, Daniela and Wahlander, Asa and Waldemarson, Sofia and Whitwell, Corbin a and You, Jinsam and Zhang, Shucha and Kinsinger, Christopher R and Mesri, Mehdi and Rodriguez, Henry and Borchers, Christoph H and Buck, Charles and Fisher, Susan J and Gibson, Bradford W and Liebler, Daniel and Maccoss, Michael and Neubert, Thomas a and Paulovich, Amanda and Regnier, Fred and Skates, Steven J and Tempst, Paul and Wang, Mu and Carr, Steven a},
doi = {10.1074/mcp.M112.027078},
file = {:Users/ed/Dropbox/10. Proteomics SPC/2013-Abbatiello{\_}MCP.pdf:pdf},
isbn = {1535-9484 (Electronic)$\backslash$r1535-9476 (Linking)},
issn = {1535-9484},
journal = {Mol. Cell. Proteomics},
keywords = {Amino Acid Sequence,Animals,Cattle,Chromatography,Limit of Detection,Liquid,Liquid: instrumentation,Liquid: methods,Mass Spectrometry,Mass Spectrometry: instrumentation,Mass Spectrometry: methods,Molecular Sequence Data,Peptides,Peptides: chemistry,Peptides: metabolism,Reference Standards,Software,Time Factors},
number = {9},
pages = {2623--39},
pmid = {23689285},
title = {{Design, implementation and multisite evaluation of a system suitability protocol for the quantitative assessment of instrument performance in liquid chromatography-multiple reaction monitoring-MS (LC-MRM-MS).}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23689285},
volume = {12},
year = {2013}
}
@article{Bittremieux2016b,
author = {Bittremieux, Wout and Meysman, Pieter and Martens, Lennart and Valkenborg, Dirk and Laukens, Kris},
doi = {10.1021/acs.jproteome.6b00028},
file = {:Users/ed/Dropbox/10. Proteomics SPC/2016-Bitter-Unsupervised Quality Assessment of Mass Spectrometry Proteomics Experiments by Multivariate Quality Control Metrics Supporting Document.pdf:pdf},
issn = {1535-3893},
journal = {J. Proteome Res.},
pages = {acs.jproteome.6b00028},
pmid = {26974716},
title = {{Unsupervised Quality Assessment of Mass Spectrometry Proteomics Experiments by Multivariate Quality Control Metrics}},
url = {http://pubs.acs.org/doi/abs/10.1021/acs.jproteome.6b00028},
year = {2016}
}
@article{Abbatiello2015,
abstract = {There is an increasing need in biology and clinical medicine to robustly and reliably measure tens-to-hundreds of peptides and proteins in clinical and biological samples with high sensitivity, specificity, reproducibility and repeatability. Previously, we demonstrated that LC-MRM-MS with isotope dilution has suitable performance for quantitative measurements of small numbers of relatively abundant proteins in human plasma, and that the resulting assays can be transferred across laboratories while maintaining high reproducibility and quantitative precision. Here we significantly extend that earlier work, demonstrating that 11 laboratories using 14 LC-MS systems can develop, determine analytical figures of merit, and apply highly multiplexed MRM-MS assays targeting 125 peptides derived from 27 cancer-relevant proteins and 7 control proteins to precisely and reproducibly measure the analytes in human plasma. To ensure consistent generation of high quality data we incorporated a system suitability protocol (SSP) into our experimental design. The SSP enabled real-time monitoring of LC-MRM-MS performance during assay development and implementation, facilitating early detection and correction of chromatographic and instrumental problems. Low to sub-nanogram/mL sensitivity for proteins in plasma was achieved by one-step immunoaffinity depletion of 14 abundant plasma proteins prior to analysis. Median intra- and inter-laboratory reproducibility was {\textless}20{\%}, sufficient for most biological studies and candidate protein biomarker verification. Digestion recovery of peptides was assessed and quantitative accuracy improved using heavy isotope labeled versions of the proteins as internal standards. Using the highly multiplexed assay, participating laboratories were able to precisely and reproducibly determine the levels of a series of analytes in blinded samples used to simulate an inter-laboratory clinical study of patient samples. Our study further establishes that LC-MRM-MS using stable isotope dilution, with appropriate attention to analytical validation and appropriate quality c`ontrol measures, enables sensitive, specific, reproducible and quantitative measurements of proteins and peptides in complex biological matrices such as plasma.},
author = {Abbatiello, Susan E and Schilling, Birgit and Mani, D R and Zimmerman, Lisa J and Hall, Steven C and MacLean, Brendan and Albertolle, Matthew and Allen, Simon and Burgess, Michael and Cusack, Michael P and Ghosh, Mousumi and Hedrick, Victoria and Held, Jason M and Inerowicz, H Dorota and Jackson, Angela and Keshishian, Hasmik and Kinsinger, Christopher R and Lyssand, John and Makowski, Lee and Mesri, Mehdi and Rodriguez, Henry and Rudnick, Paul and Sadowski, Pawel and Sedransk, Nell and Shaddox, Kent and Skates, Stephen J and Kuhn, Eric and Smith, Derek and Whiteaker, Jeffery R and Whitwell, Corbin and Zhang, Shucha and Borchers, Christoph H and Fisher, Susan J and Gibson, Bradford W and Liebler, Daniel C and MacCoss, Michael J and Neubert, Thomas A and Paulovich, Amanda G and Regnier, Fred E and Tempst, Paul and Carr, Steven A},
doi = {10.1074/mcp.M114.047050},
file = {:Users/ed/Dropbox/10. Proteomics SPC/2015-Abbatiello-2357-74.pdf:pdf},
isbn = {1535-9484},
issn = {1535-9484},
journal = {Mol. Cell. Proteomics},
keywords = {absolute quantification,abundant protein depletion,analytical validation,assay development,isotope dilution,labeled protein internal standards,monitoring,multiple reaction,multiplexed,quantification,stable,targeted mass spectrometry},
number = {409},
pages = {M114.047050--},
pmid = {25693799},
title = {{Large-scale inter-laboratory study to develop, analytically validate and apply highly multiplexed, quantitative peptide assays to measure cancer-relevant proteins in plasma.}},
url = {http://www.mcponline.org/content/early/2015/02/18/mcp.M114.047050.short?rss=1{\&}utm{\_}source=feedburner{\&}utm{\_}medium=feed{\&}utm{\_}campaign=Feed:+MCP{\_}PIPs+(MCP+Papers+in+Press)},
volume = {1},
year = {2015}
}
@article{Xiao2015,
abstract = {Questions concerning longitudinal data quality and reproducibility of proteomic laboratories spurred the Protein Research Group of the Association of Biomolecular Resource Facilities (ABRF-PRG) to design a study to systematically assess the reproducibility of proteomic laboratories over an extended period of time. Developed as an open study, initially 64 participants were recruited from the broader mass spectrometry community to analyze provided aliquots of a six bovine protein tryptic digest mixture every month for a period of nine months. Data were uploaded to a central repository, and the operators answered an accompanying survey. Ultimately, 45 laboratories submitted a minimum of eight LC-MSMS raw data files collected in data-dependent acquisition (DDA) mode. No standard operating procedures were enforced; rather the participants were encouraged to analyze the samples according to usual practices in the laboratory. Unlike previous studies, this investigation was not designed to compare laboratories or instrument configuration, but rather to assess the temporal intralaboratory reproducibility. The outcome of the study was reassuring with 80{\%} of the participating laboratories performing analyses at a medium to high level of reproducibility and quality over the 9-month period. For the groups that had one or more outlying experiments, the major contributing factor that correlated to the survey data was the performance of preventative maintenance prior to the LC-MSMS analyses. Thus, the Protein Research Group of the Association of Biomolecular Resource Facilities recommends that laboratories closely scrutinize the quality control data following such events. Additionally, improved quality control recording is imperative. This longitudinal study provides evidence that mass spectrometry-based proteomics is reproducible. When quality control measures are strictly adhered to, such reproducibility is comparable among many disparate groups. Data from the study are available via ProteomeXchange under the accession code PXD002114.},
author = {Xiao, Z and Chang, Jg and Hendriks, Ivo a. and Sigursson, J{\'{o}}nOtti and Olsen, Jesper V. and Vertegaal, A.C.O.},
doi = {10.1074/mcp.O115.051888},
file = {:Users/ed/Dropbox/10. Proteomics SPC/2015-Bennett-ABRF Proteomic Research Group Study.pdf:pdf},
isbn = {1535-9476},
issn = {1535-9476},
journal = {Mol. Cell. Proteomics},
pages = {3299--3309},
pmid = {26435129},
title = {{The 2012 ABRF Proteomic Research Group Study: Assessing Longitudinal Intra-Laboratory Variability in Routine Peptide Liquid Chromatography Tandem Mass Spectrometry Analyses}},
year = {2015}
}
@misc{,
file = {:Users/ed/Dropbox/10. Proteomics SPC/2013-Taylor et al-Metriculator quality assessment for mass spectrometry based proteomics.pdf:pdf},
title = {{2013-Taylor et al-Metriculator quality assessment for mass spectrometry based proteomics.pdf}}
}
@article{Pichler2012,
abstract = {While the performance of liquid chromatography (LC) and mass spectrometry (MS) instrumentation continues to increase, applications such as analyses of complete or near-complete proteomes and quantitative studies require constant and optimal system performance. For this reason, research laboratories and core facilities alike are recommended to implement quality control (QC) measures as part of their routine workflows. Many laboratories perform sporadic quality control checks. However, successive and systematic longitudinal monitoring of system performance would be facilitated by dedicated automatic or semiautomatic software solutions that aid an effortless analysis and display of QC metrics over time. We present the software package SIMPATIQCO (SIMPle AuTomatIc Quality COntrol) designed for evaluation of data from LTQ Orbitrap, Q-Exactive, LTQ FT, and LTQ instruments. A centralized SIMPATIQCO server can process QC data from multiple instruments. The software calculates QC metrics supervising every step of data acquisition from LC and electrospray to MS. For each QC metric the software learns the range indicating adequate system performance from the uploaded data using robust statistics. Results are stored in a database and can be displayed in a comfortable manner from any computer in the laboratory via a web browser. QC data can be monitored for individual LC runs as well as plotted over time. SIMPATIQCO thus assists the longitudinal monitoring of important QC metrics such as peptide elution times, peak widths, intensities, total ion current (TIC) as well as sensitivity, and overall LC-MS system performance; in this way the software also helps identify potential problems. The SIMPATIQCO software package is available free of charge.},
author = {Pichler, Peter and Mazanek, Michael and Dusberger, Frederico and Weilnb{\"{o}}ck, Lisa and Huber, Christian G. and Stingl, Christoph and Luider, Theo M. and Straube, Werner L. and K{\"{o}}cher, Thomas and Mechtler, Karl},
doi = {10.1021/pr300163u},
file = {:Users/ed/Dropbox/10. Proteomics SPC/2012-Pichler et al.-SIMPATIQCO A server based software suite which facilitates monitoring the time course of LC-MS Performance metrics.pdf:pdf},
isbn = {1535-3907 (Electronic)$\backslash$r1535-3893 (Linking)},
issn = {15353893},
journal = {J. Proteome Res.},
keywords = {Orbitrap,mass spectrometry,performance metrics,quality control,shotgun proteomics,software},
number = {11},
pages = {5540--5547},
pmid = {23088386},
title = {{SIMPATIQCO: A server-based software suite which facilitates monitoring the time course of LC-MS performance metrics on orbitrap instruments}},
volume = {11},
year = {2012}
}
@article{Ma2012,
abstract = {LC-MS/MS-based proteomics studies rely on stable analytical system performance that can be evaluated by objective criteria. The National Institute of Standards and Technology (NIST) introduced the MSQC software to compute diverse metrics from experimental LC-MS/MS data, enabling quality analysis and quality control (QA/QC) of proteomics instrumentation. In practice, however, several attributes of the MSQC software prevent its use for routine instrument monitoring. Here, we present QuaMeter, an open-source tool that improves MSQC in several aspects. QuaMeter can directly read raw data from instruments manufactured by different vendors. The software can work with a wide variety of peptide identification software for improved reliability and flexibility. Finally, QC metrics implemented in QuaMeter are rigorously defined and tested. The source code and binary versions of QuaMeter are available under Apache 2.0 License at http://fenchurch.mc.vanderbilt.edu.},
author = {Ma, Ze Qiang and Polzin, Kenneth O. and Dasari, Surendra and Chambers, Matthew C. and Schilling, Birgit and Gibson, Bradford W. and Tran, Bao Q. and Vega-Montoto, Lorenzo and Liebler, Daniel C. and Tabb, David L.},
doi = {10.1021/ac300629p},
file = {:Users/ed/Dropbox/10. Proteomics SPC/2012 Ma et al QuaMeter Multivendor Performance Metrics for LC−MS$\backslash$:MS Proteomics Instrumentation.pdf:pdf},
issn = {00032700},
journal = {Anal. Chem.},
number = {14},
pages = {5845--5850},
pmid = {22697456},
title = {{QuaMeter: Multivendor performance metrics for LC-MS/MS proteomics instrumentation}},
volume = {84},
year = {2012}
}
@article{Bourmaud2015,
abstract = {The harmonization of proteomics experiments facilitates the exchange and comparison of results. The definition of standards and metrics ensures reliable and consistent data quality. An internal quality control procedure was developed to assess the different steps of a proteomic analysis workflow and perform a system suitability test. The method relies on a straightforward protocol using a simple mixture of exogenous proteins, and the sequential addition of two sets of isotopically labeled peptides added to reference samples. This internal quality control procedure was applied to plasma samples to demonstrate its easy implementation, which makes it generic for most proteomics applications.},
author = {Bourmaud, Adele and Gallien, Sebastien and Domon, Bruno},
doi = {10.1016/j.euprot.2015.07.010},
file = {:Users/ed/Dropbox/10. Proteomics SPC/2015-Bourmad-A quality control of proteomic experiments based on multiple isotopologous internal standards.pdf:pdf},
issn = {22129685},
journal = {EuPA Open Proteomics},
keywords = {Internal standard,Isotopically labeled peptides,PRM,Quality control,SRM},
pages = {16--21},
publisher = {Elsevier B.V.},
title = {{A quality control of proteomic experiments based on multiple isotopologous internal standards}},
volume = {8},
year = {2015}
}
@article{Bittremieux2016a,
author = {Bittremieux, Wout and Meysman, Pieter and Martens, Lennart and Valkenborg, Dirk and Laukens, Kris},
doi = {10.1021/acs.jproteome.6b00028},
file = {:Users/ed/Dropbox/10. Proteomics SPC/2016-Bitte-Supporting file.pdf:pdf},
issn = {1535-3893},
journal = {J. Proteome Res.},
pages = {acs.jproteome.6b00028},
pmid = {26974716},
title = {{Unsupervised Quality Assessment of Mass Spectrometry Proteomics Experiments by Multivariate Quality Control Metrics}},
url = {http://pubs.acs.org/doi/abs/10.1021/acs.jproteome.6b00028},
year = {2016}
}
@article{Bielow2015,
abstract = {Mass spectrometry-based proteomics coupled to liquid chromatography has matured into an automatized, high-throughput technology, producing data on the scale of multiple gigabytes per instrument per day. Consequently, an automated quality control (QC) and quality analysis (QA), capable of detecting measurement bias, verifying consistency and avoiding propagation of error is paramount for instrument operators and scientists in charge of downstream analysis. We have developed an R-based quality control pipeline called Proteomics Quality Control (PTXQC) for bottom-up LC?MS data generated by the MaxQuant software pipeline. PTXQC creates a quality control report containing a comprehensive and powerful set of quality control metrics, augmented with automated scoring functions. The automated scores are collated to create an overview heatmap at the beginning of the report, giving valuable guidance also to non-specialists. Our software supports a wide range of experimental designs, including stable isotope labeling by amino acids in cell culture (SILAC), Tandem Mass Tags (TMT) and label-free data. Furthermore, we introduce new metrics to score MaxQuant?s Match-between-runs (MBR) functionality by which peptide identifications can be transferred across Raw files based on accurate RT and m/z. Last but not least, PTXQC is easy to install and use and represents the first QC software capable of processing MaxQuant result tables. PTXQC is freely available at https://github.com/cbielow/PTXQC.},
author = {Bielow, Chris and Mastrobuoni, Guido and Kempa, Stefan},
doi = {10.1021/acs.jproteome.5b00780},
file = {:Users/ed/Dropbox/10. Proteomics SPC/2015-Bielow et al- Proteomics Quality Control – A Quality Control Software for MaxQuant Results.pdf:pdf},
issn = {1535-3893},
journal = {J. Proteome Res.},
keywords = {maxquant,ptxqc,quality control},
pages = {acs.jproteome.5b00780},
pmid = {26653327},
title = {{Proteomics Quality Control – A Quality Control Software for MaxQuant Results}},
url = {http://pubs.acs.org/doi/10.1021/acs.jproteome.5b00780},
year = {2015}
}
@article{Sandin2014,
abstract = {Protein quantification using different LC-MS techniques is becoming a standard practice. However, with a multitude of experimental setups to choose from, as well as a wide array of software solutions for subsequent data processing, it is non-trivial to select the most appropriate workflow for a given biological question. In this review, we highlight different issues that need to be addressed by software for quantitative LC-MS experiments and describe different approaches that are available. With focus on label-free quantification, examples are discussed both for LC-MS/MS and LC-SRM data processing. We further elaborate on current quality control methodology for performing accurate protein quantification experiments. This article is part of a Special Issue entitled: Computational Proteomics in the Post-Identification Era. Guest Editors: Martin Eisenacher and Christian Stephan.},
author = {Sandin, Marianne and Teleman, Johan and Malmstr{\"{o}}m, Johan and Levander, Fredrik},
doi = {10.1016/j.bbapap.2013.03.026},
file = {:Users/ed/Dropbox/10. Proteomics SPC/2014-Sandin et al Data processing methods and quality control strategies for label-free LC–MS protein quantification.pdf:pdf},
isbn = {0006-3002 (Print)$\backslash$r0006-3002 (Linking)},
issn = {0006-3002},
journal = {Biochim. Biophys. Acta},
keywords = {Chromatography, Liquid,Chromatography, Liquid: methods,Liquid,Liquid: methods,Proteins,Proteins: analysis,Quality Control,Tandem Mass Spectrometry,Tandem Mass Spectrometry: methods},
number = {1 Pt A},
pages = {29--41},
pmid = {23567904},
publisher = {Elsevier B.V.},
title = {{Data processing methods and quality control strategies for label-free LC-MS protein quantification.}},
url = {http://www.sciencedirect.com/science/article/pii/S1570963913001398},
volume = {1844},
year = {2014}
}
@article{Eisenacher2011,
abstract = {In recent years, standardization and quality control have become important key points in industry, e.g. in drug discovery and for developing medical products. Is quality control in academic Proteomics a minor problem nowadays, where standard data formats and public repositories for data sharing exist? In this article, it is discussed how standard formats and repositories already support the documentation of quality control criteria in protein identification and quantification, and what has to be improved in the future: It is stated that the Proteomics community (represented by a group like the Proteomics Standards Initiative) will have to define a minimum document regarding quality control and to extend existing standards with additional quality control criteria enabling a substantial and standardized quality control process.},
author = {Eisenacher, Martin and Schnabel, Anke and Stephan, Christian},
doi = {10.1002/pmic.201000441},
file = {:Users/ed/Dropbox/10. Proteomics SPC/2011 Eisenacher et al-2011-Quality meets quantity – quality control, data standards and repositories PROTEOMICS.pdf:pdf},
issn = {16159853},
journal = {Proteomics},
keywords = {Bioinformatics,Data format,Data standard,Quality control,Quality criteria,Repository},
number = {6},
pages = {1031--1036},
pmid = {21365750},
title = {{Quality meets quantity - quality control, data standards and repositories}},
volume = {11},
year = {2011}
}
@article{Bereman2014,
abstract = {Statistical process control (SPC) is a robust set of tools that aids in the visualization, detection, and identification of assignable causes of variation in any process that creates products, services, or information. A tool has been developed termed Statistical Process Control in Proteomics (SProCoP) which implements aspects of SPC (e.g., control charts and Pareto analysis) into the Skyline proteomics software. It monitors five quality control metrics in a shotgun or targeted proteomic workflow. None of these metrics require peptide identification. The source code, written in the R statistical language, runs directly from the Skyline interface, which supports the use of raw data files from several of the mass spectrometry vendors. It provides real time evaluation of the chromatographic performance (e.g., retention time reproducibility, peak asymmetry, and resolution), and mass spectrometric performance (targeted peptide ion intensity and mass measurement accuracy for high resolving power instruments) via control charts. Thresholds are experiment- and instrument-specific and are determined empirically from user-defined quality control standards that enable the separation of random noise and systematic error. Finally, Pareto analysis provides a summary of performance metrics and guides the user to metrics with high variance. The utility of these charts to evaluate proteomic experiments is illustrated in two case studies.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Bereman, Michael S. and Johnson, Richard and Bollinger, James and Boss, Yuval and Shulman, Nick and MacLean, Brendan and Hoofnagle, Andrew N. and MacCoss, Michael J.},
doi = {10.1007/s13361-013-0824-5},
eprint = {NIHMS150003},
file = {:Users/ed/Dropbox/10. Proteomics SPC/2014 Bereman at al. Implementation of Statistical Process Control for Proteomic experiments.pdf:pdf},
isbn = {2122633255},
issn = {18791123},
journal = {J. Am. Soc. Mass Spectrom.},
keywords = {Mass spectrometry,Proteomics,Quality control,Shewhart control charts,Statistical process control},
number = {4},
pages = {581--587},
pmid = {24496601},
title = {{Implementation of statistical process control for proteomic experiments via LC MS/MS}},
volume = {25},
year = {2014}
}
@article{Wang2014,
abstract = {Shotgun proteomics experiments integrate a complex sequence of processes, any of which can introduce variability. Quality metrics computed from LC-MS/MS data have relied upon identifying MS/MS scans, but a new mode for the QuaMeter software produces metrics that are independent of identifications. Rather than evaluating each metric independently, we have created a robust multivariate statistical toolkit that accommodates the correlation structure of these metrics and allows for hierarchical relationships among data sets. The framework enables visualization and structural assessment of variability. Study 1 for the Clinical Proteomics Technology Assessment for Cancer (CPTAC), which analyzed three replicates of two common samples at each of two time points among 23 mass spectrometers in nine laboratories, provided the data to demonstrate this framework, and CPTAC Study 5 provided data from complex lysates under Standard Operating Procedures (SOPs) to complement these findings. Identification-independent quality metrics enabled the differentiation of sites and run-times through robust principal components analysis and subsequent factor analysis. Dissimilarity metrics revealed outliers in performance, and a nested ANOVA model revealed the extent to which all metrics or individual metrics were impacted by mass spectrometer and run time. Study 5 data revealed that even when SOPs have been applied, instrument-dependent variability remains prominent, although it may be reduced, while within-site variability is reduced significantly. Finally, identification-independent quality metrics were shown to be predictive of identification sensitivity in these data sets. QuaMeter and the associated multivariate framework are available from http://fenchurch.mc.vanderbilt.edu and http://homepages.uc.edu/{\~{}}wang2x7/ , respectively.},
author = {Wang, Xia and Chambers, Matthew C. and Vega-Montoto, Lorenzo J. and Bunk, David M. and Stein, Stephen E. and Tabb, David L.},
doi = {10.1021/ac4034455},
file = {:Users/ed/Dropbox/10. Proteomics SPC/2014 Wang et al - QC Metrics from CPTAC Raw LC-MS$\backslash$:MS Data Interpreted through Multivariate Statistics.pdf:pdf},
issn = {00032700},
journal = {Anal. Chem.},
number = {5},
pages = {2497--2509},
pmid = {24494671},
title = {{QC metrics from CPTAC raw LC-MS/MS data interpreted through multivariate statistics}},
volume = {86},
year = {2014}
}
@article{Taylor2013,
abstract = {SUMMARY: Quality control in mass spectrometry-based proteomics remains subjective, labor-intensive and inconsistent between laboratories. We introduce Metriculator, a software designed to facilitate long-term storage of extensive performance metrics as introduced by NIST in 2010. Metriculator features a web interface that generates interactive comparison plots for contextual understanding of metric values and an automated metric generation toolkit. The comparison plots are designed for at-a-glance determination of outliers and trends in the datasets, together with relevant statistical comparisons. Easy-to-use quantitative comparisons and a framework for integration plugins will encourage a culture of quality assurance within the proteomics community. Availability and Implementation: Available under the MIT license at http://github.com/princelab/metriculator.},
author = {Taylor, Ryan M. and Dance, Jamison and Taylor, Russ J. and Prince, John T.},
doi = {10.1093/bioinformatics/btt510},
file = {:Users/ed/Dropbox/10. Proteomics SPC/2013-Taylor-Metriculator.pdf:pdf},
isbn = {1367-4803},
issn = {13674803},
journal = {Bioinformatics},
number = {22},
pages = {2948--2949},
pmid = {24002108},
title = {{Metriculator: Quality assessment for mass spectrometry-based proteomics}},
volume = {29},
year = {2013}
}
@article{Barry1981,
author = {Barry, L and Burnett, Robert W and Nipper, Henry and Hunt, R},
file = {:Users/ed/Dropbox/10. Proteomics SPC/1981-Wesgard-A multi-rule shewhart chart for Qulity control in Clinical Chemistry.pdf:pdf},
number = {3},
pages = {493--501},
title = {{cIL}},
volume = {27},
year = {1981}
}
@article{Bramwell2013,
abstract = {Background: Statistical process control is a well-established and respected method which provides a general purpose, and consistent framework for monitoring and improving the quality of a process. It is routinely used in many industries where the quality of final products is critical and is often required in clinical diagnostic laboratories [1,2]. To date, the methodology has been little utilised in research proteomics. It has been shown to be capable of delivering quantitative QC procedures for qualitative clinical assays [3] making it an ideal methodology to apply to this area of biological research. Objective: To introduce statistical process control as an objective strategy for quality control and show how it could be used to benefit proteomics researchers and enhance the quality of the results they generate. Results: We demonstrate that rules which provide basic quality control are easy to derive and implement and could have a major impact on data quality for many studies. Conclusions: Statistical process control is a powerful tool for investigating and improving proteomics research work-flows. The process of characterising measurement systems and defining control rules forces the exploration of key questions that can lead to significant improvements in performance. Biological significance: This work asserts that QC is essential to proteomics discovery experiments. Every experimenter must know the current capabilities of their measurement system and have an objective means for tracking and ensuring that performance. Proteomic analysis work-flows are complicated and multi-variate. QC is critical for clinical chemistry measurements and huge strides have been made in ensuring the quality and validity of results in clinical biochemistry labs. This work introduces some of these QC concepts and works to bridge their use from single analyte QC to applications in multi-analyte systems.This article is part of a Special Issue entitled: Standardization and Quality Control in Proteomics. ?? 2013 The Author.},
author = {Bramwell, David},
doi = {10.1016/j.jprot.2013.06.010},
file = {:Users/ed/Dropbox/10. Proteomics SPC/2013 Bramwell-An introduction to statistical process control in preteomics.pdf:pdf},
issn = {18743919},
journal = {J. Proteomics},
keywords = {2-DE,Control chart,LC-MS,Quality control,Quantitative proteomics,Statistical process control},
pages = {3--21},
pmid = {23791708},
publisher = {The Author},
title = {{An introduction to statistical process control in research proteomics}},
url = {http://dx.doi.org/10.1016/j.jprot.2013.06.010},
volume = {95},
year = {2013}
}
@article{Angelica2008,
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Angelica, Michael D and Fong, Yuman},
doi = {10.1016/j.surg.2006.10.010.Use},
eprint = {NIHMS150003},
file = {:Users/ed/Dropbox/10. Proteomics SPC/2008 Koziol-Range Charts for Agreement in Measurement Comparison Studies, With Application to Replicate Mass Spectrometry.pdf:pdf},
isbn = {2156623929},
issn = {08966273},
journal = {October},
keywords = {colorectal tumor,hepatoma,hsv,immunotherapy,oncolytic therapy},
number = {4},
pages = {520--529},
pmid = {1000000221},
title = {{NIH Public Access}},
volume = {141},
year = {2008}
}
@article{Rudnick2009,
author = {Rudnick, Paul A and Clauser, Karl R and Kilpatrick, Lisa E and Tchekhovskoi, Dmitrii V and Neta, Pedatsur and Billheimer, Dean D and Blackman, Ronald K and Bunk, David M and Cardasis, Helene L and Ham, Joan L and Jaffe, Jacob D and Kinsinger, Christopher R and Mesri, Mehdi and Neubert, Thomas A and Schilling, Birgit and Tabb, David L and Tegeler, Tony J and Vega, Lorenzo and Variyath, Asokan Mulayath and Wang, Mu and Wang, Pei and Whiteaker, Jeffrey R and Zimmerman, Lisa J and Carr, Steven A and Fisher, Susan J and Gibson, Bradford W and Paulovich, Amanda G and Regnier, Fred E and Rodriguez, Henry and Spiegelman, Cliff and Tempst, Paul and Liebler, Daniel C and Stein, Stephen E and Network, Cptac and Az, Tucson and York, New and Research, Age},
doi = {10.1074/mcp.M900223-MCP200},
file = {:Users/ed/Dropbox/10. Proteomics SPC/2010 Rudnick et al Performance Metrics for Liquid Chromatography-Tandem Mass Spectrometry Systems in Proteomics Analyses Proteomics--225-41.pdf:pdf},
issn = {1535-9476},
journal = {Mol. Cell. Biol.},
number = {9.2},
pages = {225--241},
title = {{Performance Metrics for Evaluating Liquid Chromatography-Tandem Mass Spectrometry Systems in Shotgun Proteomics}},
year = {2009}
}
@article{Bereman2015,
abstract = {With advances in liquid chromatography coupled to tandem mass spectrometry technologies combined with the continued goals of biomarker discovery, clinical applications of established biomarkers, and integrating large multiomic datasets (i.e. “big data”), there remains an urgent need for robust tools to assess instrument performance (i.e. system suitability) in proteomic workflows. To this end, several freely available tools have been introduced that monitor a number of peptide identification (ID) and/or peptide ID free metrics. Peptide ID metrics include numbers of proteins, peptides, or peptide spectral matches identified from a complex mixture. Peptide ID free metrics include retention time reproducibility, full width half maximum, ion injection times, and integrated peptide intensities. The main driving force in the development of these tools is to monitor both intra- and interexperiment performance variability and to identify sources of variation. The purpose of this review is to summarize and evaluate these tools based on versatility, automation, vendor neutrality, metrics monitored, and visualization capabilities. In addition, the implementation of a robust system suitability workflow is discussed in terms of metrics, type of standard, and frequency of evaluation along with the obstacles to overcome prior to incorporating a more proactive approach to overall quality control in liquid chromatography coupled to tandem mass spectrometry based proteomic workflows.},
author = {Bereman, Michael S.},
doi = {10.1002/pmic.201400373},
file = {:Users/ed/Dropbox/10. Proteomics SPC/2015 Bereman Tools for monitoring system suitability in LC MS$\backslash$:MScentric proteomic experiments.pdf:pdf},
isbn = {1615-9861},
issn = {16159861},
journal = {Proteomics},
keywords = {Mass spectrometry,Quality control,Shewhart control charts,Statistical process control,System suitability,Technology},
number = {5-6},
pages = {891--902},
pmid = {25327420},
title = {{Tools for monitoring system suitability in LC MS/MS centric proteomic experiments}},
volume = {15},
year = {2015}
}
@article{Dogu2013a,
abstract = {The signal issued by a control chart triggers the process professionals to investigate the special cause. Change point methods simplify the efforts to search for and identify the special cause. In this study, using maximum likelihood estimation, a multivariate joint change point estimation procedure for monitoring both location and dispersion simultaneously is proposed. After a signal is generated by the simultaneously used Hotelling's T 2 and/or generalized variance control charts, the procedure starts detecting the time of the change. The performance of the proposed method for several structural changes for the mean vector and covariance matrix is discussed. {\textcopyright} 2013 Copyright Taylor and Francis Group, LLC.},
author = {Doǧu, E. and Kocako{\c{c}}, I.D.},
doi = {10.1080/03610918.2012.661907},
issn = {03610918 15324141},
journal = {Commun. Stat. Simul. Comput.},
keywords = {Change point estimation,Generalized variance control chart,Hotelling's T 2 control chart,Maximum likelihood estimation (MLE)},
number = {6},
pages = {1235--1255},
title = {{A multivariate change point detection procedure for monitoring mean and covariance simultaneously}},
volume = {42},
year = {2013}
}
@article{Dogu2014a,
abstract = {{\textcopyright} ICAQM 2014. Statistical process control efforts are considered to ensure high-quality production and reduce costs in the competitive environment of business. The signal issued by a control chart triggers the process professionals to identify and eliminate the cause(s) of an out-of-control situation. As a potential delay may exist in generating the signal, follow-up change point procedures are proposed for statistical monitoring. Knowing the time of a disturbance also simplifies the search for a special cause. Moreover, process professionals can focus on a narrower search window and find the root cause easily with change point analysis. Recent literature has indicated effective methods for normal processes, but the research for applications in gamma processes are scarce. In this study, several change point models based on maximum likelihood estimation (MLE) are considered to monitor variable TBE data which follows a Gamma distribution. The performance under the assumption that the process is monitored with cumulative quantity control (CQC-r), exponentially weighted moving average (EWMA) and cumulative sum (CUSUM) charts are compared.},
author = {Dogu, E.},
issn = {16843703},
journal = {Qual. Technol. Quant. Manag.},
keywords = {Change point estimation,Exponential distribution,Gamma distribution,Maximum likelihood estimation (MLE),variable time between events (TBE) charts},
number = {4},
pages = {383--400},
title = {{Change point estimation based statistical monitoring with variable time between events (TBE) control charts}},
volume = {11},
year = {2014}
}
@article{Dogu2011,
abstract = {In this study, using maximum likelihood estimation, a considerably effective change point model is proposed for the generalized variance control chart in which the required statistics are calculated with its distributional properties. The procedure, when used with generalized variance control charts, would be helpful for practitioners both controlling the multivariate process dispersion and detecting the time of the change in variance-covariance matrix of a process. The procedure starts after the chart issues a signal. Several structural changes for the variance-covariance matrix are considered and the precision and the accuracy of the proposed method is discussed. {\textcopyright} Taylor {\&} Francis Group, LLC.},
author = {Doǧu, E. and Kocako{\c{c}}, I.D.},
doi = {10.1080/03610918.2010.542844},
issn = {03610918 15324141},
journal = {Commun. Stat. Simul. Comput.},
keywords = {Change point estimation,Generalized variance control chart,Maximum likelihood estimation (MLE)},
number = {3},
pages = {345--363},
title = {{Estimation of change point in generalized variance control chart}},
volume = {40},
year = {2011}
}
@article{Anil2010,
abstract = {Aim: The purpose of the study is to determine the discriminative ability and calibration of Pediatric Risk of Mortality (PRISM I) I and Pediatric Index of Mortality (PIM) II in predicting the mortality in children admitted to a medical-surgical pediatric intensive care unit (PICU) in Turkey. Material and Method: A total of 277 children were evaluated from September 1, 2007, to, August 31, 2008, prospectively. Of these 277 patients, 39 patients (14.7{\%}) died at the end of the PICU stay. Discrimination between death and survival was assessed by calculating the area under the ROC curve for each model. The standardized mortality ratios (SMRs) were also determined. Calibration was assessed using Hosmer Lemeshow's test. Results: In our analysis, PRISM I (area under ROC curve: 0.884; SMR: 1; Hosmer Lemeshow chi-square p value: 0.09) and PIM II (area under ROC curve: 0.912; SMR: 1; Hosmer Lemeshow chi-square p value: 0.30) showed an adequate discrimination between death and survival as well as good calibration. Conclusions: In conclusion, both PRISM I and PIM II are reliable models for evaluating the prognosis of children in PICU. However, PIM II seems to be more useful because of its higher discrimination, better calibration and easier to use.},
author = {Anil, A.B. and Anil, M. and {\c{C}}etin, N. and Yildirimer, M. and Bal, A. and Şahbudak, Z. and Yavaşcan, {\"{O}}. and Aksu, N. and Doǧu, E.},
journal = {Turk Pediatr. Ars.},
keywords = {Pediatric index of mortality II score,Pediatric intensive care unit,Pediatric risk of mortality score,Prognosis},
number = {1},
pages = {18--24},
title = {{Pediatric risk and index of mortality in an intensive care unit | Bir Dahili-Cerrahi {\c{C}}ocuk Yoǧun Bakim Birimi'nde {\c{c}}ocuk {\"{o}}l{\"{u}}m riski I ve {\c{c}}ocuk {\"{o}}l{\"{u}}m endeksi II'nin karşilaştirilmasi}},
volume = {45},
year = {2010}
}
@article{Dogu2015,
abstract = {{\textcopyright} 2014, Taylor {\&} Francis. Change point estimation procedures simplify the efforts to search for and identify special causes in multivariate statistical process monitoring. After a signal is generated by the simultaneously used control charts or a single control chart, add-on change point procedure estimates the time of the change. In this study, multivariate joint change point estimation performance for simultaneous monitoring of both location and dispersion is compared under the assumption that various single charts are used to monitor the process. The change detection performance for several structural changes for the mean vector and covariance matrix is also discussed. It is concluded that choice of the control chart to obtain a signal may affect the change point detection performance.},
author = {Dogu, E.},
doi = {10.1080/00949655.2014.880704},
issn = {15635163 00949655},
journal = {J. Stat. Comput. Simul.},
keywords = {Hotelling's T2 control chart,change point estimation,generalized variance control chart,maximum-likelihood estimation,multivariate single control charts},
number = {8},
pages = {1529--1543},
title = {{Identifying the time of a step change with multivariate single control charts}},
volume = {85},
year = {2015}
}
@article{Kumar2014a,
author = {Kumar, N. and Chakraborti, S.},
doi = {10.1002/qre.1623},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Kumar, Chakraborti - 2014 - Improved Phase I Control Charts for Monitoring Times Between Events.pdf:pdf},
issn = {07488017},
journal = {Qual. Reliab. Eng. Int.},
keywords = {exponential distribution,ii control charts,in-control robustness,performance,phase i and phase,time between events},
pages = {n/a--n/a},
title = {{Improved Phase I Control Charts for Monitoring Times Between Events}},
url = {http://doi.wiley.com/10.1002/qre.1623},
year = {2014}
}
@article{Zhang2014,
author = {Zhang, Min and Megahed, Fadel M. and Woodall, William H.},
doi = {10.1002/qre.1495},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Megahed, Woodall - 2014 - Exponential CUSUM charts with estimated control limits.pdf:pdf},
issn = {07488017},
journal = {Qual. Reliab. Eng. Int.},
keywords = {high-yield processes,phase I control charting,safety monitoring,statistical process control (SPC),time between events (TBE)},
number = {March 2013},
pages = {275--286},
title = {{Exponential CUSUM charts with estimated control limits}},
volume = {30},
year = {2014}
}
@article{Ozsan2010,
abstract = {Count rates may reach very low levels in production processes with low$\backslash$ndefect levels. In such settings, conventional control charts for counts$\backslash$nmay become ineffective since the occurrence of many samples with zero$\backslash$ndefects would cause control statistic to be consistently zero.$\backslash$nConsequently, the exponentially weighted moving average (EWMA) control$\backslash$nchart to monitor the time between successive events (TBE) or counts has$\backslash$nbeen introduced as an effective approach for monitoring processes with$\backslash$nlow defect levels. When the counts occur according to a Poisson$\backslash$ndistribution, the TBE observations are distributed as exponential.$\backslash$nAlthough the assumption of exponential distribution is a reasonable$\backslash$nchoice as a model of TBE observations, its parameter, i.e. the mean$\backslash$n(also the standard deviation), is rarely known in practice and its$\backslash$nestimate is used in place of the unknown parameter when constructing the$\backslash$nexponential EWMA chart. In this article, we investigate the effects of$\backslash$nparameter estimation on the performance measures (average run length,$\backslash$nstandard deviation, and percentiles of the run length distribution) of$\backslash$nthe exponential EWMA control chart. A comprehensive analysis of the$\backslash$nconditional performance measures of the chart shows that the effect of$\backslash$nestimation can be serious, especially if small samples are used. An$\backslash$ninvestigation of the marginal performance measures, which are calculated$\backslash$nby averaging the conditional performance measures over the distribution$\backslash$nof the parameter estimator, allows us to provide explicit sample size$\backslash$nrecommendations in constructing these charts to reach a satisfactory$\backslash$nperformance in both the in-control and the out-of-control situation.$\backslash$nCopyright (C) 2009 John Wiley {\&} Sons, Ltd.},
author = {Ozsan, Guney and Testik, Murat Caner and Wei{\ss}, Christian H.},
doi = {10.1002/qre.1079},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Ozsan, Testik, Wei{\ss} - 2010 - Properties of the exponential EWMA chart with parameter estimation.pdf:pdf},
issn = {07488017},
journal = {Qual. Reliab. Eng. Int.},
keywords = {Estimated parameters,Exponential EWMA control chart,Inverse Gamma distribution,Low defect level,Statistical process control,Time between events (TBE)},
number = {October 2009},
pages = {555--569},
title = {{Properties of the exponential EWMA chart with parameter estimation}},
volume = {26},
year = {2010}
}
@article{Kumar2014,
author = {Kumar, Nirpeksh and Chakraborti, Subhabrata},
doi = {10.1002/qre.1752},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Kumar, Chakraborti - 2014 - Phase II Shewhart-type Control Charts for Monitoring Times Between Events and Effects of Parameter Estimatio.pdf:pdf},
issn = {07488017},
journal = {Qual. Reliab. Eng. Int.},
keywords = {10.1002/qre.1752 and average run length,MLE and UMVUE,average run length,bound,conditional run length distribution,false alarm rate,gamma distribution,mle and umvue,phase I sample,phase i sample,prediction,prediction bound},
pages = {n/a--n/a},
title = {{Phase II Shewhart-type Control Charts for Monitoring Times Between Events and Effects of Parameter Estimation}},
url = {http://doi.wiley.com/10.1002/qre.1752},
year = {2014}
}
@article{Lotze,
author = {Lotze, Thomas Harvey},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Lotze - Unknown - No Title.pdf:pdf},
title = {{No Title}}
}
@article{Singh2010,
abstract = {Key to the control of pandemic influenza are surveillance systems that raise alarms rapidly and sensitively. In addition, they must minimise false alarms during a normal influenza season. We develop a method that uses historical syndromic influenza data from the existing surveillance system 'SERVIS' (Scottish Enhanced Respiratory Virus Infection Surveillance) for influenza-like illness (ILI) in Scotland.},
author = {Singh, Brajendra K and Savill, Nicholas J and Ferguson, Neil M and Robertson, Chris and Woolhouse, Mark Ej},
doi = {10.1186/1471-2458-10-726},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Singh et al. - 2010 - Rapid detection of pandemic influenza in the presence of seasonal influenza.pdf:pdf},
isbn = {1471-2458},
issn = {1471-2458},
journal = {BMC Public Health},
number = {1},
pages = {726},
pmid = {21106071},
publisher = {BioMed Central Ltd},
title = {{Rapid detection of pandemic influenza in the presence of seasonal influenza.}},
url = {http://www.biomedcentral.com/1471-2458/10/726},
volume = {10},
year = {2010}
}
@article{,
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - No Title.pdf:pdf},
pages = {2},
title = {{No Title}}
}
@article{Ears,
author = {Ears, C D C and Cusum, C and C, C},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Ears, Cusum, C - Unknown - Additional File 1 R code for Early Aberration Reporting System ( EARS ) and negative binomial cusum ( NBC ).pdf:pdf},
pages = {2--3},
title = {{Additional File 1 : R code for Early Aberration Reporting System ( EARS ) and negative binomial cusum ( NBC ) algorithms}}
}
@article{,
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Appendix 2 RRv case notifications , expert-defined outbreak period ( shaded ) and cusum scores by day for outbreak d.pdf:pdf},
pages = {1--7},
title = {{Appendix 2 RRv case notifications , expert-defined outbreak period ( shaded ) and cusum scores by day for outbreak datasets 2 to 15 . Outbreak 2 ( days 236-653 ) cusum score cusum score cusum score}},
volume = {2}
}
@article{Buckeridge2007,
abstract = {Public health agencies and other groups have invested considerable resources in automated surveillance systems over the last decade. These systems generally follow syndromes in pre-diagnostic data drawn from sources such as emergency department visits. A main goal of syndromic surveillance systems is to detect outbreaks rapidly and the number of studies evaluating outbreak detection has increased recently. This paper reviews these studies with the goal of identifying the determinants of outbreak detection in automated syndromic surveillance systems. The review identified 35 studies with 22 studies (63{\%}) relying on naturally occurring outbreaks and 13 studies (37{\%}) relying on simulated outbreaks. In general, the results from these studies suggest that syndromic surveillance systems are capable of detecting some types of disease outbreaks rapidly with high sensitivity. The determinants of detection included characteristics of the system and of the outbreak. Influential system characteristics included representativeness, the outbreak detection algorithm, and the specificity of the algorithm. Important outbreak characteristics included the magnitude and shape of the signal and the timing of the outbreak. Future evaluations should aim to address inconsistencies in the evidence noted in this review and to identify the potential influence of other factors on outbreak detection. ?? 2006 Elsevier Inc. All rights reserved.},
author = {Buckeridge, David L.},
doi = {10.1016/j.jbi.2006.09.003},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Buckeridge - 2007 - Outbreak detection through automated surveillance A review of the determinants of detection.pdf:pdf},
isbn = {1532-0464},
issn = {15320464},
journal = {J. Biomed. Inform.},
keywords = {Evaluation,Medical informatics,Outbreak detection,Public health,Surveillance},
pages = {370--379},
pmid = {17095301},
title = {{Outbreak detection through automated surveillance: A review of the determinants of detection}},
volume = {40},
year = {2007}
}
@article{Sparks2010a,
abstract = {Automated public health surveillance of disease counts for rapid outbreak,$\backslash$nepidemic or bioterrorism detection using conventional control chart$\backslash$nmethods can be hampered by over-dispersion and background ('in-control')$\backslash$nmean counts that vary over time. An adaptive cumulative sum (CUSUM)$\backslash$nplan is developed for signalling unusually high incidence in prospectively$\backslash$nmonitored time series of over-dispersed daily disease counts with$\backslash$na non-homogeneous mean. Negative binomial transitional regression$\backslash$nis used to prospectively model background counts and provide 'one-step-ahead'$\backslash$nforecasts of the next day's count. A CUSUM plan then accumulates$\backslash$ndepartures of observed counts from an offset (reference value) that$\backslash$nis dynamically updated using the modelled forecasts. The CUSUM signals$\backslash$nwhenever the accumulated departures exceed a threshold. The amount$\backslash$nof memory of past observations retained by the CUSUM plan is determined$\backslash$nby the offset value; a smaller offset retains more memory and is$\backslash$nefficient at detecting},
author = {Sparks, Ross S. and Keighley, Tim and Muscatello, David},
doi = {10.1080/02664760903186056},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Sparks, Keighley, Muscatello - 2010 - Early warning CUSUM plans for surveillance of negative binomial daily disease counts.pdf:pdf},
isbn = {0266476090},
issn = {0266-4763},
journal = {J. Appl. Stat.},
number = {December 2014},
pages = {1911--1929},
title = {{Early warning CUSUM plans for surveillance of negative binomial daily disease counts}},
volume = {37},
year = {2010}
}
@article{Lemay2008,
abstract = {Emergency department data are currently being used by several syndromic surveillance systems to identify outbreaks of natural or man-made illnesses, and preliminary results suggest that regular outbreaks might be detected earlier with such data than with traditional reporting. This article summarizes a retrospective study of 5 influenza seasons in Ottawa,Canada; time-series analysis was used to look for an association between consultation to the emergency department for influenzalike illness and the isolation of influenza virus in the community. The population studied included both children and adults consulting to 3 local hospitals. In 4 seasons, visits to the emergency department involving children younger than 5 years consulting mainly for fever and for respiratory symptoms peaked 1 to 4 weeks before the isolation of influenza virus in the community. If monitored regularly for the presence of key symptoms, pediatric hospitals might be efficient and cost-effective sentinels of influenza and of other infectious diseases.},
author = {Lemay, Richard and Mawudeku, Abla and Shi, Yuanli and Ruben, Martha and Achonu, Camille},
doi = {10.1089/bsp.2007.0056},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Lemay et al. - 2008 - Syndromic surveillance for influenzalike illness.pdf:pdf},
issn = {1538-7135},
journal = {Biosecur. Bioterror.},
number = {2},
pages = {161--170},
pmid = {18563993},
title = {{Syndromic surveillance for influenzalike illness.}},
volume = {6},
year = {2008}
}
@article{Tokars2009,
abstract = {BioSense is a US national system that uses data from health information systems for automated disease surveillance. We studied 4 time-series algorithm modifications designed to improve sensitivity for detecting artificially added data. To test these modified algorithms, we used reports of daily syndrome visits from 308 Department of Defense (DoD) facilities and 340 hospital emergency departments (EDs). At a constant alert rate of 1{\%}, sensitivity was improved for both datasets by using a minimum standard deviation (SD) of 1.0, a 14-28 day baseline duration for calculating mean and SD, and an adjustment for total clinic visits as a surrogate denominator. Stratifying baseline days into weekdays versus weekends to account for day-of-week effects increased sensitivity for the DoD data but not for the ED data. These enhanced methods may increase sensitivity without increasing the alert rate and may improve the ability to detect outbreaks by using automated surveillance system data.},
author = {Tokars, Jerome I. and Burkom, Howard and Xing, Jian and English, Roseanne and Bloom, Steven and Cox, Kenneth and Pavlin, Julie a.},
doi = {10.3201/eid1504.080616},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Tokars et al. - 2009 - Enhancing time-series detection algorithms for automated biosurveillance.pdf:pdf},
isbn = {1080-6059 (Electronic)},
issn = {10806040},
journal = {Emerg. Infect. Dis.},
number = {4},
pages = {533--539},
pmid = {19331728},
title = {{Enhancing time-series detection algorithms for automated biosurveillance}},
volume = {15},
year = {2009}
}
@article{Watkins2008,
abstract = {BACKGROUND: The automated monitoring of routinely collected disease surveillance data has the potential to ensure that important changes in disease incidence are promptly recognised. However, few studies have established whether the signals produced by automated monitoring methods correspond with events considered by epidemiologists to be of public health importance. This study investigates the correspondence between retrospective epidemiological evaluation of notifications of Ross River virus (RRv) disease in Western Australia, and the signals produced by two cumulative sum (cusum)-based automated monitoring methods. METHODS: RRv disease case notification data between 1991 and 2004 were assessed retrospectively by two experienced epidemiologists, and the timing of identified outbreaks was compared with signals generated from two different types of cusum-based automated monitoring algorithms; the three Early Aberration Reporting System (EARS) cusum algorithms (C1, C2 and C3), and a negative binomial cusum. RESULTS: We found the negative binomial cusum to have a significantly greater area under the receiver operator characteristic curve when compared with the EARS algorithms, suggesting that the negative binomial cusum has a greater level of agreement with epidemiological opinion than the EARS algorithms with respect to the existence of outbreaks of RRv disease, particularly at low false alarm rates. However, the performance of individual EARS and negative binomial cusum algorithms were not significantly different when timeliness was also incorporated into the area under the curve analyses. CONCLUSION: Our retrospective analysis of historical data suggests that, compared with the EARS algorithms, the negative binomial cusum provides greater sensitivity for the detection of outbreaks of RRv disease at low false alarm levels, and decreased timeliness early in the outbreak period. Prospective studies are required to investigate the potential usefulness of these algorithms in practice.},
author = {Watkins, Rochelle E and Eagleson, Serryn and Veenendaal, Bert and Wright, Graeme and Plant, Aileen J},
doi = {10.1186/1472-6947-8-37},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Watkins et al. - 2008 - Applying cusum-based methods for the detection of outbreaks of Ross River virus disease in Western Australia.pdf:pdf},
isbn = {1472694783},
issn = {1472-6947},
journal = {BMC Med. Inform. Decis. Mak.},
pages = {37},
pmid = {18700044},
title = {{Applying cusum-based methods for the detection of outbreaks of Ross River virus disease in Western Australia.}},
volume = {8},
year = {2008}
}
@article{Gwilym2005,
author = {Gwilym, Stephen and Howard, Dominic P J and Davies, Nev and Willett, Keith},
doi = {10.1136/bmj.331.7531.1505},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Gwilym et al. - 2005 - Harry Potter casts a spell on accident prone children.pdf:pdf},
isbn = {1468-5833 (Electronic)0959-535X (Linking)},
issn = {0959-8138},
journal = {BMJ},
pages = {1505--1506},
pmid = {16373731},
title = {{Harry Potter casts a spell on accident prone children.}},
volume = {331},
year = {2005}
}
@article{Muscatello2005,
abstract = {BACKGROUND: In a climate of concern over bioterrorism threats and emergent diseases, public health authorities are trialling more timely surveillance systems. The 2003 Rugby World Cup (RWC) provided an opportunity to test the viability of a near real-time syndromic surveillance system in metropolitan Sydney, Australia. We describe the development and early results of this largely automated system that used data routinely collected in Emergency Departments (EDs). METHODS: Twelve of 49 EDs in the Sydney metropolitan area automatically transmitted surveillance data from their existing information systems to a central database in near real-time. Information captured for each ED visit included patient demographic details, presenting problem and nursing assessment entered as free-text at triage time, physician-assigned provisional diagnosis codes, and status at departure from the ED. Both diagnoses from the EDs and triage text were used to assign syndrome categories. The text information was automatically classified into one or more of 26 syndrome categories using automated "na{\"{i}}ve Bayes" text categorisation techniques. Automated processes were used to analyse both diagnosis and free text-based syndrome data and to produce web-based statistical summaries for daily review. An adjusted cumulative sum (cusum) was used to assess the statistical significance of trends. RESULTS: During the RWC the system did not identify any major public health threats associated with the tournament, mass gatherings or the influx of visitors. This was consistent with evidence from other sources, although two known outbreaks were already in progress before the tournament. Limited baseline in early monitoring prevented the system from automatically identifying these ongoing outbreaks. Data capture was invisible to clinical staff in EDs and did not add to their workload. CONCLUSION: We have demonstrated the feasibility and potential utility of syndromic surveillance using routinely collected data from ED information systems. Key features of our system are its nil impact on clinical staff, and its use of statistical methods to assign syndrome categories based on clinical free text information. The system is ongoing, and has expanded to cover 30 EDs. Results of formal evaluations of both the technical efficiency and the public health impacts of the system will be described subsequently.},
author = {Muscatello, David J and Churches, Tim and Kaldor, Jill and Zheng, Wei and Chiu, Clayton and Correll, Patricia and Jorm, Louisa},
doi = {10.1186/1471-2458-5-141},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Muscatello et al. - 2005 - An automated, broad-based, near real-time public health surveillance system using presentations to hospital E.pdf:pdf},
isbn = {1471-2458},
issn = {1471-2458},
journal = {BMC Public Health},
pages = {141},
pmid = {16372902},
title = {{An automated, broad-based, near real-time public health surveillance system using presentations to hospital Emergency Departments in New South Wales, Australia.}},
volume = {5},
year = {2005}
}
@article{Griffin2009,
abstract = {Since 2001, the District of Columbia Department of Health has been using an emergency room syndromic surveillance system to identify possible disease outbreaks. Data are received from a number of local hospital emergency rooms and analyzed daily using a variety of statistical detection algorithms. The aims of this paper are to characterize the performance of these statistical detection algorithms in rigorous yet practical terms in order to identify the optimal parameters for each and to compare the ability of two syndrome definition criteria and data from a children's hospital versus vs. other hospitals to determine the onset of seasonal influenza.},
author = {Griffin, Beth Ann and Jain, Arvind K and Davies-Cole, John and Glymph, Chevelle and Lum, Garret and Washington, Samuel C and Stoto, Michael a},
doi = {10.1186/1471-2458-9-483},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Griffin et al. - 2009 - Early detection of influenza outbreaks using the DC Department of Health's syndromic surveillance system.pdf:pdf},
isbn = {1471-2458 (Electronic)$\backslash$r1471-2458 (Linking)},
issn = {1471-2458},
journal = {BMC Public Health},
pages = {483},
pmid = {20028535},
title = {{Early detection of influenza outbreaks using the DC Department of Health's syndromic surveillance system.}},
volume = {9},
year = {2009}
}
@article{Pelecanos2010,
abstract = {Detection of outbreaks is an important part of disease surveillance. Although many algorithms have been designed for detecting outbreaks, few have been specifically assessed against diseases that have distinct seasonal incidence patterns, such as those caused by vector-borne pathogens.},
author = {Pelecanos, Anita M and Ryan, Peter a and Gatton, Michelle L},
doi = {10.1186/1472-6947-10-74},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Pelecanos, Ryan, Gatton - 2010 - Outbreak detection algorithms for seasonal disease data a case study using Ross River virus disease.pdf:pdf},
issn = {1472-6947},
journal = {BMC Med. Inform. Decis. Mak.},
number = {1},
pages = {74},
pmid = {21106104},
publisher = {BioMed Central Ltd},
title = {{Outbreak detection algorithms for seasonal disease data: a case study using Ross River virus disease.}},
url = {http://www.biomedcentral.com/1472-6947/10/74},
volume = {10},
year = {2010}
}
@article{Gan2010,
author = {Gan, Linmin and Reynolds, Marion R and Kim, Dong-yun},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Gan, Reynolds, Kim - 2010 - Adaptive Threshold Method for Monitoring Rates in Public Health Surveillance Adaptive Threshold Method for M.pdf:pdf},
keywords = {binomial distribution,biosurveillance,chart,exponentially weighted moving average,negative,outbreak detection,recurrence interval},
title = {{Adaptive Threshold Method for Monitoring Rates in Public Health Surveillance Adaptive Threshold Method for Monitoring Rates in Public Health Surveillance}},
year = {2010}
}
@article{Jones2002,
abstract = {A count of the number of defects is often used to monitor the quality of$\backslash$na production process. When defects rarely occur in a process, it is$\backslash$noften desirable to monitor the time between the occurrence of each$\backslash$ndefect rather than a count of the number of defects. An exponential$\backslash$ndistribution often provides a useful model of the time between defects.$\backslash$nPhase I control charts for exponentially distributed processes are$\backslash$ndiscussed. Methods for computing the control limits are given and the$\backslash$noverall Type I error rates of these charts are evaluated. Copyright (C)$\backslash$n2002 John Wiley Sons, Ltd.},
author = {Jones, L. Allison and Champ, Charles W.},
doi = {10.1002/qre.496},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Jones, Champ - 2002 - Phase I control charts for times between events.pdf:pdf},
issn = {07488017},
journal = {Qual. Reliab. Eng. Int.},
keywords = {Control chart,Exponential distribution,Phase I,Poisson process},
number = {April},
pages = {479--488},
title = {{Phase I control charts for times between events}},
volume = {18},
year = {2002}
}
@misc{,
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - CDC Short Course.ppt:ppt},
title = {{CDC Short Course}}
}
@article{Moore,
author = {Moore, Andrew and Cooper, Greg},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Moore, Cooper - Unknown - Summary of Biosurveillance-relevant technologies 1 Time-weighted averaging 2 Serfling method 3 ARIMA model.pdf:pdf},
number = {1},
pages = {1--15},
title = {{Summary of Biosurveillance-relevant technologies 1 Time-weighted averaging 2 Serfling method 3 ARIMA model}},
volume = {1}
}
@article{Reis2004,
abstract = {Study objective Data used by syndromic surveillance systems must be grouped into syndromes or prodromes. Previous studies have examined the accuracy of different methods of syndromic grouping. We seek to study the effects of different syndrome grouping methods on model accuracy, a key factor in the outbreak-detection performance of syndromic surveillance systems. Methods Daily emergency department visit rates were analyzed from 2 urban academic tertiary care hospitals for 1,680 consecutive days. During this period, each hospital census totaled approximately 230,000 patient visits. Three methods were used to group the visits into a respiratory-related syndrome category: 1 relying on chief complaint, 1 on diagnostic codes, and 1 on a combination of the two. The different groupings of the syndromic data resulting from these methods were used to build different historical models that were then tested for forecasting accuracy and for sensitivity to detecting simulated outbreaks. Results For both hospitals, the data grouped according to chief complaints alone yielded the lowest model accuracy and the lowest detection sensitivity. Using diagnostic codes to group the data yielded better results in accuracy and sensitivity. Combining the 2 grouping methods yielded the best results in accuracy and sensitivity. Temporal smoothing of the data was shown to improve sensitivity in all cases, although to various degrees in the different models. Conclusion The methods used to group input data into syndromic categories can have substantial effects on the overall performance of syndromic surveillance systems. The results suggest that incorporating diagnostic data into these systems can improve the modeling accuracy and its detection sensitivity. Furthermore, the best results may be achieved by using a combination of methods to group visits into syndromic categories.},
author = {Reis, Ben Y. and Mandl, Kenneth D.},
doi = {10.1016/j.annemergmed.2004.03.030},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Reis, Mandl - 2004 - Syndromic surveillance The effects of syndrome grouping on model accuracy and outbreak detection.pdf:pdf},
isbn = {1097-6760 (Electronic)$\backslash$r0196-0644 (Linking)},
issn = {01960644},
journal = {Ann. Emerg. Med.},
number = {September},
pages = {235--241},
pmid = {15332065},
title = {{Syndromic surveillance: The effects of syndrome grouping on model accuracy and outbreak detection}},
volume = {44},
year = {2004}
}
@article{Jr2006,
author = {Jr, Rd Fricker and Rolka, Henry},
doi = {10.1080/09332480.2006.10722809},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Jr, Rolka - 2006 - Protecting against biological terrorism statistical issues in electronic biosurveillance.pdf:pdf},
journal = {Chance},
number = {4},
pages = {1--21},
title = {{Protecting against biological terrorism: statistical issues in electronic biosurveillance}},
url = {http://www.nps.navy.mil/orfacpag/resumePages/papers/Frickerpa/ProtectingAgainstBiologicalTerrorism--StatisticalIssuesinElectronicBiosurveillance.pdf},
volume = {19},
year = {2006}
}
@article{Hohle2007,
abstract = {Public health surveillance of emerging infectious diseases is an essential instrument in the attempt to control and prevent their spread. This paper presents the R package 'surveillance', which contains functionality to visualise routinely collected surveillance data and provides algorithms for the statistical detection of aberrations in such univariate or multivariate time series. For evaluation purposes, the package includes real-world example data and the possibility to generate surveillance data by simulation. To compare algorithms, benchmark numbers like sensitivity, specificity, and detection delay can be computed for a set of time series. Package motivation, use and potential are illustrated through a mixture of surveillance theory, case study and R code snippets.},
author = {H{\"{o}}hle, Michael},
doi = {10.1007/s00180-007-0074-8},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/H{\"{o}}hle - 2007 - Surveillance An R package for the monitoring of infectious diseases.pdf:pdf},
issn = {09434062},
journal = {Comput. Stat.},
keywords = {Monitoring,Outbreak detection,Public health surveillance,Time series of counts,Univariate and multivariate surveillance},
pages = {571--582},
title = {{Surveillance: An R package for the monitoring of infectious diseases}},
volume = {22},
year = {2007}
}
@article{Zantek2005,
author = {Zantek, Paul F.},
doi = {10.1080/07408170500232297},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Zantek - 2005 - -Chart Schemes.pdf:pdf},
issn = {0740-817X},
journal = {IIE Trans.},
number = {April 2015},
pages = {1037--1045},
title = {{-Chart Schemes}},
volume = {37},
year = {2005}
}
@article{Hyndman,
author = {Hyndman, Rob J},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Hyndman - Unknown - Forecasting using 8.pdf:pdf},
title = {{Forecasting using 8.}}
}
@article{Williamson2009a,
abstract = {Although sample size calculations have become an important element in the design of research projects, such methods for studies involving current status data are scarce. Here, we propose a method for calculating power and sample size for studies using current status data. This method is based on a Weibull survival model for a two-group comparison. The Weibull model allows the investigator to specify a group difference in terms of a hazards ratio or a failure time ratio. We consider exponential, Weibull and uniformly distributed censoring distributions. We base our power calculations on a parametric approach with the Wald test because it is easy for medical investigators to conceptualize and specify the required input variables. As expected, studies with current status data have substantially less power than studies with the usual right-censored failure time data. Our simulation results demonstrate the merits of these proposed power calculations.},
author = {Williamson, John M and Lin, Hung-Mo and Kim, Hae-Young},
doi = {10.1002/sim},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Williamson, Lin, Kim - 2009 - Power and sample size calculations for current status survival analysis(2).pdf:pdf},
isbn = {2007090091480},
journal = {Stat. Med.},
keywords = {biosurveillance,exponential smoothing,forecasting,preconditioning,regression,time series},
number = {December 2006},
pages = {1999--2011},
pmid = {19455509},
title = {{Power and sample size calculations for current status survival analysis.}},
volume = {28},
year = {2009}
}
@article{Dovoedo2012,
author = {Dovoedo, Y. H. and Chakraborti, S.},
doi = {10.1002/qre.1226},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Dovoedo, Chakraborti - 2012 - Boxplot-based phase i control charts for time between events.pdf:pdf},
issn = {07488017},
journal = {Qual. Reliab. Eng. Int.},
keywords = {Poisson process,control chart,exponential distribution,phase I,reliability},
number = {June},
pages = {123--130},
title = {{Boxplot-based phase i control charts for time between events}},
volume = {28},
year = {2012}
}
@article{Stoto2006,
author = {Stoto, Michael a. and Fricker, Ronald D. and Jain, Arvind and Diamond, Alexis and Davies-Cole, John O. and Glymph, Chevelle and Kidane, Gebreyesus and Lum, Garrett and Jones, Laverne and Dehan, Kerda and Yuan, Christine},
doi = {10.1007/0-387-35209-0_9},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Stoto et al. - 2006 - Evaluating statistical methods for syndromic surveillance.pdf:pdf},
isbn = {0387329048},
journal = {Stat. Methods Counterterrorism Game Theory, Model. Syndr. Surveillance, Biometric Authentication},
pages = {141--172},
title = {{Evaluating statistical methods for syndromic surveillance}},
year = {2006}
}
@article{Lazer2014,
abstract = {In February 2013, Google Flu Trends (GFT) made headlines but not for a reason that Google executives or the creators of the flu tracking system would have hoped. Nature reported that GFT was predicting more than double the proportion of doctor visits for influenza-like illness (ILI) than the Centers for Disease Control and Prevention (CDC), which bases its estimates on surveillance reports from laboratories across the United States (1, 2). This happened despite the fact that GFT was built to predict CDC reports. Given that GFT is often held up as an exemplary use of big data (3, 4), what lessons can we draw from this error?},
author = {Lazer, David and Kennedy, Ryan and King, Gary and Vespignani, Alessandro},
doi = {10.1126/science.1248506},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Lazer et al. - 2014 - The Parable of Google Flu Traps in Big Data Analysis.pdf:pdf},
isbn = {0036-8075},
issn = {1095-9203},
journal = {Science (80-. ).},
number = {March},
pages = {1203--1205},
pmid = {24626916},
title = {{The Parable of Google Flu: Traps in Big Data Analysis}},
url = {http://www.sciencemag.org/content/343/6176/1203},
volume = {343},
year = {2014}
}
@article{Yang2013,
author = {Yang, Chao-lung and Nurtam, Mohammad Riza},
doi = {10.1007/978-981-4451-98-7},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Yang, Nurtam - 2013 - Proceedings of the Institute of Industrial Engineers Asian Conference 2013.pdf:pdf},
isbn = {978-981-4451-97-0},
keywords = {big data analytics},
pages = {827--835},
title = {{Proceedings of the Institute of Industrial Engineers Asian Conference 2013}},
url = {http://link.springer.com/10.1007/978-981-4451-98-7},
year = {2013}
}
@article{Fricker2010,
author = {Fricker, Ronald D},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Fricker - 2010 - Biosurveillance Detecting , Tracking , and Mitigating the Effects of Natural Disease and Bioterrorism.pdf:pdf},
keywords = {biosurveillance,bioterrorism,early event detection,public health,situational},
pages = {1--25},
title = {{Biosurveillance : Detecting , Tracking , and Mitigating the Effects of Natural Disease and Bioterrorism}},
year = {2010}
}
@article{Zantek2008,
author = {Zantek, Paul F.},
doi = {10.1080/00949650601146562},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Zantek - 2008 - A Markov-chain method for computing the run-length distribution of the self-starting cumulative sum scheme.pdf:pdf},
issn = {0094-9655},
journal = {J. Stat. Comput. Simul.},
number = {April 2015},
pages = {463--473},
title = {{A Markov-chain method for computing the run-length distribution of the self-starting cumulative sum scheme}},
volume = {78},
year = {2008}
}
@article{Technology2008,
author = {Technology, Quality and Management, Quantitative and He, Fangyi and Jiang, Wei and Shu, Lianjie},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Technology et al. - 2008 - Q t q m {\textcopyright}.pdf:pdf},
keywords = {{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_},average run length,bias,variance estimation},
number = {3},
pages = {289--308},
title = {{Q t q m {\textcopyright}}},
volume = {5},
year = {2008}
}
@article{Reis2003,
abstract = {BACKGROUND: Emergency department (ED) based syndromic surveillance systems identify abnormally high visit rates that may be an early signal of a bioterrorist attack. For example, an anthrax outbreak might first be detectable as an unusual increase in the number of patients reporting to the ED with respiratory symptoms. Reliably identifying these abnormal visit patterns requires a good understanding of the normal patterns of healthcare usage. Unfortunately, systematic methods for determining the expected number of (ED) visits on a particular day have not yet been well established. We present here a generalized methodology for developing models of expected ED visit rates. METHODS: Using time-series methods, we developed robust models of ED utilization for the purpose of defining expected visit rates. The models were based on nearly a decade of historical data at a major metropolitan academic, tertiary care pediatric emergency department. The historical data were fit using trimmed-mean seasonal models, and additional models were fit with autoregressive integrated moving average (ARIMA) residuals to account for recent trends in the data. The detection capabilities of the model were tested with simulated outbreaks. RESULTS: Models were built both for overall visits and for respiratory-related visits, classified according to the chief complaint recorded at the beginning of each visit. The mean absolute percentage error of the ARIMA models was 9.37{\%} for overall visits and 27.54{\%} for respiratory visits. A simple detection system based on the ARIMA model of overall visits was able to detect 7-day-long simulated outbreaks of 30 visits per day with 100{\%} sensitivity and 97{\%} specificity. Sensitivity decreased with outbreak size, dropping to 94{\%} for outbreaks of 20 visits per day, and 57{\%} for 10 visits per day, all while maintaining a 97{\%} benchmark specificity. CONCLUSIONS: Time series methods applied to historical ED utilization data are an important tool for syndromic surveillance. Accurate forecasting of emergency department total utilization as well as the rates of particular syndromes is possible. The multiple models in the system account for both long-term and recent trends, and an integrated alarms strategy combining these two perspectives may provide a more complete picture to public health authorities. The systematic methodology described here can be generalized to other healthcare settings to develop automated surveillance systems capable of detecting anomalies in disease patterns and healthcare utilization.},
author = {Reis, Ben Y and Mandl, Kenneth D},
doi = {10.1186/1472-6947-3-2},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Reis, Mandl - 2003 - Time series modeling for syndromic surveillance.pdf:pdf},
isbn = {1472-6947 (Electronic)$\backslash$n1472-6947 (Linking)},
issn = {1472-6947},
journal = {BMC Med. Inform. Decis. Mak.},
pages = {2},
pmid = {12542838},
title = {{Time series modeling for syndromic surveillance.}},
volume = {3},
year = {2003}
}
@article{FrickerJr.2008,
author = {{Fricker Jr.}, R D},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Fricker Jr. - 2008 - Syndromic Surveillance.pdf:pdf},
isbn = {030908864X},
journal = {Encycl. Quant. Risk Assess.},
keywords = {analysis,and application of real-time,biosurveillance,bioterrorism,defined as,early event detection,indicators of diseases and,interpretation,or near-real-time,outbreaks,public health,situational awareness,syndromic surveillance has been,systematic collection,the ongoing},
pages = {1743--1752},
title = {{Syndromic Surveillance}},
year = {2008}
}
@article{FrickerJr2005,
author = {{Fricker Jr}, R. D},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Fricker Jr - 2005 - Directionally Sensitive Multivariate Statistical Process Control Methods.pdf:pdf},
title = {{Directionally Sensitive Multivariate Statistical Process Control Methods}},
year = {2005}
}
@article{Fricker2008,
author = {Fricker, Ronald D. and Knitt, Matthew C. and Hu, Cecilia X.},
doi = {10.1080/08982110802334104},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Fricker, Knitt, Hu - 2008 - Comparing Directionally Sensitive MCUSUM and MEWMA Procedures with Application to Biosurveillance.pdf:pdf},
issn = {0898-2112},
journal = {Qual. Eng.},
number = {Cdc},
pages = {478--494},
title = {{Comparing Directionally Sensitive MCUSUM and MEWMA Procedures with Application to Biosurveillance}},
volume = {20},
year = {2008}
}
@article{Salmon2014,
author = {Salmon, Ma{\"{e}}lle and Schumacher, Dirk and Steiner, Stefan and Virta-, Mikko},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Salmon et al. - 2014 - Package ‘ surveillance '.pdf:pdf},
number = {1996},
title = {{Package ‘ surveillance '}},
year = {2014}
}
@article{Fricker2012,
abstract = {We describe a methodology for optimizing a threshold detection-based biosurveillance system. The goal is to maximize the system-wide probability of detecting an "event of interest" against a noisy background, subject to a constraint on the expected number of false signals. We use nonlinear programming to appropriately set detection thresholds taking into account the probability of an event of interest occurring somewhere in the coverage area. Using this approach, public health officials can "tune" their biosurveillance systems to optimally detect various threats, thereby allowing practitioners to focus their public health surveillance activities. Given some distributional assumptions, we derive a one-dimensional optimization methodology that allows for the efficient optimization of very large systems. We demonstrate that optimizing a syndromic surveillance system can improve its performance by 20-40{\%}. ?? 2009 Elsevier B.V. All rights reserved.},
author = {Fricker, Ronald D. and Banschbach, David},
doi = {10.1016/j.inffus.2009.12.002},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Fricker, Banschbach - 2012 - Optimizing biosurveillance systems that use threshold-based event detection methods.pdf:pdf},
issn = {15662535},
journal = {Inf. Fusion},
keywords = {Biosurveillance,Bioterrorism,Optimization,Public health,Shewhart chart,Syndromic surveillance},
pages = {117--128},
title = {{Optimizing biosurveillance systems that use threshold-based event detection methods}},
volume = {13},
year = {2012}
}
@article{Fricker2007,
author = {Fricker, Rd},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Fricker - 2007 - Directionally sensitive multivariate statistical process control procedures with application to syndromic surveillance.pdf:pdf},
journal = {Adv. Dis. Surveill.},
number = {Cdc},
pages = {1--17},
title = {{Directionally sensitive multivariate statistical process control procedures with application to syndromic surveillance}},
url = {http://www.isdsjournal.org/articles/1062.pdf},
year = {2007}
}
@article{Hagen2011,
author = {Hagen, Katie S. and Fricker, Ronald D and Hanni, Krista D. and Barnes, Susan and Michie, Kristy},
doi = {10.2202/2151-7509.1018},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Hagen et al. - 2011 - Assessing the Early Aberration Reporting System's Ability to Locally Detect the 2009 Influenza Pandemic.pdf:pdf},
issn = {2151-7509},
journal = {Stat. Polit. Policy},
number = {1},
title = {{Assessing the Early Aberration Reporting System's Ability to Locally Detect the 2009 Influenza Pandemic}},
volume = {2},
year = {2011}
}
@article{Fricker2009,
author = {Fricker, Ronald D},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Fricker - 2009 - Methodological Issues for Biosurveillance.pdf:pdf},
title = {{Methodological Issues for Biosurveillance}},
year = {2009}
}
@article{Pages,
author = {Pages, Read and Examples, Read and Lesson, Complete and Objectives, Learning},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Pages et al. - Unknown - Lesson 8 Regression with ARIMA errors , Cross correlation functions , and Relationships between 2 Time Series.pdf:pdf},
title = {{Lesson 8 : Regression with ARIMA errors , Cross correlation functions , and Relationships between 2 Time Series 8 . 1 Linear Regression Models with Autoregressive Errors}}
}
@article{Fricker2008a,
author = {Fricker, Ronald D and Chang, Joseph T},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Fricker, Chang - 2008 - A Spatio-temporal Methodology for Real-time Biosurveillance.pdf:pdf},
keywords = {biosurveillance,bioterrorism,kernel den-,kolmogorov-smirnov statistic,public health,sity estimation,syndromic surveillance},
pages = {1--22},
title = {{A Spatio-temporal Methodology for Real-time Biosurveillance}},
year = {2008}
}
@article{Williamson2009,
abstract = {Although sample size calculations have become an important element in the design of research projects, such methods for studies involving current status data are scarce. Here, we propose a method for calculating power and sample size for studies using current status data. This method is based on a Weibull survival model for a two-group comparison. The Weibull model allows the investigator to specify a group difference in terms of a hazards ratio or a failure time ratio. We consider exponential, Weibull and uniformly distributed censoring distributions. We base our power calculations on a parametric approach with the Wald test because it is easy for medical investigators to conceptualize and specify the required input variables. As expected, studies with current status data have substantially less power than studies with the usual right-censored failure time data. Our simulation results demonstrate the merits of these proposed power calculations.},
author = {Williamson, John M and Lin, Hung-Mo and Kim, Hae-Young},
doi = {10.1002/sim},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Williamson, Lin, Kim - 2009 - Power and sample size calculations for current status survival analysis.pdf:pdf},
isbn = {2007090091480},
journal = {Stat. Med.},
keywords = {biosurveillance,cusum,early aberration reporting system,syndromic surveillance},
number = {January},
pages = {1999--2011},
pmid = {19455509},
title = {{Power and sample size calculations for current status survival analysis.}},
volume = {28},
year = {2009}
}
@article{Andersson2008,
abstract = {We describe and discuss statistical models of Swedish influenza data, with special focus on aspects which are important in on-line monitoring. Earlier suggested statistical models are reviewed and the possibility of using them to describe the variation in influenza-like illness (ILI) and laboratory diagnoses (LDI) is discussed. Exponential functions were found to work better than earlier suggested models for describing the influenza incidence. However, the parameters of the estimated functions varied considerably between years. For monitoring purposes we need models which focus on stable indicators of the change at the outbreak and at the peak.For outbreak detection we focus on ILI data. Instead of a parametric estimate of the baseline (which could be very uncertain), we suggest a model utilizing the monotonicity property of a rise in the incidence. For ILI data at the outbreak, Poisson distributions can be used as a first approximation.To confirm that the peak has occurred and the decline has started, we focus on LDI data. A Gaussian distribution is a reasonable approximation near the peak. In view of the variability of the shape of the peak, we suggest that a detection system use the monotonicity properties of a peak.},
author = {Andersson, Eva and Bock, David and Fris{\'{e}}n, Marianne},
doi = {10.1177/0962280206078986},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Andersson, Bock, Fris{\'{e}}n - 2008 - Modeling influenza incidence for the purpose of on-line monitoring.pdf:pdf},
isbn = {0962-2802 (Print)$\backslash$r0962-2802 (Linking)},
issn = {0962-2802},
journal = {Stat. Methods Med. Res.},
pages = {421--438},
pmid = {17698935},
title = {{Modeling influenza incidence for the purpose of on-line monitoring.}},
volume = {17},
year = {2008}
}
@article{May2010,
abstract = {The purpose of syndromic surveillance is early detection of a disease outbreak. Such systems rely on the earliest data, usually chief complaint. The growing use of electronic medical records (EMR) raises the possibility that other data, such as emergency department (ED) diagnosis, may provide more specific information without significant delay, and might be more effective in detecting outbreaks if mechanisms are in place to monitor and report these data.},
author = {May, Larissa S and Griffin, Beth Ann and Bauers, Nicole Maier and Jain, Arvind and Mitchum, Marsha and Sikka, Neal and Carim, Marianne and Stoto, Michael a},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/May et al. - 2010 - Emergency department chief complaint and diagnosis data to detect influenza-like illness with an electronic medical.pdf:pdf},
isbn = {1936-9018 (Electronic)$\backslash$r1936-900X (Linking)},
issn = {1936-9018},
journal = {West. J. Emerg. Med.},
number = {February},
pages = {1--9},
pmid = {20411066},
title = {{Emergency department chief complaint and diagnosis data to detect influenza-like illness with an electronic medical record.}},
volume = {11},
year = {2010}
}
@article{Bakanli2011,
author = {Bakanli, L I K},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Bakanli - 2011 - T{\"{u}}rk {\'{y}} ye {\'{y}} nfluenza s{\"{u}}rveyans raporu 2010 - 2011.pdf:pdf},
title = {{T{\"{u}}rk {\'{y}} ye {\'{y}} nfluenza s{\"{u}}rveyans raporu 2010 - 2011}},
year = {2011}
}
@article{Fricker2011,
abstract = {This paper briefly summarizes a short course I gave at the 12th Biennial Centers for Disease Control and Prevention (CDC) and Agency for Toxic Substances and Disease Registry (ATSDR) Symposium held in Decatur, Georgia on April 6, 2009. The goal of this short course was to discuss various methodological issues of biosurveillance detection algorithms, with a focus on the issues related to developing, evaluating, and implementing such algorithms.},
author = {Fricker, Ronald D.},
doi = {10.1002/sim.3880},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Fricker - 2011 - Some methodological issues in biosurveillance.pdf:pdf},
isbn = {1097-0258 (Electronic)$\backslash$r0277-6715 (Linking)},
issn = {02776715},
journal = {Stat. Med.},
keywords = {Bioterrorism,Epidemiologic surveillance,Public health surveillance,Syndromic surveillance},
number = {January 2010},
pages = {403--415},
pmid = {21312208},
title = {{Some methodological issues in biosurveillance}},
volume = {30},
year = {2011}
}
@article{Tsui2008,
abstract = {Due to the ongoing desire for healthcare performance improvement,$\backslash$nthe latest outbreaks of the avian influenza, and the continuing bioterrorism$\backslash$nthreat, there is an urgent need for research in healthcare and disease$\backslash$nsurveillance. In this article we present an overview and review of$\backslash$nthe general issues involved in healthcare, public health, and syndromic$\backslash$nsurveillance. In particular, we review existing data collection and$\backslash$nsurveillance systems, popular surveillance methods, and appropriate$\backslash$nperformance measures in healthcare and disease surveillance. We also$\backslash$ndiscuss specific challenges and future research in temporal and spatiotemporal$\backslash$nsurveillance. [ABSTRACT FROM AUTHOR]$\backslash$n$\backslash$nCopyright of {\#}QE{\#} is the property of Taylor {\&} Francis Ltd and its$\backslash$ncontent may not be copied or emailed to multiple sites or posted$\backslash$nto a listserv without the copyright holder's express written permission.$\backslash$nHowever, users may print, download, or email articles for individual$\backslash$nuse. This abstract may be abridged. No warranty is given about the$\backslash$naccuracy of the copy. Users should refer to the original published$\backslash$nversion of the material for the full abstract. (Copyright applies$\backslash$nto all Abstracts.)},
author = {Tsui, Kwok-Leung and Chiu, Wenchi and Gierlich, Peter and Goldsman, David and Liu, Xuyuan and Maschek, Thomas},
doi = {10.1080/08982110802334138},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Tsui et al. - 2008 - A Review of Healthcare, Public Health, and Syndromic Surveillance.pdf:pdf},
issn = {0898-2112},
journal = {Qual. Eng.},
number = {May 2015},
pages = {435--450},
title = {{A Review of Healthcare, Public Health, and Syndromic Surveillance}},
volume = {20},
year = {2008}
}
@article{Fricker2007a,
author = {Fricker, Ron},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Fricker - 2007 - SPC Applications in Syndromic Surveillance.pdf:pdf},
journal = {Prevention},
keywords = {biosurveillance,bioterrorism,early event detection,situational awareness,statistical process control},
pages = {1--24},
title = {{SPC Applications in Syndromic Surveillance}},
url = {http://www.stat.lanl.gov/QPRC2007/presentations/Rfricker talk.pdf},
year = {2007}
}
@article{Knoth2014,
author = {Knoth, Author Sven},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Knoth - 2014 - Package ‘ spc '.pdf:pdf},
title = {{Package ‘ spc '}},
year = {2014}
}
@article{Bersimis2007,
abstract = {In this paper we discuss the basic procedures for the implementation of multivariate statistical process control via control charting. Furthermore, we review multivariate extensions for all kinds of univariate control charts, such as multivariate Shewhart- type control charts, multivariate CUSUM control charts and multivariate EWMA control charts. In addition, we review unique procedures for the construction of multivariate control charts, based on multivariate statistical techniques such as principal components analysis (PCA) and partial least squares (PLS). Finally, we describe the most significant methods for the interpretation of an out-of-control signal.},
author = {Bersimis, S and Psarakis, S and Panaretos, J},
doi = {10.1002/qre},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Bersimis, Psarakis, Panaretos - 2007 - Control Charts An Overview(2).pdf:pdf},
issn = {07488017},
journal = {Qual. Reliab. Eng. Int.},
keywords = {cusum,ewma,hotelling,multivariate statistical process control,pca,pls,process control,quality control,s t 2},
number = {April},
pages = {517--543},
title = {{Control Charts : An Overview}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/qre.829/abstract},
volume = {23},
year = {2007}
}
@article{Sparks2011a,
author = {Sparks, Ross and Bolt, Sarah and Okugami, Chris},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Sparks, Bolt, Okugami - 2011 - Spatio-Temporal Disease Surveillance.pdf:pdf},
title = {{Spatio-Temporal Disease Surveillance}},
year = {2011}
}
@article{Faculty2012,
author = {Faculty, Istanbul},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Faculty - 2012 - 2003-2012 Yıllarını Kapsayan Dokuz Sezonda Grip S{\"{u}}rveyansı Bulguları İstanbul Tıp Fak{\"{u}}ltesi Ulusal İnflue.pdf:pdf},
number = {4},
pages = {575--593},
title = {{2003-2012 Yıllarını Kapsayan Dokuz Sezonda Grip S{\"{u}}rveyansı Bulguları : İstanbul Tıp Fak{\"{u}}ltesi Ulusal İnfluenza Referans Laboratuvarı Sonu{\c{c}}ları Influenza Surveillance in Nine Consecutive Seasons , 2003-2012 : Results from National Influenza Reference}},
volume = {46},
year = {2012}
}
@article{Boyle2011,
author = {Boyle, Justin R and Sparks, Ross S and Keijzers, Gerben B and Crilly, Julia L and Lind, James F and Ryan, Louise M},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Boyle et al. - 2011 - Prediction and surveillance of influenza epidemics R E S EA R C H EN A B LI N G T HE E- H EA L TH R E V O LU TI O.pdf:pdf},
number = {4},
title = {{Prediction and surveillance of influenza epidemics R E S EA R C H EN A B LI N G T HE E- H EA L TH R E V O LU TI O N}},
volume = {194},
year = {2011}
}
@article{Chen2014,
author = {Chen, Huifen and Huang, Chaosian},
doi = {10.1080/0740817X.2012.761369},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Chen, Huang - 2014 - The use of a CUSUM residual chart to monitor respiratory syndromic data.pdf:pdf},
issn = {0740-817X},
journal = {IIE Trans.},
keywords = {arima,cusum chart,regression analysis,respiratory syndrome,syndromic surveillance,time series},
number = {December},
pages = {790--797},
title = {{The use of a CUSUM residual chart to monitor respiratory syndromic data}},
url = {http://www.tandfonline.com/doi/abs/10.1080/0740817X.2012.761369},
volume = {46},
year = {2014}
}
@article{Schrell2013,
abstract = {BACKGROUND: We assessed the local implementation of syndromic surveillance (SyS) as part of the European project 'System for Information on, Detection and Analysis of Risks and Threats to Health' in Santander, Spain. METHODS: We applied a cumulative sum algorithm on emergency department (ED) chief complaints for influenza-like illness in the seasons 2010-11 and 2011-12. We fine tuned the algorithm using a receiver operating characteristic analysis to identify the optimal trade-off of sensitivity and specificity and defined alert criteria. We assessed the timeliness of the SyS system to detect the onset of the influenza season. RESULTS: The ED data correlated with the sentinel data. With the best algorithm settings we achieved 70/63{\%} sensitivity and 89/95{\%} specificity for 2010-11/2011-12. At least 2 consecutive days of signals defined an alert. In 2010-11 the SyS system alerted 1 week before the sentinel system and in 2011-12 in the same week. The data from the ED is available on a daily basis providing an advantage in timeliness compared with the weekly sentinel data. CONCLUSIONS: ED-based SyS in Santander complements sentinel influenza surveillance by providing timely information. Local fine tuning and definition of alert criteria are recommended to enhance validity.},
author = {Schrell, S. and Ziemann, a. and {Garcia-Castrillo Riesgo}, L. and Rosenk{\"{o}}tter, N. and Llorca, J. and Popa, D. and Krafft, T.},
doi = {10.1093/pubmed/fdt043},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Schrell et al. - 2013 - Local implementation of a syndromic influenza surveillance system using emergency department data in Santander,.pdf:pdf},
isbn = {1741-3842},
issn = {17413842},
journal = {J. Public Heal. (United Kingdom)},
keywords = {communicable diseases,emergency care,epidemiology},
number = {3},
pages = {397--403},
pmid = {23620543},
title = {{Local implementation of a syndromic influenza surveillance system using emergency department data in Santander, Spain}},
volume = {35},
year = {2013}
}
@article{Szarka2011,
author = {Szarka, John L and Woodall, William H and Reynolds, Marion R and Smith, Eric P},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Szarka et al. - 2011 - Surveillance of Negative Binomial and Bernoulli Processes.pdf:pdf},
journal = {Distribution},
keywords = {adaptive threshold,attributes data,biosurveillance,cusum charts,quality processes,statistical process control,w2 method},
title = {{Surveillance of Negative Binomial and Bernoulli Processes}},
year = {2011}
}
@article{Karami2012,
abstract = {BACKGROUND: There are few published studies that use real data testing to examine the performance of outbreak detection methods. The aim of this study was to determine the performance of the Exponentially Weighted Moving Average (EWMA) in real time detection of a local outbreak in Mashhad City, eastern Iran. METHODS: The EWMA algorithms (both EWMA1 with lambda = 0.3 and EWMA2 with lambda = 0.6) were applied to daily counts of suspected cases of measles to detect real outbreak which has occurred in the city of Mashhad during 2010. The performances of the EWMA algorithms were evaluated using a real data testing approach and reported by correlation analysis. RESULTS: Mashhad outbreak was detected with a delay of about 2 to 7 days using EWMA algorithms as outbreak detection method. Moreover, the utility of EWMA2 algorithm in real time detection of the outbreak was better than EWMA1 algorithm. CONCLUSION: Applying the EWMA algorithm as an outbreak detection method might not be useful in timely detection of the local outbreaks.},
author = {Karami, Manoochehr and Soori, Hamid and Mehrabi, Yadollah and Haghdoost, Ali Akbar and Gouya, Mohammad Mehdi},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Karami et al. - 2012 - Real time detection of a measles outbreak using the exponentially weighted moving average Does it work.pdf:pdf},
issn = {16822765},
journal = {J. Res. Health Sci.},
keywords = {Disease outbreak,Exponentially weighted moving average (EWMA),Iran,Measles,Surveillance},
number = {1},
pages = {25--30},
pmid = {22888711},
title = {{Real time detection of a measles outbreak using the exponentially weighted moving average: Does it work?}},
volume = {12},
year = {2012}
}
@article{Sparks2011,
abstract = {Exponentially weighted moving average (EWMA) plans for non-homogeneous$\backslash$nnegative binomial counts are developed for detecting the onset of$\backslash$nseasonal disease outbreaks in public health surveillance. These plans$\backslash$nare robust to changes in the in-control mean and over-dispersion$\backslash$nparameter of the negative binomial distribution, and therefore are$\backslash$nreferred to as adaptive plans. They differ from the traditional approach$\backslash$nof using standardized forecast errors based on the normality assumption.$\backslash$nPlans are investigated in terms of early signal properties for seasonal$\backslash$nepidemics. The paper demonstrates that the proposed EWMA plan has$\backslash$nefficient early detection properties that can be useful to$\backslash$nepidemiologists for communicable and other disease control and is$\backslash$ncompared with the CUSUM plan.},
author = {Sparks, R. S. and Keighley, T. and Muscatello, D.},
doi = {10.1080/02664763.2010.545184},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Sparks, Keighley, Muscatello - 2011 - Optimal exponentially weighted moving average (EWMA) plans for detecting seasonal epidemics when f.pdf:pdf},
issn = {0266-4763},
journal = {J. Appl. Stat.},
number = {December 2014},
pages = {2165--2181},
title = {{Optimal exponentially weighted moving average (EWMA) plans for detecting seasonal epidemics when faced with non-homogeneous negative binomial counts}},
volume = {38},
year = {2011}
}
@article{Hashimoto2000,
abstract = {BACKGROUND: Surveillance of infectious diseases is done in many countries. The aims of such surveillance include the detection of epidemics. In the present study, the possibility of detecting an epidemic in its early stage using a simple method was evaluated for 16 infectious diseases. METHODS: We used as an index the number of cases per week per sentinel medical institution in the area covered by a health centre in infectious disease surveillance in Japan in 1993-1997. Periods of epidemics in health centre areas were determined according to the reported indices. The simple method used for detecting the early stage of an epidemic is that if the index exceeds a critical value, then an epidemic will begin in the following 4 weeks. The sensitivity, specificity and positive predictive value for this epidemic warning were evaluated for given critical values. RESULTS: When the specificity of the epidemic warning was more than 95{\%}, the sensitivity was more than 60{\%} in ten diseases, and more than 80{\%} in four diseases (influenza-like illness, rubella, hand-foot-and-mouth disease, and herpangina). The positive predictive value was between 15.6{\%} and 31.4{\%} in these ten diseases. CONCLUSION: The early stage of epidemics of some infectious diseases might be detectable using this simple method.},
author = {Hashimoto, S and Murakami, Y and Taniguchi, K and Nagai, M},
doi = {doi: 10.1093/ije/29.5.905},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Hashimoto et al. - 2000 - Detection of epidemics in their early stage through infectious disease surveillance.pdf:pdf},
isbn = {0300-5771 (Print)$\backslash$n0300-5771 (Linking)},
issn = {0300-5771},
journal = {Int. J. Epidemiol.},
pages = {905--910},
pmid = {11034976},
title = {{Detection of epidemics in their early stage through infectious disease surveillance.}},
volume = {29},
year = {2000}
}
@article{Sparks2010,
abstract = {Summary The paper introduces a two-pass adaptive cumulative sum (CUSUM)$\backslash$nstatistic to identify age clusters (age grouping) that significantly$\backslash$ncontribute to epidemics or unusually high counts. If epidemiologists$\backslash$nknow that an epidemic is confined to a narrow age group, then this$\backslash$ninformation not only makes it clear where to target the epidemiological$\backslash$neffort but also helps them decide whether to respond. It is much$\backslash$neasier to control an epidemic that starts in a narrow age range of$\backslash$nthe population, such as pre-school children, than an epidemic that$\backslash$nis not confined demographically or geographically.},
author = {Sparks, Ross},
doi = {10.1111/j.1467-842X.2010.00580.x},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Sparks - 2010 - Two-pass cusum to identify age-cluster outbreaks.pdf:pdf},
issn = {13691473},
journal = {Aust. New Zeal. J. Stat.},
keywords = {Describing outbreaks,Monitoring,Poisson counts,Public health,Surveillance},
number = {3},
pages = {245--260},
title = {{Two-pass cusum to identify age-cluster outbreaks}},
volume = {52},
year = {2010}
}
@article{Cowling2006,
abstract = {BACKGROUND: A variety of Serfling-type statistical algorithms requiring long series of historical data, exclusively from temperate climate zones, have been proposed for automated monitoring of influenza sentinel surveillance data. We evaluated three alternative statistical approaches where alert thresholds are based on recent data in both temperate and subtropical regions. METHODS: We compared time series, regression, and cumulative sum (CUSUM) models on empirical data from Hong Kong and the US using a composite index (range = 0-1) consisting of the key outcomes of sensitivity, specificity, and time to detection (lag). The index was calculated based on alarms generated within the first 2 or 4 weeks of the peak season. RESULTS: We found that the time series model was optimal in the Hong Kong setting, while both the time series and CUSUM models worked equally well on US data. For alarms generated within the first 2 weeks (4 weeks) of the peak season in Hong Kong, the maximum values of the index were: time series 0.77 (0.86); regression 0.75 (0.82); CUSUM 0.56 (0.75). In the US data the maximum values of the index were: time series 0.81 (0.95); regression 0.81 (0.91); CUSUM 0.90 (0.94). CONCLUSIONS: Automated influenza surveillance methods based on short-term data, including time series and CUSUM models, can generate sensitive, specific, and timely alerts, and can offer a useful alternative to Serfling-like methods that rely on long-term, historically based thresholds.},
author = {Cowling, Benjamin J and Wong, Irene O L and Ho, Lai-Ming and Riley, Steven and Leung, Gabriel M},
doi = {10.1093/ije/dyl162},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Cowling et al. - 2006 - Methods for monitoring influenza surveillance data.pdf:pdf},
isbn = {0300-5771 (Print)},
issn = {0300-5771},
journal = {Int. J. Epidemiol.},
keywords = {deployed in influenza surveillance,detection,for decades and,in,in some western countries,influenza,more recently,population surveillance,public health,sentinel practices have been},
number = {August},
pages = {1314--1321},
pmid = {16926216},
title = {{Methods for monitoring influenza surveillance data.}},
volume = {35},
year = {2006}
}
@article{Albayrak2010,
author = {Albayrak, Nurhan and Ciblak, Meral A and Altas, Ayse Basak and Kanturvardar, Melis and Odabas, Yavuz and Sucakli, Bahadir and Korukluoglu, Gulay and Badur, Selim and Ertek, Mustafa},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Albayrak et al. - 2010 - Influenza surveillance results during 2008 - 2009 season in Turkey.pdf:pdf},
keywords = {influenza,surveillance,turkey},
number = {November},
pages = {199--203},
title = {{Influenza surveillance results during 2008 - 2009 season in Turkey}},
volume = {2},
year = {2010}
}
@article{Kass-Hout2012,
author = {Kass-Hout, T. a. and Xu, Z. and McMurray, P. and Park, S. and Buckeridge, D. L. and Brownstein, J. S. and Finelli, L. and Groseclose, S. L.},
doi = {10.1136/amiajnl-2011-000793},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Kass-Hout et al. - 2012 - Application of change point analysis to daily influenza-like illness emergency department visits.pdf:pdf},
isbn = {2011000793},
issn = {1067-5027},
journal = {J. Am. Med. Informatics Assoc.},
number = {July},
pages = {1075--1081},
pmid = {22759619},
title = {{Application of change point analysis to daily influenza-like illness emergency department visits}},
volume = {19},
year = {2012}
}
@misc{,
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - 2013-Rosenkotter et al-Validity and timeliness of syndromic influeanza surveillance during the autumn{\_}winter wave of.pdf:pdf},
title = {{2013-Rosenkotter et al-Validity and timeliness of syndromic influeanza surveillance during the autumn{\_}winter wave of H1N1 influenza 2009{\_}results of ER medical dispatch ambulance and emergency department data from three EU regions.pdf}}
}
@article{Apley2012,
abstract = {We develop a Bayesian approach for monitoring and graphically exploring$\backslash$na process mean and informing decisions related to process adjustment.$\backslash$nWe assume a rather general model, in which the observations are represented$\backslash$nas a process mean plus a random error term. In contrast to previous$\backslash$nwork on Bayesian methods for monitoring a mean, we allow any Markov$\backslash$nmodel for the mean. This includes a mean that wanders slowly, that$\backslash$nis constant over periods of time with occasional random jumps or$\backslash$ncombinations thereof. The approach also allows for any distribution$\backslash$nfor the random errors, although we focus on the normal error case.$\backslash$nWe use numerical integration to update relevant posterior distributions$\backslash$n(e.g., for the current mean or for future observations), as each$\backslash$nnew observation is obtained, in a computationally inexpensive manner.$\backslash$nUsing an example from automobile body assembly, we illustrate how$\backslash$nthe approach can inform decisions regarding whether to adjust a process.$\backslash$nSupplementary Materials for this article, including code for implementing$\backslash$nthe charts, are available online on the journal web site.$\backslash$n$\backslash$nWe develop a Bayesian approach for monitoring and graphically exploring$\backslash$na process mean and informing decisions related to process adjustment.$\backslash$nWe assume a rather general model, in which the observations are represented$\backslash$nas a process mean plus a random error term. In contrast to previous$\backslash$nwork on Bayesian methods for monitoring a mean, we allow any Markov$\backslash$nmodel for the mean. This includes a mean that wanders slowly, that$\backslash$nis constant over periods of time with occasional random jumps or$\backslash$ncombinations thereof. The approach also allows for any distribution$\backslash$nfor the random errors, although we focus on the normal error case.$\backslash$nWe use numerical integration to update relevant posterior distributions$\backslash$n(e.g., for the current mean or for future observations), as each$\backslash$nnew observation is obtained, in a computationally inexpensive manner.$\backslash$nUsing an example from automobile body assembly, we illustrate how$\backslash$nthe approach can inform decisions regarding whether to adjust a process.$\backslash$nSupplementary Materials for this article, including code for implementing$\backslash$nthe charts, are available online on the journal web site.},
author = {Apley, Daniel W.},
doi = {10.1080/00401706.2012.694722},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Apley - 2012 - Posterior Distribution Charts A Bayesian Approach for Graphically Exploring a Process Mean.pdf:pdf;:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Apley - 2012 - Posterior Distribution Charts A Bayesian Approach for Graphically Exploring a Process Mean(2).pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {bayesian monitoring,control charts,mean tracking,process capability analysis,statistical process control},
number = {April 2015},
pages = {279--293},
title = {{Posterior Distribution Charts: A Bayesian Approach for Graphically Exploring a Process Mean}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00401706.2012.694722},
volume = {54},
year = {2012}
}
@article{Demirhan2012,
abstract = {This article deals with the construction of an X̄ control chart using the Bayesian perspective. We obtain new control limits for the X̄ chart for exponentially distributed data-generating processes through the sequential use of Bayes' theorem and credible intervals. Construction of the control chart is illustrated using a simulated data example. The performance of the proposed, standard, tolerance interval, exponential cumulative sum (CUSUM) and exponential exponentially weighted moving average (EWMA) control limits are examined and compared via a Monte Carlo simulation study. The proposed Bayesian control limits are found to perform better than standard, tolerance interval, exponential EWMA and exponential CUSUM control limits for exponentially distributed processes.},
author = {Demirhan, Haydar and Hamurkaroglu, Canan},
doi = {10.1080/00949655.2012.721884},
issn = {0094-9655},
journal = {J. Stat. Comput. Simul.},
keywords = {Bayesian estimation,Shewart charts,credible interval,exponential CUSUM,exponential EWMA,time between events data,tolerance interval control limits},
language = {en},
month = {sep},
number = {3},
pages = {628--643},
publisher = {Taylor {\&} Francis},
title = {{Bayesian X̄ control limits for exponentially distributed measurements}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00949655.2012.721884{\#}.VSKSRS94j{\_}U.mendeley},
volume = {84},
year = {2012}
}
@article{Tsiamyrtzis2005,
author = {Tsiamyrtzis, Panagiotis and Hawkins, Douglas M},
doi = {10.1198/004017005000000346},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Tsiamyrtzis, Hawkins - 2005 - A Bayesian Scheme to Detect Changes in the Mean of a Short-Run Process.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {bayesian statistical process control,kalman filter,normal mixture,tool wear},
number = {March 2015},
pages = {446--456},
title = {{A Bayesian Scheme to Detect Changes in the Mean of a Short-Run Process}},
volume = {47},
year = {2005}
}
@article{Toubia2009,
author = {Toubia, Gracia},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Toubia - 2009 - A SEQUENTIAL BAYESIAN CUMULATIVE CONFORMANCE CONTROL CHART FOR HIGH YIELD PROCESSES.pdf:pdf},
number = {May},
title = {{A SEQUENTIAL BAYESIAN CUMULATIVE CONFORMANCE CONTROL CHART FOR HIGH YIELD PROCESSES}},
year = {2009}
}
@article{Bayarri2005a,
author = {Bayarri, M.J and Garc{\'{i}}a-Donato, G},
doi = {10.1198/004017005000000085},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Bayarri, Garc{\'{i}}a-Donato - 2005 - A Bayesian Sequential Look at u-Control Charts.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {empirical bayes approach,objective bayesian methods,overdispersion,predictive dis-,statistical process control,tributions},
number = {March 2015},
pages = {142--151},
title = {{A Bayesian Sequential Look at u-Control Charts}},
volume = {47},
year = {2005}
}
@article{Toubia-Stucky2012,
abstract = {Cumulative conformance count (CCC) control chart is a powerful$\backslash$nalternative to the traditional p-control chart, particularly in$\backslash$nmonitoring high yield processes with extremely low proportions of$\backslash$nnonconformance. However, a prevalent limitation of the CCC control chart$\backslash$nis its inability to detect small process deterioration. A sequential$\backslash$nBayesian CCC approach capable of detecting small process deterioration$\backslash$nis proposed in this paper. The new approach outperforms the traditional$\backslash$nCCC chart in that it does not require a large sample of initial$\backslash$nobservations of the process, which may be difficult, if not impossible$\backslash$nto obtain in practice. Moreover, the approach is self-starting, and thus$\backslash$nmay be used in short production runs. A Bayesian updating procedure is$\backslash$ndeveloped, which allows for the determination of initial control limits$\backslash$nbased on only three initial observations or some prior knowledge about$\backslash$nthe proportion of nonconformance of the process. Values of proportions$\backslash$nof nonconformance, ranging from 0.1 to 0.00001, are tested to$\backslash$ndemonstrate the deterioration detection capability of the new approach$\backslash$nin conjunction with the proposed deterioration detection rules.$\backslash$nCopyright (c) 2011 John Wiley {\&} Sons, Ltd.},
author = {Toubia-Stucky, Gracia and Liao, Haitao and Twomey, Janet},
doi = {10.1002/qre.1236},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Toubia-Stucky, Liao, Twomey - 2012 - A sequential bayesian cumulative conformance count approach to deterioration detection in high yiel.pdf:pdf},
issn = {07488017},
journal = {Qual. Reliab. Eng. Int.},
keywords = {cumulative conformance count chart,high yield process,sequential Bayesian},
number = {August 2011},
pages = {203--214},
title = {{A sequential bayesian cumulative conformance count approach to deterioration detection in high yield processes}},
volume = {28},
year = {2012}
}
@article{Zhang2006,
author = {Zhang, Cai Wen and Xie, Min and Goh, Thong Ngee},
doi = {10.1080/07408170600728905},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Xie, Goh - 2006 - Design of exponential control charts using a sequential sampling scheme.pdf:pdf},
issn = {0740-817X},
journal = {IIE Trans.},
number = {March 2015},
pages = {1105--1116},
title = {{Design of exponential control charts using a sequential sampling scheme}},
volume = {38},
year = {2006}
}
@article{Lee2013,
author = {Lee, Jaeheon and Wang, Ning and Xu, Liaosa and Schuh, Anna and Woodall, William H.},
doi = {10.1002/qre.1413},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Lee et al. - 2013 - The effect of parameter estimation on upper-sided bernoulli cumulative sum charts.pdf:pdf},
issn = {07488017},
journal = {Qual. Reliab. Eng. Int.},
keywords = {Bernoulli CUSUM chart,average number of observations to signal (ANOS),sample size,statistical process control},
number = {May 2012},
pages = {639--651},
title = {{The effect of parameter estimation on upper-sided bernoulli cumulative sum charts}},
volume = {29},
year = {2013}
}
@article{Zhang2012,
author = {Zhang, Cai Wen and Xie, Min and Jin, Tongdan},
doi = {10.1080/00207543.2011.649305},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Xie, Jin - 2012 - An improved self-starting cumulative count of conforming chart for monitoring high-quality processes under grou.pdf:pdf},
issn = {0020-7543},
journal = {Int. J. Prod. Res.},
number = {March 2015},
pages = {7026--7043},
title = {{An improved self-starting cumulative count of conforming chart for monitoring high-quality processes under group inspection}},
volume = {50},
year = {2012}
}
@article{Tang2004,
author = {Tang, Loon Ching and Cheong, Wee Tat},
doi = {10.1080/07408170490473024},
issn = {0740-817X},
journal = {IIE Trans.},
number = {February 2015},
pages = {841--853},
title = {{Cumulative conformance count chart with sequentially updated parameters}},
volume = {36},
year = {2004}
}
@article{Yang2015,
abstract = {Exponential charts based on time-between-events (TBE) data are widely investigated and applied in various fields. The average time to signal (ATS) is used instead of the average run length to evaluate the performance of TBE charts, since the ATS involves both the number and the time of samples inspected until a signal occurs. An ATS-unbiased exponential control chart is proposed when the in-control parameter is known. Considering the need in practice to start monitoring a production process as soon as possible, a sequential sampling scheme is adopted and the in-control parameter is estimated by an unbiased and consistent estimator. Some specific guidelines to stop updating control limits are obtained from the relationship between the phase I sample size and the actual false alarm rate. Finally, two real examples are given to illustrate the implementation and efficiency of the proposed method.},
author = {Yang, Jun and Yu, Huan and Cheng, Yuan and Xie, Min},
doi = {10.1080/00207543.2014.974848},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Yang et al. - 2015 - Design of exponential control charts based on average time to signal using a sequential sampling scheme.pdf:pdf},
issn = {0020-7543},
journal = {Int. J. Prod. Res.},
keywords = {Gamma distribution,average time to signal (ATS),sequential sampling,time between events,unbiased control chart},
language = {en},
month = {feb},
number = {7},
pages = {2131--2145},
publisher = {Taylor {\&} Francis},
title = {{Design of exponential control charts based on average time to signal using a sequential sampling scheme}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00207543.2014.974848{\#}.VOyaTpR3Gk8.mendeley},
volume = {53},
year = {2015}
}
@article{Assareh2012,
author = {Assareh, Hassan},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Assareh - 2012 - Bayesian Hierarchical Models in Statistical Quality Control Methods to Improve Healthcare in Hospitals.pdf:pdf},
number = {June},
title = {{Bayesian Hierarchical Models in Statistical Quality Control Methods to Improve Healthcare in Hospitals}},
year = {2012}
}
@article{Forrester2007,
abstract = {This paper describes a stochastic epidemic model developed to infer transmission rates of asymptomatic communicable pathogens within a hospital ward. Inference is complicated by partial observation of the epidemic process and dependencies within the data. The epidemic process of nosocomial communicable pathogens can be partially observed by routine swabs testing for the presence of the pathogen. False-negative swab results must be accounted for and make it difficult to ascertain the number of patients who were colonized. Reversible jump Markov chain Monte Carlo methods are used within a Bayesian framework to make inferences about the colonization rates and unknown colonization times. The methods are applied to routinely collected data concerning methicillin-resistant Staphylococcus Aureus in an intensive care unit to estimate the effectiveness of isolation on reducing transmission of the bacterium.},
author = {Forrester, M L and Pettitt, A N and Gibson, G J},
doi = {10.1093/biostatistics/kxl017},
issn = {1465-4644},
journal = {Biostatistics},
keywords = {Bayes Theorem,Computer Simulation,Cross Infection,Cross Infection: epidemiology,Cross Infection: prevention {\&} control,Cross Infection: transmission,Data Interpretation, Statistical,Disease Outbreaks,Humans,Infection Control,Infection Control: methods,Markov Chains,Methicillin Resistance,Models, Statistical,Monte Carlo Method,Queensland,Staphylococcal Infections,Staphylococcal Infections: epidemiology,Staphylococcal Infections: prevention {\&} control,Staphylococcal Infections: transmission,Staphylococcus aureus,Staphylococcus aureus: growth {\&} development},
month = {apr},
number = {2},
pages = {383--401},
pmid = {16926230},
title = {{Bayesian inference of hospital-acquired infectious diseases and control measures given imperfect surveillance data.}},
url = {http://biostatistics.oxfordjournals.org/cgi/content/long/8/2/383},
volume = {8},
year = {2007}
}
@article{Marques2015,
author = {Marques, Pedro a and Cardeira, Carlos B and Paranhos, Paula and Ribeiro, Sousa and Gouveia, Helena},
doi = {10.7763/IJIET.2015.V5.521},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Marques et al. - 2015 - Selection of the Most Suitable Statistical Process Control Approach for Short Production Runs A Decision-Model.pdf:pdf},
number = {4},
pages = {303--310},
title = {{Selection of the Most Suitable Statistical Process Control Approach for Short Production Runs : A Decision-Model}},
volume = {5},
year = {2015}
}
@article{Bersimis2007a,
abstract = {In this paper we discuss the basic procedures for the implementation of multivariate statistical process control via control charting. Furthermore, we review multivariate extensions for all kinds of univariate control charts, such as multivariate Shewhart- type control charts, multivariate CUSUM control charts and multivariate EWMA control charts. In addition, we review unique procedures for the construction of multivariate control charts, based on multivariate statistical techniques such as principal components analysis (PCA) and partial least squares (PLS). Finally, we describe the most significant methods for the interpretation of an out-of-control signal.},
author = {Bersimis, S and Psarakis, S and Panaretos, J},
doi = {10.1002/qre},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Bersimis, Psarakis, Panaretos - 2007 - Control Charts An Overview.pdf:pdf},
issn = {07488017},
journal = {Qual. Reliab. Eng. Int.},
keywords = {cusum,ewma,hotelling,multivariate statistical process control,pca,pls,process control,quality control,s t 2},
number = {November 2006},
pages = {517--543},
title = {{Control Charts : An Overview}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/qre.829/abstract},
volume = {23},
year = {2007}
}
@article{Vermaat2003,
abstract = {Several control charts for individual observations are compared. Traditional ones are the well-known Shewhart individuals control charts based on moving ranges. Alternative ones are non-parametric control charts based on empirical quantiles, on kernel estimators, and on extreme-value theory. Their in-control and out-of-control performance are studied by simulation combined with computation. It turns out that the alternative control charts are not only quite robust against deviations from normality but also perform reasonably well under normality of the observations. The performance of the Empirical Quantile control chart is excellent for all distributions considered, if the Phase I sample is sufficiently large. Copyright c JohnWiley {\&} Sons, Ltd. 2003},
author = {Vermaat, M. B. and Ion, Roxana a. and Does, R. J M M and Klaassen, C. a J},
doi = {10.1002/qre.586},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Vermaat et al. - 2003 - A comparison of Shewhart individuals control charts based on normal, non-parametric, and extreme-value theory.pdf:pdf},
issn = {07488017},
journal = {Qual. Reliab. Eng. Int.},
keywords = {Bootstrap,Extreme-value theory,Kernel estimators,Non-parametrics,Shewhart control charts,Statistical process control},
number = {May},
pages = {337--353},
title = {{A comparison of Shewhart individuals control charts based on normal, non-parametric, and extreme-value theory}},
volume = {19},
year = {2003}
}
@article{Fatahi2010,
author = {Fatahi, Amir Afshin and Noorossana, Rassoul and Dokouhaki, Pershang and Babakhani, Massoud},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Fatahi et al. - 2010 - Truncated zero inflated binomial control chart for monitoring rare health events.pdf:pdf},
journal = {Ijrras},
keywords = {distribution,health quality engineering,inflated binomial,overdispersion,rare health events,truncated distribution,zero,zib},
number = {September},
title = {{Truncated zero inflated binomial control chart for monitoring rare health events}},
volume = {4},
year = {2010}
}
@article{Rakitzis2015,
author = {Rakitzis, Athanasios C and Maravelakis, E and Castagliola, Philippe},
doi = {10.1002/qre.1764},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Rakitzis, Maravelakis, Castagliola - 2015 - CUSUM Control Charts for the Monitoring of Zero-inflated Binomial Processes.pdf:pdf},
keywords = {10.1002/qre.1764 and average run length (ARL),Markov chain,arl,average run length,health-related processes,high-yield processes,markov chain,standard deviation of run,standard deviation run length (SDRL)},
title = {{CUSUM Control Charts for the Monitoring of Zero-inflated Binomial Processes}},
year = {2015}
}
@article{Rakitzis2014,
author = {Rakitzis, Athanasios C and Maravelakis, Petros E and Castagliola, Philippe C},
doi = {10.1109/ARES.2014.63},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Rakitzis, Maravelakis, Castagliola - 2014 - A Comparative Study of Control Charts for Zero-Inflated Binomial Processes.pdf:pdf},
isbn = {9781479942237},
title = {{A Comparative Study of Control Charts for Zero-Inflated Binomial Processes}},
year = {2014}
}
@article{Simon,
author = {Simon, Kai a},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Simon - Unknown - Control charts.pdf:pdf},
pages = {1--9},
title = {{Control charts}}
}
@article{Sim2008,
author = {Sim, C. H. and Lim, M. H.},
doi = {10.1080/03610910801983145},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Sim, Lim - 2008 - Attribute Charts for Zero-Inflated Processes.pdf:pdf},
isbn = {0361091080198},
issn = {0361-0918},
journal = {Commun. Stat. - Simul. Comput.},
number = {February 2015},
pages = {1440--1452},
title = {{Attribute Charts for Zero-Inflated Processes}},
volume = {37},
year = {2008}
}
@article{Ho2012,
author = {Ho, Linda Lee and {da Costa Quinino}, Roberto and Suyama, Em{\'{i}}lio and Louren{\c{c}}o, Ruth Pereira},
doi = {10.1007/s00362-010-0356-z},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Ho et al. - 2012 - Monitoring the conforming fraction of high-quality processes using a control chart p under a small sample size and an.pdf:pdf},
issn = {09325026},
journal = {Stat. Pap.},
keywords = {Average run length,Control charts p,Estimator for parameter of conformance fraction,Mean square error},
pages = {507--519},
title = {{Monitoring the conforming fraction of high-quality processes using a control chart p under a small sample size and an alternative estimator}},
volume = {53},
year = {2012}
}
@article{Saghir2014,
author = {Saghir, Aamir and Lin, Zhengyan},
doi = {10.1002/qre.1642},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Saghir, Lin - 2014 - Control Charts for Dispersed Count Data An Overview.pdf:pdf},
keywords = {attributes,com-poisson process,dispersion data,ewma chart,overdispersion},
title = {{Control Charts for Dispersed Count Data : An Overview}},
year = {2014}
}
@article{Morton2011a,
abstract = {Reporting of hospital adverse event data is becoming increasingly mandated and this has motivated work on methods for the analysis and display of these data for groups of institutions. Currently, the method preferred by many workers is the funnel plot. Often, indirect standardisation is employed to produce these plots. It appears that, when used to display binary data such as surgical site infection or mortality data, the method is satisfactory. Increasingly, these data are risk-adjusted. However, risk adjustment of these data usually involves individual patients undergoing the same or similar procedures and the method does not appear to mislead. However, when dealing with count data such as bacteraemias it appears that this method can mislead, particularly where methods for risk adjustment of these data are used. Information about the hospitals or units of interest rather than individual patients is employed. For example, one hospital may have plastic and cardiac surgery units in which bacteraemias occur infrequently whereas another may provide treatment for renal failure (including transplantation) and have a large haematology-oncology unit (also including transplantation), each of which would expect higher bacteraemia rates. Moreover, the hospitals and units within them may differ substantially in size. It is well known that indirect standardisation can give biased results when denominators differ substantially. We illustrate this difficulty with risk-adjusted bacteraemia data from the Queensland Health Centre for Healthcare Infection, Surveillance and Prevention (CHRISP) database.},
author = {Morton, A and Mengersen, K and Rajmokan, M and Whitby, M and Playford, E G and Jones, M},
doi = {10.1016/j.jhin.2011.03.015},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Morton et al. - 2011 - Funnel plots and risk-adjusted count data adverse events. A limitation of indirect standardisation.pdf:pdf},
issn = {1532-2939},
journal = {J. Hosp. Infect.},
keywords = {Bacteremia,Bacteremia: epidemiology,Bacteremia: prevention {\&} control,Cross Infection,Cross Infection: epidemiology,Cross Infection: prevention {\&} control,Data Interpretation, Statistical,Humans,Infection Control,Infection Control: methods,Queensland,Queensland: epidemiology},
month = {aug},
number = {4},
pages = {260--3},
pmid = {21658799},
title = {{Funnel plots and risk-adjusted count data adverse events. A limitation of indirect standardisation.}},
url = {http://www.sciencedirect.com/science/article/pii/S0195670111001447},
volume = {78},
year = {2011}
}
@article{Padoveze2010,
abstract = {Governmental programmes should be developed to collect and analyse data on healthcare associated infections (HAIs). This study describes the healthcare setting and both the implementation and preliminary results of the Programme for Surveillance of Healthcare Associated Infections in the State of S{\~{a}}o Paulo (PSHAISP), Brazil, from 2004 to 2006. Characterisation of the healthcare settings was carried out using a national database. The PSHAISP was implemented using components for acute care hospitals (ACH) or long term care facilities (LTCF). The components for surveillance in ACHs were surgical unit, intensive care unit and high risk nursery. The infections included in the surveillance were surgical site infection in clean surgery, pneumonia, urinary tract infection and device-associated bloodstream infections. Regarding the LTCF component, pneumonia, scabies and gastroenteritis in all inpatients were reported. In the first year of the programme there were 457 participating healthcare settings, representing 51.1{\%} of the hospitals registered in the national database. Data obtained in this study are the initial results and have already been used for education in both surveillance and the prevention of HAI. The results of the PSHAISP show that it is feasible to collect data from a large number of hospitals. This will assist the State of S{\~{a}}o Paulo in assessing the impact of interventions and in resource allocation.},
author = {Padoveze, M C and Assis, D B and Freire, M P and Madalosso, G and Ferreira, S A and Valente, M G and Fortaleza, C M C B},
doi = {10.1016/j.jhin.2010.07.005},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Padoveze et al. - 2010 - Surveillance Programme for Healthcare Associated Infections in the State of S{\~{a}}o Paulo, Brazil. Implementation.pdf:pdf},
issn = {1532-2939},
journal = {J. Hosp. Infect.},
keywords = {Brazil,Brazil: epidemiology,Catheter-Related Infections,Catheter-Related Infections: epidemiology,Cross Infection,Cross Infection: epidemiology,Humans,Intensive Care Units,Pneumonia,Pneumonia: epidemiology,Prevalence,Sentinel Surveillance,Surgery Department, Hospital,Surgical Wound Infection,Surgical Wound Infection: epidemiology,Urinary Tract Infections,Urinary Tract Infections: epidemiology},
month = {dec},
number = {4},
pages = {311--5},
pmid = {20884080},
title = {{Surveillance Programme for Healthcare Associated Infections in the State of S{\~{a}}o Paulo, Brazil. Implementation and the first three years' results.}},
url = {http://www.sciencedirect.com/science/article/pii/S0195670110003257},
volume = {76},
year = {2010}
}
@article{Morton2015,
author = {Morton, Anthony P and Clements, Archie C A and Doidge, Shane R and Stackelroth, Jenny and Curtis, Merrilyn and Whitby, Michael and Morton, Anthony P and Clements, Archie C A and Doidge, Shane R and Stackelroth, Jenny and Curtis, Merrilyn and Whitby, Michael and Morton, Anthony P and Clements, Archie C A and Doidge, Shane R},
doi = {10.1086/591478},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Morton et al. - 2015 - Control Surveillance of Healthcare-Acquired Infections in Queensland , Australia Data and Lessons From the First.pdf:pdf},
isbn = {0195941700048},
number = {2008},
pages = {695--701},
title = {{Control : Surveillance of Healthcare-Acquired Infections in Queensland , Australia Data and Lessons From the First 5 Years How to cite this article : Surveillance of Healthcare-Acquired Infections in Queensland , Australia : Data and Lessons From the Firs}},
year = {2015}
}
@article{Richards2007,
abstract = {Surveillance programmes for hospital-acquired infections differ amongst the Australian states. Victoria, New South Wales, Queensland and South Australia have recent substantial initiatives in development of statewide programmes. Whilst the definitions for surgical site infections (SSIs) and bloodstream infections (BSI) developed by the Australian Infection Control Association (AICA) do not differ from the US National Nosocomial Infection Surveillance (NNIS) programme definitions for SSI and intensive care unit (ICU) acquired central line-associated BSI, only two states use NNIS risk adjustment methods in reporting infection rates. Differences exist in the surgical procedures under surveillance, ICU surveillance, hospital-wide BSI surveillance, staff health immunization surveillance, process measures such us surgical antibiotic prophylaxis and small hospital programmes. Only in the area of antibiotic use surveillance has national consensus been reached. In Victoria, NNIS risk adjustment had limited usefulness in predicting SSIs, especially after coronary artery bypass graft (CABG) surgery. Ventilator-associated pneumonia (VAP) surveillance had limited acceptance, and is not undertaken in other states. Regular reporting of surgical antibiotic prophylaxis data has been followed by improvement in choice of antibiotic in some procedures. The South Australian programme for the surveillance of multiresistant organisms (MROs) has documented substantial improvement in meticillin-resistant Staphylococcus aureus (MRSA) morbidity over time coincident with the introduction of hand hygiene programmes and other measures. In Queensland, statewide monitoring of needlestick injuries is established. In Victoria, the small hospital programme concentrated on process measures, and in Queensland with a standardized investigation pathways for "signal" events. Data quality presented substantial challenges in small Victorian hospitals. Whilst state-based programmes have facilitated communication between hospitals and their coordinating centre, Australia still lacks national coordination and a national database on hospital infections. The differing approaches of the states illustrate many of the fundamental questions facing hospital infection surveillance today.},
author = {Richards, Michael J and Russo, Phillip L},
doi = {10.1016/S0195-6701(07)60039-5},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Richards, Russo - 2007 - Surveillance of hospital-acquired infections in Australia--One Nation, Many States.pdf:pdf},
issn = {0195-6701},
journal = {J. Hosp. Infect.},
keywords = {Australia,Australia: epidemiology,Cross Infection,Cross Infection: epidemiology,Humans,Infection Control,Infection Control: organization {\&} administration,Outcome and Process Assessment (Health Care),Outcome and Process Assessment (Health Care): stat,Sentinel Surveillance},
month = {jun},
pages = {174--81},
pmid = {17540266},
title = {{Surveillance of hospital-acquired infections in Australia--One Nation, Many States.}},
url = {http://www.sciencedirect.com/science/article/pii/S0195670107600395},
volume = {65 Suppl 2},
year = {2007}
}
@article{Morton2010,
abstract = {Sequential analysis of uncommon adverse outcomes (AEs) such as surgical site infections (SSIs) is desirable. Short postoperative lengths of stay (LOS) result in many SSIs occurring after discharge and they are often superficial. Deep and organ space (complex) SSIs occur less frequently but are detected more reliably and are suitable for monitoring wound care. Those occurring post-discharge usually require readmissison and can be counted accurately. Sequential analysis of meticillin-resistant Staphylococcus aureus bacteraemia is also needed. The key to prevention is to implement systems based on evidence, e.g. using 'bundles' and checklists. Regular mortality and morbidity audit meetings are required and these may need to be followed by independent audits. Sequential statistical analysis is desirable for data presentation, to detect changes, and to discourage tampering with processes when occasional AEs occur in a reliable system. Tabulations and cumulative observed minus expected (O-E) charts and funnel plots are valuable, supplemented in the presence of apparent 'runs' of AEs by cumulative sum analysis. Used prospectively, they may enable staff to visualise and detect patterns or shifts in rates and counts that might not otherwise be apparent. {\textcopyright} 2010 The Hospital Infection Society.},
author = {Morton, a. and Mengersen, K. and Waterhouse, M. and Steiner, S. and Looke, D.},
doi = {10.1016/j.jhin.2010.04.022},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Morton et al. - 2010 - Sequential analysis of uncommon adverse outcomes.pdf:pdf},
isbn = {0195-6701},
issn = {01956701},
journal = {J. Hosp. Infect.},
keywords = {Complex surgical site infections,Evidence-based systems,MRSA bacteraemias,Statistical process control,Uncommon adverse events},
number = {2},
pages = {114--118},
pmid = {20656377},
publisher = {Elsevier Ltd},
title = {{Sequential analysis of uncommon adverse outcomes}},
url = {http://dx.doi.org/10.1016/j.jhin.2010.04.022},
volume = {76},
year = {2010}
}
@article{Morton2010a,
abstract = {Analysis and reporting of among-institution aggregated hospital-acquired infection data are necessary for transparency and accountability. Different analytical methods are required for ensuring transparency and accountability for within-institution sequential analysis. In addition, unbiased summary information is needed for planning and informing the public. We believe that implementation of systems based on evidence is the key to improving institutional performance and safety. This must be accompanied by compliance, outcome audit and sequential analysis of outcome data, e.g. using statistical process control methods. Checklists can be a valuable aid for ensuring implementation of evidence-based systems. Aggregated outcome data analysis for transparency and accountability should concentrate primarily on accurately presenting the outcomes together with their precision. We describe tabulations, funnel plots and random-effects (shrinkage) analysis and avoid comparisons using league tables, star ratings and confidence intervals. {\textcopyright} 2010 The Hospital Infection Society.},
author = {Morton, a. and Mengersen, K. and Waterhouse, M. and Steiner, S.},
doi = {10.1016/j.jhin.2010.06.030},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Morton et al. - 2010 - Analysis of aggregated hospital infection data for accountability.pdf:pdf},
isbn = {0195-6701},
issn = {01956701},
journal = {J. Hosp. Infect.},
keywords = {Accountability,Aggregated outcome data,Audit,Evidence-based systems,Hospital infection,Outcome data,Statistical analysis},
number = {4},
pages = {287--291},
pmid = {20870312},
publisher = {Elsevier Ltd},
title = {{Analysis of aggregated hospital infection data for accountability}},
url = {http://dx.doi.org/10.1016/j.jhin.2010.06.030},
volume = {76},
year = {2010}
}
@article{Smith2013,
abstract = {Background: Graphical Statistical Process Control (SPC) tools have been shown to promptly identify significant variations in clinical outcomes in a range of health care settings. We explored the application of these techniques to quantitatively inform the routine cardiac surgical (CAS) morbidity and mortality (M{\&}M) review processes at a single site. Methods: Baseline clinical and procedural data relating to 5265 consecutive cardiac surgical procedures, performed at St Andrew's War Memorial Hospital (SAWMH) between the 1st January 2003 and the 30th April 2012, were retrospectively evaluated. A range of appropriate clinical outcome indicators (COIs) were developed and evaluated using a combination of Cumulative Sum charts, Exponentially Weighted Moving Average charts and Funnel Plots. Charts were updated regularly and discussed at the cardiac surgery unit's bi-monthly M{\&}M meetings. Risk adjustment (RA) for the COIs was developed and validated for incorporation into the charts to improve monitoring performance. Results: Discrete and aggregated measures, including blood product/reoperation, major acute post-procedural complications, cardiopulmonary bypass duration and Length of Stay/Readmission {\textless}28. days have proved to be valuable measures for monitoring outcomes. Instances of variation in performance identified using the charts were examined thoroughly and could be related to changes in clinical practice (e.g. antifibrinolytic use) as well as differences in individual operator performance (in some instances, driven by case mix). Conclusions: SPC tools can promptly detect meaningful changes in clinical outcome thereby allowing early intervention to address altered performance. Careful interpretation of charts for group and individual operators has proven helpful in detecting and differentiating systemic versus individual variation. {\textcopyright} 2013 Australian and New Zealand Society of Cardiac and Thoracic Surgeons (ANZSCTS) and the Cardiac Society of Australia and New Zealand (CSANZ).},
author = {Smith, Ian R. and Gardner, Michael a. and Garlick, Bruce and Brighouse, Russell D. and Cameron, James and Lavercombe, Peter S. and Mengersen, Kerrie and Foster, Kelley a. and Rivers, John T.},
doi = {10.1016/j.hlc.2013.01.011},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Smith et al. - 2013 - Performance monitoring in cardiac surgery Application of statistical process control to a single-site database.pdf:pdf},
issn = {14439506},
journal = {Hear. Lung Circ.},
keywords = {Cardiac surgical procedures,Outcome measures,Quality improvement,Statistical data interpretation},
number = {8},
pages = {634--641},
publisher = {Australian and New Zealand Society of Cardiac and Thoracic Surgeons (ANZSCTS) and the Cardiac Society of Australia and New Zealand (CSANZ)},
title = {{Performance monitoring in cardiac surgery: Application of statistical process control to a single-site database}},
url = {http://dx.doi.org/10.1016/j.hlc.2013.01.011},
volume = {22},
year = {2013}
}
@article{Morton2011,
abstract = {Objective: To describe monitoring of four years' isolated coronary artery bypass surgery outcomes and complications at The Prince Charles Hospital, Brisbane, Australia. Methods: Analysis of Cardiac Surgical Register database using tabulations, funnel plots and random-effects (Bayesian shrinkage) analysis for aggregated data. Combined CUSUM and cumulative observed minus expected (modified VLAD) charts and combined CUSUM and cumulative funnel plots used for individual observation sequential data and binomial control charts and generalised additive models (GAMs) for quarterly sequential data. Risk adjustment employed re-calibrated EuroSCORE. Results: There were 2575 procedures with an unadjusted in-hospital mortality rate of 1.17{\%}. Mean age was 65 years and 21{\%} of patients were female; 43.6{\%} were elective procedures. Median ventilation time was 10 hours and median length of stay in intensive care (ICU) was 23 hours. Return to theatre for bleeding occurred in 3{\%} of cases. Return to theatre for surgical site infection occurred in 0.4{\%} of cases; 4{\%} were re-do procedures. Permanent stroke or neurological deficit occurred in 1{\%}, perioperative myocardial infarction in 0.8{\%}, arrest in 1.2{\%}, renal failure in 1.6{\%} and ICU return in 2.3{\%} of cases. Conclusions: Complication rates and mortality were comparable with similar units. Use of random-effects (Bayesian shrinkage) analysis for aggregated data is encouraged together with generalised additive models (GAMs) and combined CUSUM and cumulative observed minus expected (modified VLAD) charts for sequential data. ?? 2011.},
author = {Morton, Anthony Park and Smith, Susan Erica and Mullany, Daniel V. and Clarke, Andrew J B and Wall, Douglas and Pohlner, Peter},
doi = {10.1016/j.hlc.2011.01.020},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Morton et al. - 2011 - An application of outcomes monitoring for coronary artery bypass surgery 2005-2008 at TPCH.pdf:pdf},
isbn = {1444-2892},
issn = {14439506},
journal = {Hear. Lung Circ.},
keywords = {Cardiac surgical patients,Coronary Artery Bypass Grafts,Outcomes monitoring,Statistical process control},
number = {5},
pages = {312--317},
pmid = {21377423},
publisher = {Australasian Society of Cardiac and Thoracic Surgeons and The Cardiac Society of Australia and New Zealand},
title = {{An application of outcomes monitoring for coronary artery bypass surgery 2005-2008 at TPCH}},
url = {http://dx.doi.org/10.1016/j.hlc.2011.01.020},
volume = {20},
year = {2011}
}
@article{Waterhouse2011,
abstract = {The transmission of multiple antibiotic-resistant organisms (MROs) in hospitals is affected by many inter-related factors. These include the background prevalence of the organism (burden), hand hygiene, the efficiency of patient screening, the isolation or cohorting of carriers, the quality of hospital cleaning, and bed occupancy. In addition, the prevalence of one MRO may influence the transmission of another by occupying isolation beds, and thus reducing isolation resources for the latter. For example, the overuse of third generation cephalosporin antibiotics can increase extended-spectrum $\beta$-lactamase-producing Klebsiella pneumoniae, thus indirectly influencing the transmission of meticillin-resistant Staphylococcus aureus (MRSA). In order to study this complex system of interrelationships, we have employed a Bayesian network. We report results of the first two years of analysis for a single public hospital. We conclude that, within this institution, the association between high bed occupancy and increased transmission of MRSA may be subject to a dynamic multidimensional threshold and tipping point. This may be influenced by other factors such as MRSA burden and whether the high bed occupancy interferes with preparation and cleaning of beds for new patients and with hand hygiene and efforts to isolate or cohort carriers. {\textcopyright} 2011 The Healthcare Infection Society.},
author = {Waterhouse, M. and Morton, a. and Mengersen, K. and Cook, D. and Playford, G.},
doi = {10.1016/j.jhin.2011.01.016},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Waterhouse et al. - 2011 - Role of overcrowding in meticillin-resistant Staphylococcus aureus transmission Bayesian network analysis for.pdf:pdf},
isbn = {1532-2939},
issn = {01956701},
journal = {J. Hosp. Infect.},
keywords = {Bayesian network,Meticillin-resistant Staphylococcus aureus (MRSA),Overcrowding},
number = {2},
pages = {92--96},
pmid = {21459478},
publisher = {Elsevier Ltd},
title = {{Role of overcrowding in meticillin-resistant Staphylococcus aureus transmission: Bayesian network analysis for a single public hospital}},
url = {http://dx.doi.org/10.1016/j.jhin.2011.01.016},
volume = {78},
year = {2011}
}
@article{Smith2013b,
abstract = {Aims: This paper describes the development of a risk adjustment (RA) model predictive of individual lesion treatment failure in percutaneous coronary interventions (PCI) for use in a quality monitoring and improvement program. Methods and results: Prospectively collected data for 3972 consecutive revascularisation procedures (5601 lesions) performed between January 2003 and September 2011 were studied. Data on procedures to September 2009 (n=3100) were used to identify factors predictive of lesion treatment failure. Factors identified included lesion risk class (p{\textless}0.001), occlusion type (p{\textless}0.001), patient age (p=0.001), vessel system (p{\textless}0.04), vessel diameter (p{\textless}0.001), unstable angina (p=0.003) and presence of major cardiac risk factors (p=0.01). A Bayesian RA model was built using these factors with predictive performance of the model tested on the remaining procedures (area under the receiver operating curve: 0.765, Hosmer-Lemeshow p value: 0.11). Cumulative sum, exponentially weighted moving average and funnel plots were constructed using the RA model and subjectively evaluated. Conclusion: A RA model was developed and applied to SPC monitoring for lesion failure in a PCI database. If linked to appropriate quality improvement governance response protocols, SPC using this RA tool might improve quality control and risk management by identifying variation in performance based on a comparison of observed and expected outcomes. {\textcopyright} 2012 Australian and New Zealand Society of Cardiac and Thoracic Surgeons (ANZSCTS) and the Cardiac Society of Australia and New Zealand (CSANZ).},
author = {Smith, Ian R. and Cameron, James and Mengersen, Kerrie L. and Foster, Kelley a. and Rivers, John T.},
doi = {10.1016/j.hlc.2012.10.001},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Smith et al. - 2013 - Risk Modelling in Quality Clinical Registries Monitoring Lesion Treatment Failure Rate in Percutaneous Coronary In.pdf:pdf},
isbn = {1443-9506},
issn = {14439506},
journal = {Hear. Lung Circ.},
keywords = {Angioplasty,Case mix or risk adjustment,Clinical governance,Quality improvement,Statistical process control},
number = {3},
pages = {193--203},
pmid = {23154197},
publisher = {Australian and New Zealand Society of Cardiac and Thoracic Surgeons (ANZSCTS) and the Cardiac Society of Australia and New Zealand (CSANZ)},
title = {{Risk Modelling in Quality Clinical Registries: Monitoring Lesion Treatment Failure Rate in Percutaneous Coronary Interventions}},
url = {http://dx.doi.org/10.1016/j.hlc.2012.10.001},
volume = {22},
year = {2013}
}
@article{Smith2013a,
abstract = {Background: Graphical Statistical Process Control (SPC) tools have been shown to promptly identify significant variations in clinical outcomes in a range of health care settings. We explored the application of these techniques to qualitatively inform the routine cardiac surgical morbidity and mortality (M{\&}M) review process at a single site. Methods: Baseline clinical and procedural data relating to 4774 consecutive cardiac surgical procedures, performed between the 1st January 2003 and the 30th April 2011, were retrospectively evaluated. A range of appropriate performance measures and benchmarks were developed and evaluated using a combination of CUmulative SUM (CUSUM) charts, Exponentially Weighted Moving Average (EWMA) charts and Funnel Plots. Charts have been discussed at the unit's routine M{\&}M meetings. Risk adjustment (RA) based on EuroSCORE has been incorporated into the charts to improve performance. Results: Discrete and aggregated measures, including Blood Product/Reoperation, major acute post-procedural complications and Length of Stay/Readmission. {\textless} 28 days have proved to be usable measures for monitoring outcomes. Monitoring trends in minor morbidities provides a valuable warning of impending changes in significant events. Instances of variation in performance have been examined and could be related to differences in individual operator performance via individual operator curves. Conclusion: SPC tools facilitate near "real-time" performance monitoring allowing early detection and intervention in altered performance. Careful interpretation of charts for group and individual operators has proven helpful in detecting and differentiating systemic vs. individual variation. {\textcopyright} 2012 Australian and New Zealand Society of Cardiac and Thoracic Surgeons (ANZSCTS) and the Cardiac Society of Australia and New Zealand (CSANZ).},
author = {Smith, Ian R. and Garlick, Bruce and Gardner, Michael a. and Brighouse, Russell D. and Foster, Kelley a. and Rivers, John T.},
doi = {10.1016/j.hlc.2012.08.060},
file = {:Users/ed/Library/Application Support/Mendeley Desktop/Downloaded/Smith et al. - 2013 - Use of graphical statistical process control tools to monitor and improve outcomes in cardiac surgery.pdf:pdf},
issn = {14439506},
journal = {Hear. Lung Circ.},
keywords = {Cardiac surgery,Outcome measures,Postoperative complications,Quality improvement,Statistical data interpretation},
number = {2},
pages = {92--99},
pmid = {23063751},
publisher = {Australian and New Zealand Society of Cardiac and Thoracic Surgeons (ANZSCTS) and the Cardiac Society of Australia and New Zealand (CSANZ)},
title = {{Use of graphical statistical process control tools to monitor and improve outcomes in cardiac surgery}},
url = {http://dx.doi.org/10.1016/j.hlc.2012.08.060},
volume = {22},
year = {2013}
}
@article{Downes2002,
abstract = {Introduction This is a concise summary of recommended features in LATEX and a couple of extension packages for writing math formulas. Readers needing greater depth of detail are referred to the sources listed in the bibliography, especially [Lamport], [LaTeX-U], [AMUG], [LaTeX-F], [LaTeX-G], and [GMS]. A certain amount of familiarity with standard LATEX terminology is assumed; if your memory needs refreshing on the LATEX meaning of command, optional argument, environment, package, and so forth, see [Lamport].},
author = {Downes, M.},
doi = {10.1.1.96.645},
editor = {Rogers, E and O'Reilly, J},
institution = {American Mathematical Society},
isbn = {1581139306},
journal = {Am. Math. Soc.},
pages = {1--17},
publisher = {Citeseer},
title = {{Short Math Guide for LATEX}},
year = {2002}
}
